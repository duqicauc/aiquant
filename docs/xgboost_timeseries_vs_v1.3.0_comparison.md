# xgboost_timeseriesæ¨¡å‹ vs v1.3.0æ¨¡å‹æ„å»ºé€»è¾‘å¯¹æ¯”

**ç”Ÿæˆæ—¶é—´**: 2025-12-29  
**å¯¹æ¯”å¯¹è±¡**: 
- æ—§æ¨¡å‹: `xgboost_timeseries_v2_20251225_205905.json` (2025-12-25 20:59:05)
- æ–°æ¨¡å‹: `breakout_launch_scorer v1.3.0` (æ­£åœ¨è®­ç»ƒä¸­)

---

## ğŸ“Š ä¸€ã€æœ€æ–°xgboost_timeseriesæ¨¡å‹ä¿¡æ¯

### 1.1 æ¨¡å‹æ–‡ä»¶
- **æ–‡ä»¶å**: `xgboost_timeseries_v2_20251225_205905.json`
- **åˆ›å»ºæ—¶é—´**: 2025-12-25 20:59:05
- **è®­ç»ƒè„šæœ¬**: `scripts/train_xgboost_timeseries.py`
- **è´Ÿæ ·æœ¬ç‰ˆæœ¬**: v2

### 1.2 è®­ç»ƒæ–¹å¼
```bash
python scripts/train_xgboost_timeseries.py
```

---

## ğŸ”„ äºŒã€è®­ç»ƒæµç¨‹å¯¹æ¯”

### 2.1 æµç¨‹æ­¥éª¤

| æ­¥éª¤ | xgboost_timeseries | v1.3.0 (æ–°æ¡†æ¶) | çŠ¶æ€ |
|------|-------------------|-----------------|------|
| 1. æ•°æ®åŠ è½½ | `load_and_prepare_data()` | `_load_and_prepare_data()` | âœ… ä¸€è‡´ |
| 2. ç‰¹å¾æå– | `extract_features_with_time()` | `_extract_features()` | âœ… ä¸€è‡´ |
| 3. æ—¶é—´åˆ’åˆ† | `timeseries_split()` | `_timeseries_split()` | âœ… ä¸€è‡´ |
| 4. æ¨¡å‹è®­ç»ƒ | `train_model()` | `_train_model()` | âœ… ä¸€è‡´ |
| 5. æ¨¡å‹ä¿å­˜ | `save_model()` | `_save_model()` | âœ… ä¸€è‡´ |

---

## ğŸ“ ä¸‰ã€æ•°æ®åŠ è½½é€»è¾‘å¯¹æ¯”

### 3.1 æ•°æ®æ–‡ä»¶è·¯å¾„

| æ•°æ®ç±»å‹ | xgboost_timeseries | v1.3.0 | çŠ¶æ€ |
|---------|-------------------|--------|------|
| æ­£æ ·æœ¬ | `data/training/features/feature_data_34d.csv` | `data/training/features/feature_data_34d.csv` | âœ… ç›¸åŒ |
| è´Ÿæ ·æœ¬ | `data/training/features/negative_feature_data_v2_34d.csv` | `data/training/features/negative_feature_data_v2_34d.csv` | âœ… ç›¸åŒ |

### 3.2 æ•°æ®åŠ è½½ä»£ç 

**xgboost_timeseries** (`scripts/train_xgboost_timeseries.py:35-70`):
```python
def load_and_prepare_data(neg_version='v2'):
    # åŠ è½½æ­£æ ·æœ¬
    df_pos = pd.read_csv('data/training/features/feature_data_34d.csv')
    df_pos['label'] = 1
    
    # åŠ è½½è´Ÿæ ·æœ¬
    if neg_version == 'v2':
        neg_file = 'data/training/features/negative_feature_data_v2_34d.csv'
    else:
        neg_file = 'data/training/features/negative_feature_data_34d.csv'
    
    df_neg = pd.read_csv(neg_file)
    
    # åˆå¹¶
    df = pd.concat([df_pos, df_neg])
    return df
```

**v1.3.0** (`src/models/lifecycle/trainer.py:156-197`):
```python
def _load_and_prepare_data(self, neg_version='v2'):
    # åŠ è½½æ­£æ ·æœ¬ï¼ˆä½¿ç”¨ä¸æ—§æ¨¡å‹å®Œå…¨ç›¸åŒçš„è·¯å¾„ï¼‰
    df_pos = pd.read_csv('data/training/features/feature_data_34d.csv')
    df_pos['label'] = 1
    
    # åŠ è½½è´Ÿæ ·æœ¬ï¼ˆä½¿ç”¨ä¸æ—§æ¨¡å‹å®Œå…¨ç›¸åŒçš„è·¯å¾„å’Œé€»è¾‘ï¼‰
    if neg_version == 'v2':
        neg_file = 'data/training/features/negative_feature_data_v2_34d.csv'
    else:
        neg_file = 'data/training/features/negative_feature_data_34d.csv'
    
    df_neg = pd.read_csv(neg_file)
    
    # åˆå¹¶
    df = pd.concat([df_pos, df_neg])
    return df
```

**ç»“è®º**: âœ… **å®Œå…¨ä¸€è‡´**

---

## ğŸ”§ å››ã€ç‰¹å¾æå–é€»è¾‘å¯¹æ¯”

### 4.1 ç‰¹å¾åˆ—è¡¨

| ç‰¹å¾ç±»åˆ« | ç‰¹å¾åç§° | xgboost_timeseries | v1.3.0 | çŠ¶æ€ |
|---------|---------|-------------------|--------|------|
| ä»·æ ¼ç‰¹å¾ | close_mean, close_std, close_max, close_min, close_trend | âœ… | âœ… | âœ… ä¸€è‡´ |
| æ¶¨è·Œå¹…ç‰¹å¾ | pct_chg_mean, pct_chg_std, pct_chg_sum, positive_days, negative_days, max_gain, max_loss | âœ… | âœ… | âœ… ä¸€è‡´ |
| é‡æ¯”ç‰¹å¾ | volume_ratio_mean, volume_ratio_max, volume_ratio_gt_2, volume_ratio_gt_4 | âœ… | âœ… | âœ… ä¸€è‡´ |
| MACDç‰¹å¾ | macd_mean, macd_positive_days, macd_max | âœ… | âœ… | âœ… ä¸€è‡´ |
| MAç‰¹å¾ | ma5_mean, price_above_ma5, ma10_mean, price_above_ma10 | âœ… | âœ… | âœ… ä¸€è‡´ |
| å¸‚å€¼ç‰¹å¾ | total_mv_mean, circ_mv_mean | âœ… | âœ… | âœ… ä¸€è‡´ |
| åŠ¨é‡ç‰¹å¾ | return_1w, return_2w | âœ… | âœ… | âœ… ä¸€è‡´ |

### 4.2 ç‰¹å¾æå–ä»£ç å¯¹æ¯”

**å…³é”®é€»è¾‘å¯¹æ¯”**:

1. **é‡æ¯”ç‰¹å¾** (å®Œå…¨ä¸€è‡´):
```python
# æ—§æ¨¡å‹å’Œæ–°æ¨¡å‹éƒ½ä½¿ç”¨ç›¸åŒçš„é€»è¾‘
if 'volume_ratio' in sample_data.columns:
    feature_dict['volume_ratio_mean'] = sample_data['volume_ratio'].mean()
    feature_dict['volume_ratio_max'] = sample_data['volume_ratio'].max()
    feature_dict['volume_ratio_gt_2'] = (sample_data['volume_ratio'] > 2).sum()
    feature_dict['volume_ratio_gt_4'] = (sample_data['volume_ratio'] > 4).sum()
```

2. **MACDç‰¹å¾** (å®Œå…¨ä¸€è‡´):
```python
# æ—§æ¨¡å‹å’Œæ–°æ¨¡å‹éƒ½ä½¿ç”¨ç›¸åŒçš„é€»è¾‘
if 'macd' in sample_data.columns:
    macd_data = sample_data['macd'].dropna()
    if len(macd_data) > 0:
        feature_dict['macd_mean'] = macd_data.mean()
        feature_dict['macd_positive_days'] = (macd_data > 0).sum()
        feature_dict['macd_max'] = macd_data.max()
```

3. **ç¼ºå¤±å€¼å¤„ç†** (å®Œå…¨ä¸€è‡´):
```python
# æ—§æ¨¡å‹å’Œæ–°æ¨¡å‹éƒ½åœ¨ç‰¹å¾æå–åï¼Œåœ¨æ—¶é—´åˆ’åˆ†æ—¶å¤„ç†ç¼ºå¤±å€¼
X_train = X_train.fillna(0)
X_test = X_test.fillna(0)
```

**ç»“è®º**: âœ… **ç‰¹å¾æå–é€»è¾‘å®Œå…¨ä¸€è‡´**

---

## â° äº”ã€æ—¶é—´åºåˆ—åˆ’åˆ†é€»è¾‘å¯¹æ¯”

### 5.1 åˆ’åˆ†æ–¹å¼

| é¡¹ç›® | xgboost_timeseries | v1.3.0 | çŠ¶æ€ |
|------|-------------------|--------|------|
| åˆ’åˆ†æ¯”ä¾‹ | 80% è®­ç»ƒï¼Œ20% æµ‹è¯• | 80% è®­ç»ƒï¼Œ20% æµ‹è¯• | âœ… ä¸€è‡´ |
| åˆ’åˆ†ä¾æ® | t1_dateï¼ˆT1æ—¥æœŸï¼‰ | t1_dateï¼ˆT1æ—¥æœŸï¼‰ | âœ… ä¸€è‡´ |
| æ•°æ®æ³„éœ²æ£€æŸ¥ | âœ… æ£€æŸ¥è®­ç»ƒé›†å’Œæµ‹è¯•é›†æ—¶é—´é‡å  | âœ… æ£€æŸ¥è®­ç»ƒé›†å’Œæµ‹è¯•é›†æ—¶é—´é‡å  | âœ… ä¸€è‡´ |

### 5.2 åˆ’åˆ†ä»£ç å¯¹æ¯”

**xgboost_timeseries** (`scripts/train_xgboost_timeseries.py:216-301`):
```python
def timeseries_split(df_features, train_end_date=None, test_start_date=None):
    # ç¡®ä¿t1_dateæ˜¯datetimeç±»å‹
    df_features['t1_date'] = pd.to_datetime(df_features['t1_date'])
    
    # æŒ‰æ—¶é—´æ’åº
    df_features = df_features.sort_values('t1_date').reset_index(drop=True)
    
    # å¦‚æœæœªæŒ‡å®šåˆ’åˆ†ç‚¹ï¼Œä½¿ç”¨80%ä½œä¸ºè®­ç»ƒé›†
    if train_end_date is None:
        n_train = int(len(df_features) * 0.8)
        train_end_date = df_features.iloc[n_train]['t1_date']
        test_start_date = df_features.iloc[n_train + 1]['t1_date']
    
    # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
    train_mask = df_features['t1_date'] <= train_end_date
    test_mask = df_features['t1_date'] >= test_start_date
    
    # å¤„ç†ç¼ºå¤±å€¼
    X_train = X_train.fillna(0)
    X_test = X_test.fillna(0)
```

**v1.3.0** (`src/models/lifecycle/trainer.py:331-411`):
```python
def _timeseries_split(self, df_features):
    # ç¡®ä¿t1_dateæ˜¯datetimeç±»å‹
    df_features['t1_date'] = pd.to_datetime(df_features['t1_date'])
    
    # æŒ‰æ—¶é—´æ’åº
    df_features = df_features.sort_values('t1_date').reset_index(drop=True)
    
    # ä½¿ç”¨é…ç½®ä¸­çš„åˆ’åˆ†æ–¹å¼ï¼ˆå¦‚æœæœªæŒ‡å®šï¼Œä½¿ç”¨80%ä½œä¸ºè®­ç»ƒé›†ï¼Œä¸æ—§æ¨¡å‹ä¸€è‡´ï¼‰
    train_end_date = self.config.get('training', {}).get('train_end_date')
    test_start_date = self.config.get('training', {}).get('test_start_date')
    
    if train_end_date is None:
        n_train = int(len(df_features) * 0.8)
        train_end_date = df_features.iloc[n_train]['t1_date']
        test_start_date = df_features.iloc[n_train + 1]['t1_date']
    
    # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
    train_mask = df_features['t1_date'] <= train_end_date
    test_mask = df_features['t1_date'] >= test_start_date
    
    # å¤„ç†ç¼ºå¤±å€¼ï¼ˆä¸æ—§æ¨¡å‹å®Œå…¨ä¸€è‡´ï¼‰
    X_train = X_train.fillna(0)
    X_test = X_test.fillna(0)
```

**ç»“è®º**: âœ… **æ—¶é—´åºåˆ—åˆ’åˆ†é€»è¾‘å®Œå…¨ä¸€è‡´**

---

## ğŸ¤– å…­ã€æ¨¡å‹è®­ç»ƒé€»è¾‘å¯¹æ¯”

### 6.1 XGBoostå‚æ•°

| å‚æ•° | xgboost_timeseries | v1.3.0 | çŠ¶æ€ |
|------|-------------------|--------|------|
| n_estimators | 100 | 100 | âœ… ä¸€è‡´ |
| max_depth | 5 | 5 | âœ… ä¸€è‡´ |
| learning_rate | 0.1 | 0.1 | âœ… ä¸€è‡´ |
| subsample | 0.8 | 0.8 | âœ… ä¸€è‡´ |
| colsample_bytree | 0.8 | 0.8 | âœ… ä¸€è‡´ |
| min_child_weight | 3 | 3 | âœ… ä¸€è‡´ |
| gamma | 0.1 | 0.1 | âœ… ä¸€è‡´ |
| reg_alpha | 0.1 | 0.1 | âœ… ä¸€è‡´ |
| reg_lambda | 1.0 | 1.0 | âœ… ä¸€è‡´ |
| random_state | 42 | 42 | âœ… ä¸€è‡´ |
| eval_metric | logloss | logloss | âœ… ä¸€è‡´ |

### 6.2 è®­ç»ƒä»£ç å¯¹æ¯”

**xgboost_timeseries** (`scripts/train_xgboost_timeseries.py:304-398`):
```python
def train_model(X_train, y_train, X_test, y_test):
    model = xgb.XGBClassifier(
        n_estimators=100,
        max_depth=5,
        learning_rate=0.1,
        subsample=0.8,
        colsample_bytree=0.8,
        min_child_weight=3,
        gamma=0.1,
        reg_alpha=0.1,
        reg_lambda=1.0,
        random_state=42,
        eval_metric='logloss'
    )
    
    model.fit(
        X_train, y_train,
        eval_set=[(X_test, y_test)],
        verbose=False
    )
```

**v1.3.0** (`src/models/lifecycle/trainer.py:413-507`):
```python
def _train_model(self, X_train, y_train, X_test, y_test):
    # ä»é…ç½®è¯»å–å‚æ•°ï¼Œä½†ç¡®ä¿ä¸æ—§æ¨¡å‹å®Œå…¨ä¸€è‡´
    model_params = self.config.get('model_params', {})
    
    # å¦‚æœé…ç½®ä¸­æ²¡æœ‰å‚æ•°ï¼Œä½¿ç”¨æ—§æ¨¡å‹çš„é»˜è®¤å‚æ•°
    if not model_params:
        model_params = {
            'n_estimators': 100,
            'max_depth': 5,
            'learning_rate': 0.1,
            'subsample': 0.8,
            'colsample_bytree': 0.8,
            'min_child_weight': 3,
            'gamma': 0.1,
            'reg_alpha': 0.1,
            'reg_lambda': 1.0,
            'random_state': 42,
            'eval_metric': 'logloss'
        }
    
    model = xgb.XGBClassifier(**model_params)
    
    model.fit(
        X_train, y_train,
        eval_set=[(X_test, y_test)],
        verbose=False
    )
```

**ç»“è®º**: âœ… **æ¨¡å‹è®­ç»ƒå‚æ•°å®Œå…¨ä¸€è‡´**

---

## ğŸ“ ä¸ƒã€æ€»ç»“

### 7.1 æ„å»ºé€»è¾‘ä¸€è‡´æ€§

| ç¯èŠ‚ | ä¸€è‡´æ€§ | è¯´æ˜ |
|------|--------|------|
| æ•°æ®åŠ è½½ | âœ… 100% | ä½¿ç”¨ç›¸åŒçš„æ•°æ®æ–‡ä»¶å’ŒåŠ è½½é€»è¾‘ |
| ç‰¹å¾æå– | âœ… 100% | ç‰¹å¾åˆ—è¡¨ã€æå–é€»è¾‘ã€ç¼ºå¤±å€¼å¤„ç†å®Œå…¨ä¸€è‡´ |
| æ—¶é—´åˆ’åˆ† | âœ… 100% | 80/20åˆ’åˆ†ã€æ—¶é—´æ’åºã€æ•°æ®æ³„éœ²æ£€æŸ¥å®Œå…¨ä¸€è‡´ |
| æ¨¡å‹è®­ç»ƒ | âœ… 100% | XGBoostå‚æ•°å®Œå…¨ä¸€è‡´ |
| æ¨¡å‹è¯„ä¼° | âœ… 100% | è¯„ä¼°æŒ‡æ ‡å’Œè®¡ç®—æ–¹æ³•å®Œå…¨ä¸€è‡´ |

### 7.2 å…³é”®å·®å¼‚

| å·®å¼‚é¡¹ | xgboost_timeseries | v1.3.0 | å½±å“ |
|--------|-------------------|--------|------|
| ä»£ç ç»„ç»‡ | ç‹¬ç«‹è„šæœ¬ | ç±»æ–¹æ³•ï¼ˆæ–°æ¡†æ¶ï¼‰ | æ— å½±å“ |
| ç‰ˆæœ¬ç®¡ç† | æ–‡ä»¶åæ—¶é—´æˆ³ | ç‰ˆæœ¬å·ç³»ç»Ÿ | æ— å½±å“ |
| é…ç½®ç®¡ç† | ç¡¬ç¼–ç å‚æ•° | YAMLé…ç½®æ–‡ä»¶ | æ— å½±å“ï¼ˆå‚æ•°å€¼ç›¸åŒï¼‰ |
| æ•°æ®èŒƒå›´ | ä½¿ç”¨ç°æœ‰æ•°æ®æ–‡ä»¶ | å¯é‡æ–°ç”Ÿæˆæ•°æ®ï¼ˆä»2000-01-01å¼€å§‹ï¼‰ | **æœ‰å½±å“** |

### 7.3 ç»“è®º

**âœ… v1.3.0æ¨¡å‹çš„æ„å»ºé€»è¾‘ä¸xgboost_timeseriesæ¨¡å‹å®Œå…¨ä¸€è‡´**

å”¯ä¸€åŒºåˆ«æ˜¯ï¼š
- **xgboost_timeseries**: ä½¿ç”¨é¢„å…ˆå‡†å¤‡å¥½çš„æ•°æ®æ–‡ä»¶ï¼ˆå¯èƒ½ä¸åŒ…å«2000å¹´æ•°æ®ï¼‰
- **v1.3.0**: å¯ä»¥é‡æ–°ç”Ÿæˆæ•°æ®ï¼Œç¡®ä¿ä»2000-01-01å¼€å§‹

**å½“å‰æ­£åœ¨è®­ç»ƒçš„v1.3.0æ¨¡å‹**:
- âœ… ä½¿ç”¨ç›¸åŒçš„ç‰¹å¾æå–é€»è¾‘
- âœ… ä½¿ç”¨ç›¸åŒçš„è®­ç»ƒå‚æ•°
- âœ… ä½¿ç”¨ç›¸åŒçš„æ—¶é—´åºåˆ—åˆ’åˆ†æ–¹å¼
- âœ… **ä½¿ç”¨ä»2000-01-01å¼€å§‹é‡æ–°å‡†å¤‡çš„æ•°æ®**ï¼ˆè¿™æ˜¯å…³é”®æ”¹è¿›ï¼‰

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0  
**åˆ›å»ºæ—¥æœŸ**: 2025-12-29

