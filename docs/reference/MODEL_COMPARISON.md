# æœºå™¨å­¦ä¹ æ¨¡å‹é€‰æ‹©æŒ‡å— ğŸ¤–

é’ˆå¯¹è‚¡ç¥¨é€‰è‚¡æ¨¡å‹çš„å®Œæ•´æ¨¡å‹å¯¹æ¯”ä¸å®éªŒè®¾è®¡

---

## ğŸ“Š é—®é¢˜å®šä¹‰

### æ•°æ®ç‰¹ç‚¹
```
ä»»åŠ¡ç±»å‹: äºŒåˆ†ç±»ï¼ˆæ­£æ ·æœ¬/è´Ÿæ ·æœ¬ï¼‰
ç‰¹å¾ç»´åº¦: 34å¤© Ã— Nä¸ªæŒ‡æ ‡ï¼ˆä»·æ ¼ã€æ¶¨è·Œå¹…ã€MACDã€RSIã€MAã€é‡æ¯”ç­‰ï¼‰
æ ·æœ¬é‡: ~2,300ä¸ªï¼ˆ1,145æ­£ + 1,145è´Ÿï¼‰
æ•°æ®ç±»å‹: æ—¶é—´åºåˆ— â†’ è¡¨æ ¼åŒ–ç‰¹å¾
```

### ç›®æ ‡
åœ¨T1æ—¶åˆ»ï¼Œæ ¹æ®å‰34å¤©çš„æ•°æ®ï¼Œé¢„æµ‹è¯¥è‚¡ç¥¨æ˜¯å¦ä¸ºæ½œåŠ›è‚¡ï¼ˆæœªæ¥3å‘¨æ¶¨å¹…>50%ï¼‰

---

## ğŸ† æ¨¡å‹æ¨èæ’è¡Œæ¦œ

### ğŸ¥‡ ç¬¬ä¸€åï¼šæ ‘æ¨¡å‹ï¼ˆXGBoost / LightGBMï¼‰â­â­â­â­â­

**æœ€å¼ºçƒˆæ¨èï¼**

#### æ¨èç†ç”±

| ç»´åº¦ | è¯„åˆ† | è¯´æ˜ |
|------|------|------|
| **æ•ˆæœ** | â­â­â­â­â­ | åœ¨é‡‘èè¡¨æ ¼æ•°æ®ä¸Šè¡¨ç°å“è¶Š |
| **é€Ÿåº¦** | â­â­â­â­â­ | è®­ç»ƒå¿«ï¼ˆå‡ åˆ†é’Ÿï¼‰ |
| **æ˜“ç”¨æ€§** | â­â­â­â­â­ | è°ƒå‚ç®€å•ï¼Œé»˜è®¤å‚æ•°å°±ä¸é”™ |
| **é²æ£’æ€§** | â­â­â­â­â­ | å¯¹ç¼ºå¤±å€¼ã€å¼‚å¸¸å€¼ä¸æ•æ„Ÿ |
| **å¯è§£é‡Šæ€§** | â­â­â­â­â­ | å¯æŸ¥çœ‹ç‰¹å¾é‡è¦æ€§ |
| **æ ·æœ¬éœ€æ±‚** | â­â­â­â­â­ | 2000+æ ·æœ¬å·²è¶³å¤Ÿ |

#### ä¼˜ç‚¹ âœ…
- **æ•ˆæœå¥½**ï¼šåœ¨Kaggleé‡‘èç«èµ›ä¸­å¸¸èƒœå†›
- **è®­ç»ƒå¿«**ï¼šXGBoostè®­ç»ƒå‡ åˆ†é’Ÿï¼ŒLightGBMæ›´å¿«
- **ä¸è¿‡æ‹Ÿåˆ**ï¼šæœ‰æ­£åˆ™åŒ–ï¼Œæ ·æœ¬å°‘ä¹Ÿä¸æ€•
- **ç‰¹å¾å·¥ç¨‹å‹å¥½**ï¼šè‡ªåŠ¨å¤„ç†ç‰¹å¾äº¤äº’
- **å¯è§£é‡Š**ï¼šçŸ¥é“å“ªäº›æŒ‡æ ‡æœ€é‡è¦ï¼ˆå¦‚ï¼šMACDæƒé‡0.23ï¼Œé‡æ¯”0.18...ï¼‰
- **ç”Ÿäº§éƒ¨ç½²ç®€å•**ï¼šæ¨¡å‹å°ï¼Œæ¨ç†å¿«

#### ç¼ºç‚¹ âš ï¸
- ä¸èƒ½ç›´æ¥å¤„ç†åŸå§‹æ—¶é—´åºåˆ—ï¼ˆéœ€è¦ç‰¹å¾å·¥ç¨‹ï¼‰
- éš¾ä»¥æ•æ‰å¤æ‚çš„æ—¶åºä¾èµ–

#### é€‚ç”¨åœºæ™¯
âœ… **ä½ çš„æ•°æ®å·²ç»æ˜¯è¡¨æ ¼å‹ç‰¹å¾** â†’ å®Œç¾åŒ¹é…ï¼  
âœ… **æ ·æœ¬é‡2000+** â†’ è¶³å¤Ÿäº†ï¼  
âœ… **éœ€è¦å¿«é€ŸéªŒè¯** â†’ æœ€ä½³é€‰æ‹©ï¼  
âœ… **æƒ³çŸ¥é“å“ªäº›æŒ‡æ ‡é‡è¦** â†’ å¯è§£é‡Šæ€§å¼ºï¼

#### å®ç°ç¤ºä¾‹

```python
"""
XGBoost/LightGBM è®­ç»ƒè„šæœ¬
"""
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score
import xgboost as xgb
import lightgbm as lgb

# 1. åŠ è½½æ•°æ®
df_pos = pd.read_csv('data/processed/feature_data_34d.csv')
df_pos['label'] = 1

df_neg = pd.read_csv('data/processed/negative_feature_data_v2_34d.csv')
# label=0å·²åŒ…å«

df = pd.concat([df_pos, df_neg])
df = df.sample(frac=1, random_state=42).reset_index(drop=True)

# 2. ç‰¹å¾å·¥ç¨‹ï¼ˆå±•å¹³æ—¶é—´åºåˆ—ï¼‰
# æ¯ä¸ªæ ·æœ¬34å¤© â†’ éœ€è¦è½¬æ¢ä¸ºä¸€è¡Œå¤šåˆ—
# æ–¹æ³•1ï¼šèšåˆç»Ÿè®¡ç‰¹å¾
features = []
for sample_id in df['sample_id'].unique():
    sample_data = df[df['sample_id'] == sample_id].sort_values('days_to_t1')
    
    feature_dict = {
        'sample_id': sample_id,
        'label': sample_data['label'].iloc[0],
        
        # ä»·æ ¼ç›¸å…³
        'close_mean': sample_data['close'].mean(),
        'close_std': sample_data['close'].std(),
        'close_max': sample_data['close'].max(),
        'close_min': sample_data['close'].min(),
        'close_trend': (sample_data['close'].iloc[-1] - sample_data['close'].iloc[0]) / sample_data['close'].iloc[0],
        
        # æ¶¨è·Œå¹…
        'pct_chg_mean': sample_data['pct_chg'].mean(),
        'pct_chg_std': sample_data['pct_chg'].std(),
        'pct_chg_sum': sample_data['pct_chg'].sum(),
        'positive_days': (sample_data['pct_chg'] > 0).sum(),
        'negative_days': (sample_data['pct_chg'] < 0).sum(),
        
        # é‡æ¯”
        'volume_ratio_mean': sample_data['volume_ratio'].mean(),
        'volume_ratio_max': sample_data['volume_ratio'].max(),
        'volume_ratio_gt_2': (sample_data['volume_ratio'] > 2).sum(),
        
        # MACDï¼ˆå¦‚æœæœ‰ï¼‰
        'macd_mean': sample_data['macd'].mean() if 'macd' in sample_data.columns else np.nan,
        'macd_positive_days': (sample_data['macd'] > 0).sum() if 'macd' in sample_data.columns else np.nan,
        
        # MA
        'ma5_mean': sample_data['ma5'].mean() if 'ma5' in sample_data.columns else np.nan,
        'ma10_mean': sample_data['ma10'].mean() if 'ma10' in sample_data.columns else np.nan,
        'price_above_ma5': (sample_data['close'] > sample_data['ma5']).sum() if 'ma5' in sample_data.columns else np.nan,
        
        # å¸‚å€¼
        'total_mv_mean': sample_data['total_mv'].mean() if 'total_mv' in sample_data.columns else np.nan,
    }
    
    features.append(feature_dict)

df_features = pd.DataFrame(features)

# 3. å‡†å¤‡è®­ç»ƒæ•°æ®
X = df_features.drop(['sample_id', 'label'], axis=1)
y = df_features['label']

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# 4. XGBoostè®­ç»ƒ
print("="*80)
print("è®­ç»ƒ XGBoost æ¨¡å‹...")
print("="*80)

xgb_model = xgb.XGBClassifier(
    n_estimators=100,
    max_depth=5,
    learning_rate=0.1,
    random_state=42,
    eval_metric='logloss'
)

xgb_model.fit(X_train, y_train)

# é¢„æµ‹
y_pred_xgb = xgb_model.predict(X_test)
y_prob_xgb = xgb_model.predict_proba(X_test)[:, 1]

# è¯„ä¼°
print("\nXGBoost æ€§èƒ½:")
print(classification_report(y_test, y_pred_xgb, target_names=['è´Ÿæ ·æœ¬', 'æ­£æ ·æœ¬']))
print(f"AUC: {roc_auc_score(y_test, y_prob_xgb):.4f}")

# ç‰¹å¾é‡è¦æ€§
feature_importance = pd.DataFrame({
    'feature': X.columns,
    'importance': xgb_model.feature_importances_
}).sort_values('importance', ascending=False)

print("\nç‰¹å¾é‡è¦æ€§ Top 10:")
print(feature_importance.head(10))

# 5. LightGBMè®­ç»ƒ
print("\n" + "="*80)
print("è®­ç»ƒ LightGBM æ¨¡å‹...")
print("="*80)

lgb_model = lgb.LGBMClassifier(
    n_estimators=100,
    max_depth=5,
    learning_rate=0.1,
    random_state=42
)

lgb_model.fit(X_train, y_train)

y_pred_lgb = lgb_model.predict(X_test)
y_prob_lgb = lgb_model.predict_proba(X_test)[:, 1]

print("\nLightGBM æ€§èƒ½:")
print(classification_report(y_test, y_pred_lgb, target_names=['è´Ÿæ ·æœ¬', 'æ­£æ ·æœ¬']))
print(f"AUC: {roc_auc_score(y_test, y_prob_lgb):.4f}")

# 6. ä¿å­˜æ¨¡å‹
import joblib
joblib.dump(xgb_model, 'models/xgboost_model.pkl')
joblib.dump(lgb_model, 'models/lightgbm_model.pkl')

print("\nâœ… æ¨¡å‹è®­ç»ƒå®Œæˆå¹¶å·²ä¿å­˜ï¼")
```

---

### ğŸ¥ˆ ç¬¬äºŒåï¼šéšæœºæ£®æ—ï¼ˆRandom Forestï¼‰â­â­â­â­

**ç¨³å®šå¯é çš„baseline**

#### æ¨èç†ç”±

| ç»´åº¦ | è¯„åˆ† | è¯´æ˜ |
|------|------|------|
| **æ•ˆæœ** | â­â­â­â­ | ç¨³å®šï¼Œä¸è¿‡é€šå¸¸ä¸å¦‚XGBoost |
| **é€Ÿåº¦** | â­â­â­â­ | è¾ƒå¿«ï¼Œå¯å¹¶è¡Œ |
| **æ˜“ç”¨æ€§** | â­â­â­â­â­ | è¶…ç®€å•ï¼Œå‡ ä¹ä¸éœ€è¦è°ƒå‚ |
| **é²æ£’æ€§** | â­â­â­â­â­ | éå¸¸é²æ£’ |
| **å¯è§£é‡Šæ€§** | â­â­â­â­ | æœ‰ç‰¹å¾é‡è¦æ€§ |

#### ä¼˜ç‚¹ âœ…
- **æç®€å•**ï¼šåŸºæœ¬ä¸éœ€è¦è°ƒå‚
- **é²æ£’**ï¼šå¯¹å™ªå£°ä¸æ•æ„Ÿ
- **å¯è§£é‡Š**ï¼šç‰¹å¾é‡è¦æ€§ç›´è§‚

#### ç¼ºç‚¹ âš ï¸
- æ•ˆæœé€šå¸¸ä¸å¦‚XGBoost
- æ¨¡å‹æ–‡ä»¶å¤§

#### å®ç°ç¤ºä¾‹

```python
from sklearn.ensemble import RandomForestClassifier

rf_model = RandomForestClassifier(
    n_estimators=200,
    max_depth=10,
    random_state=42,
    n_jobs=-1  # å¹¶è¡Œè®­ç»ƒ
)

rf_model.fit(X_train, y_train)
y_pred = rf_model.predict(X_test)

print(classification_report(y_test, y_pred))
```

---

### ğŸ¥‰ ç¬¬ä¸‰åï¼šLSTMï¼ˆé•¿çŸ­æœŸè®°å¿†ç½‘ç»œï¼‰â­â­â­

**é€‚åˆå¤æ‚æ—¶åºæ¨¡å¼ï¼Œä½†ä¸æ˜¯å¿…éœ€**

#### æ¨èç†ç”±

| ç»´åº¦ | è¯„åˆ† | è¯´æ˜ |
|------|------|------|
| **æ•ˆæœ** | â­â­â­ | å¯èƒ½æ›´å¥½ï¼Œä½†éœ€è¦å¤§é‡è°ƒå‚ |
| **é€Ÿåº¦** | â­â­ | è®­ç»ƒæ…¢ï¼ˆéœ€è¦GPUï¼‰ |
| **æ˜“ç”¨æ€§** | â­â­ | å¤æ‚ï¼Œè°ƒå‚å›°éš¾ |
| **é²æ£’æ€§** | â­â­ | å®¹æ˜“è¿‡æ‹Ÿåˆ |
| **å¯è§£é‡Šæ€§** | â­ | é»‘ç›’ |
| **æ ·æœ¬éœ€æ±‚** | â­â­ | éœ€è¦æ›´å¤šæ•°æ®ï¼ˆ5000+æ›´å¥½ï¼‰ |

#### ä¼˜ç‚¹ âœ…
- **æ•æ‰æ—¶åºä¾èµ–**ï¼šèƒ½å­¦ä¹ å¤æ‚çš„æ—¶é—´æ¨¡å¼
- **è‡ªåŠ¨ç‰¹å¾å­¦ä¹ **ï¼šä¸éœ€è¦æ‰‹åŠ¨ç‰¹å¾å·¥ç¨‹
- **æ½œåŠ›å¤§**ï¼šæ•°æ®å¤šæ—¶æ•ˆæœå¥½

#### ç¼ºç‚¹ âš ï¸
- **æ ·æœ¬é‡éœ€æ±‚é«˜**ï¼š2000+æ ·æœ¬å¯èƒ½ä¸å¤Ÿï¼Œå®¹æ˜“è¿‡æ‹Ÿåˆ
- **è®­ç»ƒæ…¢**ï¼šéœ€è¦GPUï¼Œè°ƒå‚è€—æ—¶
- **ä¸å¯è§£é‡Š**ï¼šé»‘ç›’æ¨¡å‹
- **è°ƒå‚å¤æ‚**ï¼šå±‚æ•°ã€å•å…ƒæ•°ã€dropoutç­‰

#### ä½•æ—¶ä½¿ç”¨LSTMï¼Ÿ

âœ… **é€‚åˆçš„åœºæ™¯**ï¼š
- æ ·æœ¬é‡ > 5000
- ç‰¹å¾é—´æœ‰å¤æ‚çš„æ—¶åºä¾èµ–
- æœ‰GPUèµ„æº
- æœ‰æ—¶é—´è°ƒå‚
- è¿½æ±‚æè‡´æ•ˆæœ

âŒ **ä¸é€‚åˆçš„åœºæ™¯**ï¼ˆä½ çš„æƒ…å†µï¼‰ï¼š
- æ ·æœ¬é‡ ~2300ï¼ˆåå°‘ï¼‰
- ç‰¹å¾å·²ç»å¾ˆå¥½ï¼ˆMACDã€RSIç­‰éƒ½æ˜¯æˆç†ŸæŒ‡æ ‡ï¼‰
- éœ€è¦å¿«é€ŸéªŒè¯
- éœ€è¦å¯è§£é‡Šæ€§

#### å®ç°ç¤ºä¾‹

```python
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout

# å‡†å¤‡3Dæ•°æ®ï¼š(samples, timesteps, features)
# éœ€è¦å°†æ•°æ®reshapeä¸º (æ ·æœ¬æ•°, 34å¤©, ç‰¹å¾æ•°)

# å‡è®¾æ¯ä¸ªæ ·æœ¬34å¤©ï¼Œæ¯å¤©10ä¸ªç‰¹å¾
X_train_lstm = X_train.values.reshape(-1, 34, 10)
X_test_lstm = X_test.values.reshape(-1, 34, 10)

# æ„å»ºLSTMæ¨¡å‹
model = Sequential([
    LSTM(64, input_shape=(34, 10), return_sequences=True),
    Dropout(0.3),
    LSTM(32),
    Dropout(0.3),
    Dense(16, activation='relu'),
    Dense(1, activation='sigmoid')
])

model.compile(
    optimizer='adam',
    loss='binary_crossentropy',
    metrics=['accuracy', 'AUC']
)

# è®­ç»ƒ
history = model.fit(
    X_train_lstm, y_train,
    validation_split=0.2,
    epochs=50,
    batch_size=32,
    callbacks=[
        tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)
    ]
)

# é¢„æµ‹
y_pred_prob = model.predict(X_test_lstm)
y_pred = (y_pred_prob > 0.5).astype(int)

print(classification_report(y_test, y_pred))
```

---

### ğŸ… ç¬¬å››åï¼šä¼ ç»Ÿç¥ç»ç½‘ç»œï¼ˆMLPï¼‰â­â­â­

**ä¸­è§„ä¸­çŸ©çš„æ·±åº¦å­¦ä¹ æ–¹æ¡ˆ**

#### æ¨èç†ç”±

| ç»´åº¦ | è¯„åˆ† | è¯´æ˜ |
|------|------|------|
| **æ•ˆæœ** | â­â­â­ | ä¸­ç­‰ |
| **é€Ÿåº¦** | â­â­â­ | ä¸€èˆ¬ |
| **æ˜“ç”¨æ€§** | â­â­â­ | éœ€è¦è°ƒå‚ |
| **é²æ£’æ€§** | â­â­ | å®¹æ˜“è¿‡æ‹Ÿåˆ |
| **å¯è§£é‡Šæ€§** | â­ | é»‘ç›’ |

#### é€‚åˆåœºæ™¯
- ç‰¹å¾å·²ç»æ˜¯è¡¨æ ¼å‹
- æƒ³å°è¯•æ·±åº¦å­¦ä¹ ä½†ä¸æƒ³å¤ªå¤æ‚
- æ•°æ®é‡é€‚ä¸­

```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout

model = Sequential([
    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),
    Dropout(0.3),
    Dense(64, activation='relu'),
    Dropout(0.3),
    Dense(32, activation='relu'),
    Dense(1, activation='sigmoid')
])

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
model.fit(X_train, y_train, validation_split=0.2, epochs=50, batch_size=32)
```

---

## ğŸ¯ ç»ˆææ¨èæ–¹æ¡ˆ

### ç¬¬ä¸€é˜¶æ®µï¼šå¿«é€ŸéªŒè¯ï¼ˆ1-2å¤©ï¼‰

```
1ï¸âƒ£ XGBoost/LightGBMï¼ˆå¿…åšï¼‰
   â†“
   æ•ˆæœå¥½ â†’ ç›´æ¥ç”¨
   æ•ˆæœä¸å¥½ â†’ ä¸‹ä¸€æ­¥
```

### ç¬¬äºŒé˜¶æ®µï¼šæ¨¡å‹ä¼˜åŒ–ï¼ˆ3-5å¤©ï¼‰

```
2ï¸âƒ£ éšæœºæ£®æ—ï¼ˆensemble baselineï¼‰
   +
   æ›´å¤šç‰¹å¾å·¥ç¨‹
   +
   è¶…å‚æ•°è°ƒä¼˜
   â†“
   æ•ˆæœæå‡ â†’ ä½¿ç”¨ä¼˜åŒ–åçš„æ¨¡å‹
   æ•ˆæœå¡ä½ â†’ ä¸‹ä¸€æ­¥
```

### ç¬¬ä¸‰é˜¶æ®µï¼šæ·±åº¦å­¦ä¹ å°è¯•ï¼ˆ1-2å‘¨ï¼‰

```
3ï¸âƒ£ LSTM / Transformer
   ï¼ˆå¦‚æœæœ‰æ›´å¤šæ•°æ®ã€GPUèµ„æºï¼‰
   â†“
   æ•ˆæœæ˜¾è‘—æå‡ â†’ è€ƒè™‘ä½¿ç”¨
   æå‡ä¸æ˜æ˜¾ â†’ å›åˆ°æ ‘æ¨¡å‹
```

---

## ğŸ“Š å®éªŒå¯¹æ¯”è®¾è®¡

### å®Œæ•´å®éªŒçŸ©é˜µ

| æ¨¡å‹ | è´Ÿæ ·æœ¬æ–¹æ¡ˆ | é¢„æœŸå‡†ç¡®ç‡ | è®­ç»ƒæ—¶é—´ | å¤æ‚åº¦ |
|------|-----------|-----------|---------|--------|
| XGBoost | V1 | 85-92% | 5åˆ†é’Ÿ | ç®€å• |
| XGBoost | V2 | 83-90% | 5åˆ†é’Ÿ | ç®€å• |
| LightGBM | V1 | 85-92% | 3åˆ†é’Ÿ | ç®€å• |
| LightGBM | V2 | 83-90% | 3åˆ†é’Ÿ | ç®€å• |
| Random Forest | V2 | 80-88% | 10åˆ†é’Ÿ | ç®€å• |
| LSTM | V2 | 80-90% | 1å°æ—¶ | å¤æ‚ |
| MLP | V2 | 78-86% | 20åˆ†é’Ÿ | ä¸­ç­‰ |

### æ¨èå®éªŒæµç¨‹

```bash
# å®éªŒ1: XGBoost + V2è´Ÿæ ·æœ¬ï¼ˆæœ€å¿«baselineï¼‰
python scripts/train_model.py --model xgboost --neg_version v2

# å®éªŒ2: XGBoost + V1è´Ÿæ ·æœ¬ï¼ˆæ›´éš¾è´Ÿæ ·æœ¬ï¼‰
python scripts/train_model.py --model xgboost --neg_version v1

# å®éªŒ3: LightGBM + V2è´Ÿæ ·æœ¬ï¼ˆå¯¹æ¯”XGBoostï¼‰
python scripts/train_model.py --model lightgbm --neg_version v2

# å®éªŒ4: LSTMï¼ˆå¦‚æœå‰é¢æ•ˆæœä¸ç†æƒ³ï¼‰
python scripts/train_model.py --model lstm --neg_version v2
```

---

## ğŸ’¡ ç‰¹å¾å·¥ç¨‹å»ºè®®

### æ—¶é—´åºåˆ—ç‰¹å¾è½¬æ¢

å¯¹äº34å¤©çš„æ—¶åºæ•°æ®ï¼Œå¯ä»¥æå–ä»¥ä¸‹ç»Ÿè®¡ç‰¹å¾ï¼š

#### 1. ä»·æ ¼ç›¸å…³
```python
- close_mean, close_std, close_max, close_min
- close_trend (é¦–å°¾å·®/é¦–å€¼)
- close_volatility (æ ‡å‡†å·®/å‡å€¼)
```

#### 2. æ¶¨è·Œå¹…
```python
- pct_chg_mean, pct_chg_std, pct_chg_sum
- positive_days_count, negative_days_count
- max_gain, max_loss
- consecutive_positive_days (æœ€é•¿è¿ç»­ä¸Šæ¶¨å¤©æ•°)
```

#### 3. æŠ€æœ¯æŒ‡æ ‡
```python
- macd_mean, macd_positive_days
- rsi_mean, rsi_overbought_days (>70)
- volume_ratio_mean, volume_ratio_gt_2_count
- price_above_ma5_days, price_above_ma10_days
```

#### 4. åŠ¨é‡ç‰¹å¾
```python
- return_1w (æœ€å1å‘¨æ”¶ç›Š)
- return_2w (æœ€å2å‘¨æ”¶ç›Š)
- return_4w (å…¨éƒ¨4å‘¨æ”¶ç›Š)
- acceleration (æ”¶ç›Šç‡çš„å˜åŒ–ç‡)
```

---

## ğŸ”§ è°ƒå‚å»ºè®®

### XGBoost å…³é”®å‚æ•°

```python
xgb.XGBClassifier(
    n_estimators=100,        # æ ‘çš„æ•°é‡ï¼Œå…ˆä»100å¼€å§‹
    max_depth=5,             # æ ‘æ·±åº¦ï¼Œ5-7æ¯”è¾ƒå¥½
    learning_rate=0.1,       # å­¦ä¹ ç‡ï¼Œ0.01-0.1
    subsample=0.8,           # æ ·æœ¬é‡‡æ ·æ¯”ä¾‹
    colsample_bytree=0.8,    # ç‰¹å¾é‡‡æ ·æ¯”ä¾‹
    min_child_weight=3,      # æœ€å°å­èŠ‚ç‚¹æƒé‡
    gamma=0.1,               # åˆ†è£‚æœ€å°æŸå¤±å‡å°‘
    reg_alpha=0.1,           # L1æ­£åˆ™
    reg_lambda=1.0,          # L2æ­£åˆ™
    scale_pos_weight=1,      # æ­£è´Ÿæ ·æœ¬æƒé‡ï¼ˆå¦‚æœä¸å¹³è¡¡ï¼‰
)
```

### LSTM å…³é”®å‚æ•°

```python
- å±‚æ•°: 2-3å±‚
- å•å…ƒæ•°: 64 â†’ 32ï¼ˆé€’å‡ï¼‰
- Dropout: 0.2-0.5
- Batch size: 32-64
- Epochs: 50-100 (ç”¨early stopping)
- Optimizer: Adam (lr=0.001)
```

---

## âœ… è¯„ä¼°æŒ‡æ ‡

### å¿…çœ‹æŒ‡æ ‡

| æŒ‡æ ‡ | å…¬å¼ | é‡è¦æ€§ | è¯´æ˜ |
|------|------|--------|------|
| **Accuracy** | (TP+TN)/(P+N) | â­â­â­ | æ•´ä½“å‡†ç¡®ç‡ |
| **Precision** | TP/(TP+FP) | â­â­â­â­â­ | é¢„æµ‹ä¸ºç‰›è‚¡çš„å‡†ç¡®ç‡ |
| **Recall** | TP/(TP+FN) | â­â­â­â­â­ | çœŸç‰›è‚¡è¢«æ‰¾å‡ºçš„æ¯”ä¾‹ |
| **F1-Score** | 2Ã—PÃ—R/(P+R) | â­â­â­â­â­ | ç»¼åˆæŒ‡æ ‡ |
| **AUC-ROC** | - | â­â­â­â­ | åˆ†ç±»èƒ½åŠ› |

### ä¸šåŠ¡æŒ‡æ ‡

- **Top-Kå‡†ç¡®ç‡**: æ¨¡å‹é¢„æµ‹æ¦‚ç‡æœ€é«˜çš„Kåªè‚¡ç¥¨ä¸­ï¼Œå®é™…ç‰›è‚¡çš„å æ¯”
- **å›æµ‹æ”¶ç›Š**: åŸºäºæ¨¡å‹é€‰è‚¡çš„å®é™…æ”¶ç›Š
- **å¤æ™®æ¯”ç‡**: æ”¶ç›Š/æ³¢åŠ¨ç‡

---

## ğŸ“ æœ€ç»ˆå»ºè®®

### ğŸ¯ é’ˆå¯¹ä½ çš„é¡¹ç›®

**å¼ºçƒˆæ¨èï¼šXGBoost/LightGBM** âœ…

ç†ç”±ï¼š
1. âœ… ä½ çš„æ•°æ®å·²ç»æ˜¯è¡¨æ ¼å‹ç‰¹å¾ï¼ˆMACDã€RSIç­‰ï¼‰
2. âœ… æ ·æœ¬é‡é€‚ä¸­ï¼ˆ2300ä¸ªï¼‰
3. âœ… è®­ç»ƒå¿«é€Ÿï¼ˆ5åˆ†é’Ÿï¼‰
4. âœ… æ•ˆæœå¥½ï¼ˆé‡‘èæ•°æ®ä¸Šprovenï¼‰
5. âœ… å¯è§£é‡Šï¼ˆçŸ¥é“å“ªäº›æŒ‡æ ‡é‡è¦ï¼‰
6. âœ… ç”Ÿäº§éƒ¨ç½²ç®€å•

**ä¸æ¨èï¼šLSTM** âŒï¼ˆè‡³å°‘æš‚æ—¶ä¸æ¨èï¼‰

ç†ç”±ï¼š
1. âŒ æ ·æœ¬é‡åå°‘ï¼ˆLSTMæ›´é€‚åˆ5000+ï¼‰
2. âŒ ä½ çš„ç‰¹å¾å·²ç»å¾ˆå¥½ï¼ˆæ‰‹å·¥ç‰¹å¾å·¥ç¨‹è´¨é‡é«˜ï¼‰
3. âŒ è®­ç»ƒæ…¢ï¼Œè°ƒå‚å¤æ‚
4. âŒ å®¹æ˜“è¿‡æ‹Ÿåˆ
5. âŒ é»‘ç›’ï¼Œä¸å¯è§£é‡Š

### ğŸš€ è¡ŒåŠ¨è®¡åˆ’

```
ç¬¬1å¤©: 
  - å‡†å¤‡æ•°æ®ï¼ˆç‰¹å¾å±•å¹³ï¼‰
  - è®­ç»ƒXGBoost baseline
  - æŸ¥çœ‹æ•ˆæœå’Œç‰¹å¾é‡è¦æ€§
  
ç¬¬2å¤©:
  - ä¼˜åŒ–ç‰¹å¾å·¥ç¨‹
  - è°ƒæ•´è¶…å‚æ•°
  - å¯¹æ¯”V1å’ŒV2è´Ÿæ ·æœ¬
  
ç¬¬3-5å¤©:
  - å°è¯•LightGBMã€Random Forest
  - Ensembleå¤šä¸ªæ¨¡å‹
  - å›æµ‹éªŒè¯
  
å¦‚æœæ•ˆæœå¡ä½ï¼ˆ1-2å‘¨åï¼‰:
  - å†è€ƒè™‘LSTM
  - æˆ–è€…æ”¶é›†æ›´å¤šæ•°æ®
```

---

## ğŸ“š ç›¸å…³èµ„æº

### å­¦ä¹ èµ„æ–™
- [XGBoostå®˜æ–¹æ–‡æ¡£](https://xgboost.readthedocs.io/)
- [LightGBMå®˜æ–¹æ–‡æ¡£](https://lightgbm.readthedocs.io/)
- [Scikit-learnç”¨æˆ·æŒ‡å—](https://scikit-learn.org/stable/user_guide.html)

### ç«èµ›æ¡ˆä¾‹
- Kaggleé‡‘èç«èµ›ï¼šå‡ ä¹éƒ½ç”¨XGBoost/LightGBM
- é‡åŒ–äº¤æ˜“ï¼šæ ‘æ¨¡å‹ > æ·±åº¦å­¦ä¹ ï¼ˆåœ¨è¡¨æ ¼æ•°æ®ä¸Šï¼‰

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0  
**åˆ›å»ºæ—¶é—´**: 2024-12-23  
**æœ€åæ›´æ–°**: 2024-12-23

