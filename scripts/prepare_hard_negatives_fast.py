"""
ç¡¬è´Ÿæ ·æœ¬å¿«é€Ÿå‡†å¤‡è„šæœ¬

ä»å·²æœ‰çš„è´Ÿæ ·æœ¬æ•°æ®ä¸­ç­›é€‰"ç¡¬è´Ÿæ ·æœ¬"ï¼š
- è®¡ç®—æ¯ä¸ªè´Ÿæ ·æœ¬çš„34æ—¥æ¶¨å¹…
- ç­›é€‰æ¶¨å¹…åœ¨20-45%ä¹‹é—´çš„æ ·æœ¬ä½œä¸ºç¡¬è´Ÿæ ·æœ¬
- æ— éœ€é¢å¤–APIè°ƒç”¨ï¼Œé€Ÿåº¦å¿«

ä½¿ç”¨æ–¹æ³•ï¼š
    python scripts/prepare_hard_negatives_fast.py
"""
import sys
import os
import warnings
import pandas as pd
import numpy as np
import json
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

# å¿½ç•¥FutureWarning
warnings.filterwarnings('ignore', category=FutureWarning)

from src.utils.logger import log


def calculate_return_34d(df_sample):
    """è®¡ç®—å•ä¸ªæ ·æœ¬çš„34æ—¥æ¶¨å¹…"""
    if len(df_sample) < 20:
        return None
    
    df_sorted = df_sample.sort_values('days_to_t1')
    start_price = df_sorted.iloc[0]['close']
    end_price = df_sorted.iloc[-1]['close']
    
    if start_price <= 0:
        return None
    
    return (end_price - start_price) / start_price * 100


def main():
    """ä¸»å‡½æ•°"""
    log.info("="*80)
    log.info("ç¡¬è´Ÿæ ·æœ¬å¿«é€Ÿå‡†å¤‡ï¼ˆä»å·²æœ‰æ•°æ®ç­›é€‰ï¼‰")
    log.info("="*80)
    
    # é…ç½®å‚æ•°
    MIN_RETURN = 20.0          # æœ€å°34æ—¥æ¶¨å¹…
    MAX_RETURN = 45.0          # æœ€å¤§34æ—¥æ¶¨å¹…ï¼ˆä½äºæ­£æ ·æœ¬çš„50%ï¼‰
    
    NEG_FEATURES_FILE = 'data/training/features/negative_feature_data_v2_34d.csv'
    OUTPUT_HARD_FEATURES = 'data/training/features/hard_negative_feature_data_34d.csv'
    OUTPUT_STATS = 'data/training/samples/hard_negative_statistics.json'
    
    log.info(f"\nå½“å‰è®¾ç½®ï¼š")
    log.info(f"  æ–¹æ³•: ä»å·²æœ‰è´Ÿæ ·æœ¬ä¸­ç­›é€‰ï¼ˆ34æ—¥æ¶¨å¹…{MIN_RETURN}%-{MAX_RETURN}%ï¼‰")
    log.info(f"  è´Ÿæ ·æœ¬ç‰¹å¾æ–‡ä»¶: {NEG_FEATURES_FILE}")
    log.info("")
    
    # 1. åŠ è½½è´Ÿæ ·æœ¬ç‰¹å¾æ•°æ®
    log.info("="*80)
    log.info("ç¬¬ä¸€æ­¥ï¼šåŠ è½½è´Ÿæ ·æœ¬ç‰¹å¾æ•°æ®")
    log.info("="*80)
    
    try:
        df_neg = pd.read_csv(NEG_FEATURES_FILE)
        log.success(f"âœ“ è´Ÿæ ·æœ¬åŠ è½½æˆåŠŸ: {len(df_neg)} æ¡è®°å½•")
        log.info(f"  æ ·æœ¬æ•°: {df_neg['sample_id'].nunique()}")
    except Exception as e:
        log.error(f"âœ— åŠ è½½è´Ÿæ ·æœ¬æ•°æ®å¤±è´¥: {e}")
        return
    
    # 2. è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„34æ—¥æ¶¨å¹…
    log.info("\n" + "="*80)
    log.info("ç¬¬äºŒæ­¥ï¼šè®¡ç®—34æ—¥æ¶¨å¹…")
    log.info("="*80)
    
    sample_returns = []
    sample_ids = df_neg['sample_id'].unique()
    
    for i, sample_id in enumerate(sample_ids):
        if (i + 1) % 500 == 0:
            log.info(f"è¿›åº¦: {i+1}/{len(sample_ids)}")
        
        sample_data = df_neg[df_neg['sample_id'] == sample_id]
        return_34d = calculate_return_34d(sample_data)
        
        if return_34d is not None:
            sample_returns.append({
                'sample_id': sample_id,
                'return_34d': return_34d,
                'ts_code': sample_data['ts_code'].iloc[0],
                'name': sample_data['name'].iloc[0] if 'name' in sample_data.columns else ''
            })
    
    df_returns = pd.DataFrame(sample_returns)
    log.success(f"âœ“ æ¶¨å¹…è®¡ç®—å®Œæˆ: {len(df_returns)} ä¸ªæ ·æœ¬")
    
    # 3. ç­›é€‰ç¡¬è´Ÿæ ·æœ¬
    log.info("\n" + "="*80)
    log.info("ç¬¬ä¸‰æ­¥ï¼šç­›é€‰ç¡¬è´Ÿæ ·æœ¬")
    log.info("="*80)
    
    # æ˜¾ç¤ºæ¶¨å¹…åˆ†å¸ƒ
    log.info(f"\næ‰€æœ‰è´Ÿæ ·æœ¬34æ—¥æ¶¨å¹…åˆ†å¸ƒ:")
    log.info(f"  å‡å€¼: {df_returns['return_34d'].mean():.2f}%")
    log.info(f"  ä¸­ä½æ•°: {df_returns['return_34d'].median():.2f}%")
    log.info(f"  æœ€å°: {df_returns['return_34d'].min():.2f}%")
    log.info(f"  æœ€å¤§: {df_returns['return_34d'].max():.2f}%")
    
    # ç»Ÿè®¡å„åŒºé—´æ ·æœ¬æ•°
    log.info(f"\næ¶¨å¹…åŒºé—´åˆ†å¸ƒ:")
    bins = [-100, -20, 0, 10, 20, 30, 40, 50, 100, 500]
    for i in range(len(bins)-1):
        count = len(df_returns[(df_returns['return_34d'] >= bins[i]) & (df_returns['return_34d'] < bins[i+1])])
        log.info(f"  {bins[i]}% ~ {bins[i+1]}%: {count} ä¸ª ({count/len(df_returns)*100:.1f}%)")
    
    # ç­›é€‰ç›®æ ‡èŒƒå›´
    hard_mask = (df_returns['return_34d'] >= MIN_RETURN) & (df_returns['return_34d'] <= MAX_RETURN)
    df_hard_returns = df_returns[hard_mask]
    
    log.info(f"\nç­›é€‰ç»“æœï¼ˆ{MIN_RETURN}% - {MAX_RETURN}%ï¼‰:")
    log.info(f"  ç¡¬è´Ÿæ ·æœ¬æ•°: {len(df_hard_returns)} ä¸ª")
    log.info(f"  å æ¯”: {len(df_hard_returns)/len(df_returns)*100:.1f}%")
    
    if len(df_hard_returns) == 0:
        log.warning("âš ï¸  æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„ç¡¬è´Ÿæ ·æœ¬")
        log.info("å°è¯•æ‰©å¤§æ¶¨å¹…èŒƒå›´...")
        
        # æ‰©å¤§èŒƒå›´
        MIN_RETURN_EXT = 15.0
        MAX_RETURN_EXT = 48.0
        hard_mask = (df_returns['return_34d'] >= MIN_RETURN_EXT) & (df_returns['return_34d'] <= MAX_RETURN_EXT)
        df_hard_returns = df_returns[hard_mask]
        
        log.info(f"æ‰©å¤§èŒƒå›´ï¼ˆ{MIN_RETURN_EXT}% - {MAX_RETURN_EXT}%ï¼‰:")
        log.info(f"  ç¡¬è´Ÿæ ·æœ¬æ•°: {len(df_hard_returns)} ä¸ª")
    
    if len(df_hard_returns) == 0:
        log.error("âœ— ä»æœªæ‰¾åˆ°ç¡¬è´Ÿæ ·æœ¬ï¼Œè¯·æ£€æŸ¥æ•°æ®")
        return
    
    # 4. æå–ç¡¬è´Ÿæ ·æœ¬ç‰¹å¾
    log.info("\n" + "="*80)
    log.info("ç¬¬å››æ­¥ï¼šæå–ç¡¬è´Ÿæ ·æœ¬ç‰¹å¾")
    log.info("="*80)
    
    hard_sample_ids = set(df_hard_returns['sample_id'].tolist())
    df_hard_features = df_neg[df_neg['sample_id'].isin(hard_sample_ids)].copy()
    
    # æ·»åŠ return_34dä¿¡æ¯
    return_map = dict(zip(df_hard_returns['sample_id'], df_hard_returns['return_34d']))
    df_hard_features['return_34d'] = df_hard_features['sample_id'].map(return_map)
    
    log.success(f"âœ“ ç¡¬è´Ÿæ ·æœ¬ç‰¹å¾æå–å®Œæˆ: {len(df_hard_features)} æ¡è®°å½•")
    log.info(f"  æ ·æœ¬æ•°: {df_hard_features['sample_id'].nunique()}")
    
    # 5. ä¿å­˜ç»“æœ
    log.info("\n" + "="*80)
    log.info("ç¬¬äº”æ­¥ï¼šä¿å­˜ç»“æœ")
    log.info("="*80)
    
    # ä¿å­˜ç¡¬è´Ÿæ ·æœ¬ç‰¹å¾æ•°æ®
    df_hard_features.to_csv(OUTPUT_HARD_FEATURES, index=False)
    log.success(f"âœ“ ç¡¬è´Ÿæ ·æœ¬ç‰¹å¾æ•°æ®å·²ä¿å­˜: {OUTPUT_HARD_FEATURES}")
    
    # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
    stats = {
        'generation_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'method': 'ä»å·²æœ‰è´Ÿæ ·æœ¬ç­›é€‰ï¼ˆå¿«é€Ÿæ–¹æ³•ï¼‰',
        'total_negative_samples': len(df_returns),
        'hard_negative_samples': len(df_hard_returns),
        'hard_negative_ratio': round(len(df_hard_returns) / len(df_returns) * 100, 2),
        'hard_feature_records': len(df_hard_features),
        'return_range': {
            'min': MIN_RETURN,
            'max': MAX_RETURN
        },
        'return_statistics': {
            'mean': round(df_hard_returns['return_34d'].mean(), 2),
            'median': round(df_hard_returns['return_34d'].median(), 2),
            'min': round(df_hard_returns['return_34d'].min(), 2),
            'max': round(df_hard_returns['return_34d'].max(), 2),
        },
        'files': {
            'hard_features': OUTPUT_HARD_FEATURES,
            'source_features': NEG_FEATURES_FILE
        }
    }
    
    with open(OUTPUT_STATS, 'w', encoding='utf-8') as f:
        json.dump(stats, f, indent=2, ensure_ascii=False)
    
    log.success(f"âœ“ ç»Ÿè®¡æŠ¥å‘Šå·²ä¿å­˜: {OUTPUT_STATS}")
    
    # 6. æœ€ç»ˆæ€»ç»“
    log.info("\n" + "="*80)
    log.success("âœ… ç¡¬è´Ÿæ ·æœ¬å¿«é€Ÿå‡†å¤‡å®Œæˆï¼")
    log.info("="*80)
    log.info("")
    log.info("ğŸ“Š æ•°æ®ç»Ÿè®¡ï¼š")
    log.info(f"  åŸè´Ÿæ ·æœ¬æ•°: {len(df_returns)}")
    log.info(f"  ç¡¬è´Ÿæ ·æœ¬æ•°: {len(df_hard_returns)} ({len(df_hard_returns)/len(df_returns)*100:.1f}%)")
    log.info(f"  ç¡¬è´Ÿæ ·æœ¬ç‰¹å¾: {len(df_hard_features)} æ¡")
    log.info(f"\nğŸ“ˆ 34æ—¥æ¶¨å¹…åˆ†å¸ƒï¼š")
    log.info(f"  å‡å€¼: {df_hard_returns['return_34d'].mean():.2f}%")
    log.info(f"  ä¸­ä½æ•°: {df_hard_returns['return_34d'].median():.2f}%")
    log.info("")
    log.info("ä¸‹ä¸€æ­¥ï¼š")
    log.info("  è¿è¡Œ python scripts/train_optimized_model.py è®­ç»ƒä¼˜åŒ–ç‰ˆæ¨¡å‹")
    log.info("")


if __name__ == '__main__':
    main()

