#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
V2.1.0æ¨¡å‹è¯„ä¼° - å¸¦é£é™©è¿‡æ»¤åå¤„ç†
é¢„æµ‹12æœˆ12æ—¥ï¼Œç”¨12æœˆ31æ—¥è¯„ä»·

é£é™©è¿‡æ»¤è§„åˆ™ï¼š
1. æ’é™¤34æ—¥æ¶¨å¹… > 50% çš„è‚¡ç¥¨ï¼ˆå·²è§é¡¶é£é™©ï¼‰
2. æ’é™¤æ³¢åŠ¨ç‡è¿‡é«˜çš„è‚¡ç¥¨ï¼ˆæ³¢åŠ¨ç‡ > å†å²å‡å€¼2å€ï¼‰
3. æ’é™¤è¿‘5æ—¥è¿ç»­ä¸‹è·Œçš„è‚¡ç¥¨
4. æ’é™¤è·ç¦»å†å²é«˜ç‚¹è¿‡è¿‘çš„è‚¡ç¥¨ï¼ˆå¯èƒ½é‡é˜»ï¼‰
"""

import sys
import json
import argparse
import warnings
from pathlib import Path
from datetime import datetime, timedelta

import pandas as pd
import numpy as np
import xgboost as xgb

# æŠ‘åˆ¶è­¦å‘Š
warnings.filterwarnings('ignore', category=pd.errors.PerformanceWarning)
warnings.filterwarnings('ignore', category=FutureWarning)

# æ·»åŠ é¡¹ç›®è·¯å¾„
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

from src.utils.logger import log
from src.data.data_manager import DataManager


def load_model(version):
    """åŠ è½½æ¨¡å‹"""
    model_dir = PROJECT_ROOT / 'data' / 'models' / 'breakout_launch_scorer' / 'versions' / version / 'model'
    model_file = model_dir / 'model.json'
    feature_file = model_dir / 'feature_names.json'
    
    booster = xgb.Booster()
    booster.load_model(str(model_file))
    
    with open(feature_file, 'r') as f:
        feature_names = json.load(f)
    
    return booster, feature_names


def get_valid_stock_list(dm):
    """è·å–æœ‰æ•ˆè‚¡ç¥¨åˆ—è¡¨"""
    stock_list = dm.get_stock_list()
    
    # è¿‡æ»¤STã€é€€å¸‚ã€ç§‘åˆ›æ¿ã€åŒ—äº¤æ‰€
    valid = stock_list[
        ~stock_list['name'].str.contains('ST|é€€', na=False) &
        ~stock_list['ts_code'].str.startswith('688') &
        ~stock_list['ts_code'].str.startswith('8')
    ]
    
    return valid


def get_market_data(dm, predict_date):
    """è·å–å¸‚åœºæ•°æ®"""
    end_date = predict_date
    start_date = (datetime.strptime(predict_date, '%Y%m%d') - timedelta(days=100)).strftime('%Y%m%d')
    
    df = dm.get_index_daily('000001.SH', start_date, end_date)
    if df is not None and len(df) > 0:
        df = df.sort_values('trade_date')
    return df


def get_stock_data(dm, ts_code, predict_date, df_market):
    """è·å–å•åªè‚¡ç¥¨æ•°æ®å¹¶è®¡ç®—ç‰¹å¾"""
    try:
        end_date = predict_date
        start_date = (datetime.strptime(predict_date, '%Y%m%d') - timedelta(days=200)).strftime('%Y%m%d')
        
        df = dm.get_daily_data(ts_code, start_date, end_date)
        if df is None or len(df) < 60:
            return None
        
        df = df.sort_values('trade_date').reset_index(drop=True)
        
        # åŸºç¡€æŠ€æœ¯æŒ‡æ ‡
        df['ma5'] = df['close'].rolling(5).mean()
        df['ma10'] = df['close'].rolling(10).mean()
        df['ma_20d'] = df['close'].rolling(20).mean()
        
        # MACD
        df['ema12'] = df['close'].ewm(span=12, adjust=False).mean()
        df['ema26'] = df['close'].ewm(span=26, adjust=False).mean()
        df['macd_dif'] = df['ema12'] - df['ema26']
        df['macd_dea'] = df['macd_dif'].ewm(span=9, adjust=False).mean()
        df['macd'] = 2 * (df['macd_dif'] - df['macd_dea'])
        
        # RSI
        delta = df['close'].diff()
        gain = delta.where(delta > 0, 0).rolling(6).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(6).mean()
        df['rsi_6'] = 100 - (100 / (1 + gain / (loss + 1e-10)))
        
        gain12 = delta.where(delta > 0, 0).rolling(12).mean()
        loss12 = (-delta.where(delta < 0, 0)).rolling(12).mean()
        df['rsi_12'] = 100 - (100 / (1 + gain12 / (loss12 + 1e-10)))
        
        gain24 = delta.where(delta > 0, 0).rolling(24).mean()
        loss24 = (-delta.where(delta < 0, 0)).rolling(24).mean()
        df['rsi_24'] = 100 - (100 / (1 + gain24 / (loss24 + 1e-10)))
        
        # KDJ
        low_9 = df['low'].rolling(9).min()
        high_9 = df['high'].rolling(9).max()
        rsv = (df['close'] - low_9) / (high_9 - low_9 + 1e-10) * 100
        df['kdj_k'] = rsv.ewm(com=2, adjust=False).mean()
        df['kdj_d'] = df['kdj_k'].ewm(com=2, adjust=False).mean()
        df['kdj_j'] = 3 * df['kdj_k'] - 2 * df['kdj_d']
        
        # OBV
        df['obv'] = (np.sign(df['close'].diff()) * df['vol']).fillna(0).cumsum()
        
        # é‡æ¯”
        df['volume_ratio'] = df['vol'] / (df['vol'].rolling(5).mean() + 1e-8)
        
        # å¸‚åœºæ•°æ®
        if df_market is not None and len(df_market) > 0:
            market_dict = df_market.set_index('trade_date')['pct_chg'].to_dict()
            df['market_pct_chg'] = df['trade_date'].map(market_dict).fillna(0)
            df['market_return_34d'] = df['market_pct_chg'].rolling(34).sum()
            df['market_volatility_34d'] = df['market_pct_chg'].rolling(34).std()
            df['market_trend'] = (df['market_pct_chg'].rolling(10).mean() > 0).astype(int)
            df['excess_return'] = df['pct_chg'] - df['market_pct_chg']
            df['excess_return_cumsum'] = df['excess_return'].rolling(34).sum()
        else:
            df['market_pct_chg'] = 0
            df['market_return_34d'] = 0
            df['market_volatility_34d'] = 0
            df['market_trend'] = 0
            df['excess_return'] = df['pct_chg']
            df['excess_return_cumsum'] = df['pct_chg'].rolling(34).sum()
        
        # å¤šå‘¨æœŸç‰¹å¾
        for period in [8, 34, 55]:
            df[f'return_{period}d'] = df['close'].pct_change(period) * 100
            df[f'ma_{period}d'] = df['close'].rolling(period).mean()
            df[f'price_vs_ma_{period}d'] = (df['close'] - df[f'ma_{period}d']) / df[f'ma_{period}d'] * 100
            df[f'volatility_{period}d'] = df['pct_chg'].rolling(period).std()
            df[f'high_{period}d'] = df['high'].rolling(period).max()
            df[f'low_{period}d'] = df['low'].rolling(period).min()
            price_range = df[f'high_{period}d'] - df[f'low_{period}d']
            df[f'price_position_{period}d'] = (df['close'] - df[f'low_{period}d']) / (price_range + 1e-10)
            df[f'trend_slope_{period}d'] = df['close'].rolling(period).apply(
                lambda x: np.polyfit(range(len(x)), x, 1)[0] if len(x) == period else 0, raw=False
            )
        
        # åŠ¨é‡ç‰¹å¾
        df['momentum_5d'] = df['close'].pct_change(5) * 100
        df['momentum_10d'] = df['close'].pct_change(10) * 100
        df['momentum_20d'] = df['close'].pct_change(20) * 100
        df['momentum_acceleration'] = df['momentum_5d'] - df['momentum_5d'].shift(5)
        
        # ä»·é‡ç‰¹å¾
        df['price_change'] = df['close'].diff()
        df['volume_change'] = df['vol'].diff()
        df['volume_price_corr_10d'] = df['close'].rolling(10).corr(df['vol'])
        df['volume_price_corr_20d'] = df['close'].rolling(20).corr(df['vol'])
        df['volume_price_match'] = ((df['price_change'] > 0) & (df['volume_change'] > 0)).astype(int)
        df['volume_price_match_sum_10d'] = df['volume_price_match'].rolling(10).sum()
        
        # çªç ´ç‰¹å¾
        for period in [10, 20, 55]:
            df[f'prev_high_{period}d'] = df['high'].rolling(period).max().shift(1)
            df[f'breakout_high_{period}d'] = (df['close'] > df[f'prev_high_{period}d']).astype(int)
        
        # MAçªç ´
        df['ma_5d'] = df['close'].rolling(5).mean()
        df['breakout_ma5'] = (df['close'] > df['ma_5d']).astype(int)
        df['ma_10d'] = df['close'].rolling(10).mean()
        df['breakout_ma10'] = (df['close'] > df['ma_10d']).astype(int)
        df['breakout_ma20'] = (df['close'] > df['ma_20d']).astype(int)
        ma_55d = df['close'].rolling(55).mean()
        df['breakout_ma55'] = (df['close'] > ma_55d).astype(int)
        
        # æ”¾é‡çªç ´
        df['breakout_volume_ratio'] = df['vol'] / (df['vol'].rolling(20).mean() + 1e-8)
        df['high_volume_breakout'] = ((df['breakout_high_20d'] == 1) & (df['breakout_volume_ratio'] > 1.5)).astype(int)
        
        # è¿ç»­æ–°é«˜
        df['consecutive_new_high'] = df['breakout_high_10d'].rolling(5).sum()
        
        # æ”¯æ’‘å‹åŠ›ä½
        for period in [10, 20, 55]:
            df[f'resistance_{period}d'] = df['high'].rolling(period).max()
            df[f'support_{period}d'] = df['low'].rolling(period).min()
            df[f'dist_to_resistance_{period}d'] = (df[f'resistance_{period}d'] - df['close']) / df['close'] * 100
            df[f'dist_to_support_{period}d'] = (df['close'] - df[f'support_{period}d']) / df['close'] * 100
            df[f'support_strength_{period}d'] = (df['low'] - df[f'support_{period}d']).abs().rolling(period).mean()
            df[f'resistance_strength_{period}d'] = (df[f'resistance_{period}d'] - df['high']).abs().rolling(period).mean()
        
        df['channel_width_20d'] = (df['resistance_20d'] - df['support_20d']) / df['close'] * 100
        
        # æˆäº¤é‡ç‰¹å¾
        df['volume_trend_slope_10d'] = df['vol'].rolling(10).apply(
            lambda x: np.polyfit(range(len(x)), x, 1)[0] if len(x) == 10 else 0, raw=False
        )
        df['volume_trend_slope_20d'] = df['vol'].rolling(20).apply(
            lambda x: np.polyfit(range(len(x)), x, 1)[0] if len(x) == 20 else 0, raw=False
        )
        df['volume_breakout_count_20d'] = (df['vol'] > df['vol'].rolling(20).mean() * 1.5).rolling(20).sum()
        
        # é‡ä»·èƒŒç¦»
        df['price_up_vol_down'] = ((df['price_change'] > 0) & (df['volume_change'] < 0)).astype(int)
        df['price_up_vol_down_count_10d'] = df['price_up_vol_down'].rolling(10).sum()
        df['price_down_vol_up'] = ((df['price_change'] < 0) & (df['volume_change'] > 0)).astype(int)
        df['price_down_vol_up_count_10d'] = df['price_down_vol_up'].rolling(10).sum()
        
        # æˆäº¤é‡RSV
        vol_low_20 = df['vol'].rolling(20).min()
        vol_high_20 = df['vol'].rolling(20).max()
        df['volume_rsv_20d'] = (df['vol'] - vol_low_20) / (vol_high_20 - vol_low_20 + 1e-10) * 100
        
        # OBVè®¡ç®—
        df['obv_calc'] = (np.sign(df['close'].diff()) * df['vol']).fillna(0).cumsum()
        df['obv_ma10'] = df['obv_calc'].rolling(10).mean()
        df['obv_trend'] = (df['obv_calc'] > df['obv_ma10']).astype(int)
        
        # ä¹–ç¦»ç‡
        if 'ma5' in df.columns:
            df['bias_short'] = (df['close'] - df['ma5']) / df['ma5'] * 100
        if 'ma10' in df.columns:
            df['bias_mid'] = (df['close'] - df['ma10']) / df['ma10'] * 100
        if 'ma_20d' in df.columns:
            df['bias_long'] = (df['close'] - df['ma_20d']) / df['ma_20d'] * 100
        
        # EMA
        df['ema_5'] = df['close'].ewm(span=5, adjust=False).mean()
        df['ema_10'] = df['close'].ewm(span=10, adjust=False).mean()
        df['ema_20'] = df['close'].ewm(span=20, adjust=False).mean()
        df['ema_60'] = df['close'].ewm(span=60, adjust=False).mean()
        
        # vol_maæ¯”ç‡
        if 'vol' in df.columns:
            df['vol_ma5_ratio'] = df['vol'] / (df['vol'].rolling(5).mean() + 1e-8)
            df['vol_ma20_ratio'] = df['vol'] / (df['vol'].rolling(20).mean() + 1e-8)
        
        # æ¶¨åœæ ‡è®°
        df['is_limit_up'] = (df['pct_chg'] >= 9.8).astype(int)
        
        # å†å²ä»·æ ¼ä½ç½®
        if len(df) >= 34:
            df['price_vs_hist_mean'] = (df['close'] - df['close'].rolling(34).mean()) / df['close'].rolling(34).mean() * 100
            df['price_vs_hist_high'] = (df['close'] - df['close'].rolling(34).max()) / df['close'].rolling(34).max() * 100
            df['volatility_vs_hist'] = df['pct_chg'].rolling(10).std() / (df['pct_chg'].rolling(34).std() + 1e-8)
        
        # å–æœ€å34å¤©
        df = df.tail(34)
        
        return df
    
    except Exception as e:
        return None


def extract_features(df, feature_names):
    """ä»æ•°æ®ä¸­æå–ç‰¹å¾å‘é‡"""
    if df is None or len(df) < 20:
        return None
    
    features = {}
    last_row = df.iloc[-1]
    
    for fn in feature_names:
        if fn in last_row:
            val = last_row[fn]
            features[fn] = 0 if pd.isna(val) else val
        else:
            features[fn] = 0
    
    return features


def calculate_risk_metrics(df):
    """è®¡ç®—é£é™©æŒ‡æ ‡ç”¨äºè¿‡æ»¤"""
    if df is None or len(df) < 20:
        return None
    
    risk = {}
    
    # 34æ—¥æ¶¨å¹…
    if len(df) >= 34:
        risk['return_34d'] = (df['close'].iloc[-1] / df['close'].iloc[0] - 1) * 100
    else:
        risk['return_34d'] = (df['close'].iloc[-1] / df['close'].iloc[0] - 1) * 100
    
    # æ³¢åŠ¨ç‡
    risk['volatility'] = df['pct_chg'].std()
    risk['volatility_mean'] = df['pct_chg'].rolling(20).std().mean()
    
    # è¿‘5æ—¥è¿ç»­ä¸‹è·Œ
    last_5_pct = df['pct_chg'].tail(5)
    risk['consecutive_down'] = (last_5_pct < 0).sum()
    risk['last_5_return'] = last_5_pct.sum()
    
    # è·ç¦»å†å²é«˜ç‚¹
    high_55d = df['high'].tail(55).max() if len(df) >= 55 else df['high'].max()
    risk['dist_to_hist_high'] = (high_55d - df['close'].iloc[-1]) / high_55d * 100
    
    # RSIè¶…ä¹°
    delta = df['close'].diff()
    gain = delta.where(delta > 0, 0).rolling(14).mean()
    loss = (-delta.where(delta < 0, 0)).rolling(14).mean()
    rsi = 100 - (100 / (1 + gain / (loss + 1e-10)))
    risk['rsi_14'] = rsi.iloc[-1] if not pd.isna(rsi.iloc[-1]) else 50
    
    # è¿‘æœŸæ¶¨åœæ¬¡æ•°ï¼ˆå¯èƒ½æ˜¯æ¸¸èµ„ç‚’ä½œï¼‰
    risk['limit_up_count_10d'] = (df['pct_chg'].tail(10) >= 9.8).sum()
    
    return risk


def apply_risk_filter(row, risk_metrics):
    """
    åº”ç”¨é£é™©è¿‡æ»¤è§„åˆ™ï¼Œè¿”å›é£é™©ç³»æ•° (0-1)
    ç³»æ•°è¶Šä½é£é™©è¶Šé«˜ï¼Œ0è¡¨ç¤ºåº”è¯¥æ’é™¤
    """
    if risk_metrics is None:
        return 0.5  # é»˜è®¤ä¸­ç­‰é£é™©
    
    risk_score = 1.0
    risk_reasons = []
    
    # è§„åˆ™1: 34æ—¥æ¶¨å¹…è¿‡å¤§ï¼ˆå·²è§é¡¶é£é™©ï¼‰
    return_34d = risk_metrics.get('return_34d', 0)
    if return_34d > 80:
        risk_score *= 0.3
        risk_reasons.append(f'34æ—¥æ¶¨å¹…è¿‡å¤§({return_34d:.1f}%)')
    elif return_34d > 60:
        risk_score *= 0.5
        risk_reasons.append(f'34æ—¥æ¶¨å¹…è¾ƒå¤§({return_34d:.1f}%)')
    elif return_34d > 40:
        risk_score *= 0.7
        risk_reasons.append(f'34æ—¥æ¶¨å¹…åé«˜({return_34d:.1f}%)')
    
    # è§„åˆ™2: æ³¢åŠ¨ç‡è¿‡é«˜
    volatility = risk_metrics.get('volatility', 0)
    vol_mean = risk_metrics.get('volatility_mean', volatility)
    if volatility > vol_mean * 2.5:
        risk_score *= 0.5
        risk_reasons.append(f'æ³¢åŠ¨ç‡è¿‡é«˜')
    elif volatility > vol_mean * 2:
        risk_score *= 0.7
        risk_reasons.append(f'æ³¢åŠ¨ç‡åé«˜')
    
    # è§„åˆ™3: è¿‘5æ—¥è¿ç»­ä¸‹è·Œ
    consecutive_down = risk_metrics.get('consecutive_down', 0)
    if consecutive_down >= 5:
        risk_score *= 0.4
        risk_reasons.append('è¿ç»­5æ—¥ä¸‹è·Œ')
    elif consecutive_down >= 4:
        risk_score *= 0.6
        risk_reasons.append('è¿‘5æ—¥å¤šæ•°ä¸‹è·Œ')
    
    # è§„åˆ™4: RSIè¶…ä¹°
    rsi = risk_metrics.get('rsi_14', 50)
    if rsi > 85:
        risk_score *= 0.5
        risk_reasons.append(f'RSIè¶…ä¹°({rsi:.1f})')
    elif rsi > 75:
        risk_score *= 0.7
        risk_reasons.append(f'RSIåé«˜({rsi:.1f})')
    
    # è§„åˆ™5: è¿‘æœŸå¤šæ¬¡æ¶¨åœï¼ˆæ¸¸èµ„ç‚’ä½œé£é™©ï¼‰
    limit_up_count = risk_metrics.get('limit_up_count_10d', 0)
    if limit_up_count >= 3:
        risk_score *= 0.5
        risk_reasons.append(f'è¿‘æœŸå¤šæ¬¡æ¶¨åœ({limit_up_count}æ¬¡)')
    elif limit_up_count >= 2:
        risk_score *= 0.7
        risk_reasons.append(f'è¿‘æœŸæ¶¨åœ({limit_up_count}æ¬¡)')
    
    # è§„åˆ™6: è·ç¦»å†å²é«˜ç‚¹å¤ªè¿‘ï¼ˆå‹åŠ›ä½é£é™©ï¼‰
    dist_high = risk_metrics.get('dist_to_hist_high', 10)
    if dist_high < 2:
        risk_score *= 0.7
        risk_reasons.append('æ¥è¿‘å†å²é«˜ç‚¹')
    
    return risk_score, risk_reasons, risk_metrics


def score_stocks_with_risk_filter(dm, stock_list, booster, feature_names, predict_date, df_market):
    """å¯¹è‚¡ç¥¨è¯„åˆ†å¹¶åº”ç”¨é£é™©è¿‡æ»¤"""
    log.info(f"\nå¼€å§‹è¯„åˆ†ï¼ˆå¸¦é£é™©è¿‡æ»¤ï¼‰...")
    
    results = []
    total = len(stock_list)
    processed = 0
    
    for idx, row in stock_list.iterrows():
        ts_code = row['ts_code']
        name = row['name']
        processed += 1
        
        if processed % 500 == 0:
            log.info(f"è¿›åº¦: {processed}/{total} | å·²è¯„åˆ†: {len(results)}")
        
        try:
            # è·å–è‚¡ç¥¨æ•°æ®
            df = get_stock_data(dm, ts_code, predict_date, df_market)
            if df is None or len(df) < 20:
                continue
            
            # æå–ç‰¹å¾
            features = extract_features(df, feature_names)
            if features is None:
                continue
            
            # è®¡ç®—é£é™©æŒ‡æ ‡
            risk_metrics = calculate_risk_metrics(df)
            risk_score, risk_reasons, risk_data = apply_risk_filter(row, risk_metrics)
            
            # æ„å»ºç‰¹å¾å‘é‡
            feature_vector = [features.get(fn, 0) for fn in feature_names]
            
            # é¢„æµ‹
            dmatrix = xgb.DMatrix([feature_vector], feature_names=feature_names)
            prob = booster.predict(dmatrix)[0]
            
            # è°ƒæ•´åæ¦‚ç‡ = åŸå§‹æ¦‚ç‡ Ã— é£é™©ç³»æ•°
            adjusted_prob = prob * risk_score
            
            results.append({
                'ts_code': ts_code,
                'name': name,
                'raw_probability': prob,
                'risk_score': risk_score,
                'adjusted_probability': adjusted_prob,
                'predict_price': df['close'].iloc[-1],
                'predict_date': str(df['trade_date'].iloc[-1])[:10],
                'return_34d': risk_data.get('return_34d', 0) if risk_data else 0,
                'rsi_14': risk_data.get('rsi_14', 50) if risk_data else 50,
                'volatility': risk_data.get('volatility', 0) if risk_data else 0,
                'risk_reasons': '; '.join(risk_reasons) if risk_reasons else ''
            })
        except Exception as e:
            if processed <= 10:
                log.warning(f"å¤„ç† {ts_code} å¤±è´¥: {e}")
            continue
    
    df_results = pd.DataFrame(results)
    
    # æŒ‰è°ƒæ•´åæ¦‚ç‡æ’åº
    df_results = df_results.sort_values('adjusted_probability', ascending=False)
    
    log.success(f"âœ“ è¯„åˆ†å®Œæˆ: {len(df_results)} åªè‚¡ç¥¨")
    
    return df_results


def evaluate_predictions(dm, df_predictions, eval_date, version):
    """è¯„ä¼°é¢„æµ‹ç»“æœ"""
    log.info(f"\nè¯„ä¼°é¢„æµ‹ç»“æœ...")
    
    results = []
    
    for idx, row in df_predictions.iterrows():
        ts_code = row['ts_code']
        
        # è·å–è¯„ä¼°æ—¥çš„ä»·æ ¼
        eval_start = (datetime.strptime(eval_date, '%Y%m%d') - timedelta(days=10)).strftime('%Y%m%d')
        eval_end = (datetime.strptime(eval_date, '%Y%m%d') + timedelta(days=5)).strftime('%Y%m%d')
        
        df_eval = dm.get_daily_data(ts_code, eval_start, eval_end)
        if df_eval is None or len(df_eval) == 0:
            continue
        
        # æ‰¾æœ€æ¥è¿‘è¯„ä¼°æ—¥çš„æ•°æ®
        df_eval['date_diff'] = abs(pd.to_datetime(df_eval['trade_date']) - pd.to_datetime(eval_date))
        closest = df_eval.loc[df_eval['date_diff'].idxmin()]
        
        eval_price = closest['close']
        eval_date_actual = str(closest['trade_date'])[:10]
        
        # è®¡ç®—æ”¶ç›Š
        predict_price = row['predict_price']
        return_pct = (eval_price / predict_price - 1) * 100
        
        result = row.to_dict()
        result['eval_price'] = eval_price
        result['eval_date_actual'] = eval_date_actual
        result['return_pct'] = return_pct
        result['status'] = 'æ­£å¸¸'
        
        results.append(result)
    
    return pd.DataFrame(results)


def print_evaluation_summary(df_eval, version, filter_type):
    """æ‰“å°è¯„ä¼°æ‘˜è¦"""
    log.info("="*80)
    log.info(f"{version} æ¨¡å‹è¯„ä¼°ç»“æœ ({filter_type})")
    log.info("="*80)
    
    if len(df_eval) == 0:
        log.warning("æ— æœ‰æ•ˆè¯„ä¼°æ•°æ®")
        return {}
    
    # åŸºç¡€ç»Ÿè®¡
    avg_return = df_eval['return_pct'].mean()
    median_return = df_eval['return_pct'].median()
    win_rate = (df_eval['return_pct'] > 0).mean() * 100
    max_return = df_eval['return_pct'].max()
    min_return = df_eval['return_pct'].min()
    
    log.info(f"\nğŸ“Š æ•´ä½“ç»Ÿè®¡ï¼ˆTop {len(df_eval)}ï¼‰:")
    log.info(f"  æœ‰æ•ˆè‚¡ç¥¨æ•°: {len(df_eval)}")
    log.info(f"  å¹³å‡æ”¶ç›Šç‡: {avg_return:.2f}%")
    log.info(f"  ä¸­ä½æ•°æ”¶ç›Š: {median_return:.2f}%")
    log.info(f"  èƒœç‡: {win_rate:.1f}%")
    log.info(f"  æœ€é«˜æ”¶ç›Š: {max_return:.2f}%")
    log.info(f"  æœ€ä½æ”¶ç›Š: {min_return:.2f}%")
    
    # Top 5
    log.info(f"\nğŸ† æ”¶ç›Šæœ€é«˜çš„5åª:")
    top5 = df_eval.nlargest(5, 'return_pct')
    for _, row in top5.iterrows():
        prob_col = 'adjusted_probability' if 'adjusted_probability' in row else 'probability'
        log.info(f"  {row['ts_code']} {row['name']}: æ¦‚ç‡{row[prob_col]:.2%}, æ”¶ç›Š{row['return_pct']:.2f}%")
    
    # Bottom 5
    log.info(f"\nâŒ æ”¶ç›Šæœ€ä½çš„5åª:")
    bottom5 = df_eval.nsmallest(5, 'return_pct')
    for _, row in bottom5.iterrows():
        prob_col = 'adjusted_probability' if 'adjusted_probability' in row else 'probability'
        risk_col = row.get('risk_reasons', '')
        log.info(f"  {row['ts_code']} {row['name']}: æ¦‚ç‡{row[prob_col]:.2%}, æ”¶ç›Š{row['return_pct']:.2f}%")
        if risk_col:
            log.info(f"    é£é™©æ ‡ç­¾: {risk_col}")
    
    return {
        'avg_return': avg_return,
        'median_return': median_return,
        'win_rate': win_rate,
        'max_return': max_return,
        'min_return': min_return,
        'count': len(df_eval)
    }


def main():
    parser = argparse.ArgumentParser(description='V2.1.0æ¨¡å‹è¯„ä¼°ï¼ˆå¸¦é£é™©è¿‡æ»¤ï¼‰')
    parser.add_argument('--predict-date', type=str, default='20251212', help='é¢„æµ‹æ—¥æœŸ')
    parser.add_argument('--eval-date', type=str, default='20251231', help='è¯„ä¼°æ—¥æœŸ')
    parser.add_argument('--top-n', type=int, default=50, help='Top Nè‚¡ç¥¨æ•°é‡')
    args = parser.parse_args()
    
    log.info("="*80)
    log.info("V2.1.0æ¨¡å‹è¯„ä¼° - å¸¦é£é™©è¿‡æ»¤åå¤„ç†")
    log.info("="*80)
    log.info(f"é¢„æµ‹æ—¥æœŸ: {args.predict_date}")
    log.info(f"è¯„ä¼°æ—¥æœŸ: {args.eval_date}")
    log.info(f"Top N: {args.top_n}")
    log.info("")
    
    # åˆå§‹åŒ–
    dm = DataManager()
    stock_list = get_valid_stock_list(dm)
    log.info(f"æœ‰æ•ˆè‚¡ç¥¨æ•°: {len(stock_list)}")
    
    # è·å–å¸‚åœºæ•°æ®
    log.info("\nè·å–å¸‚åœºæ•°æ®...")
    df_market = get_market_data(dm, args.predict_date)
    if df_market is not None:
        log.success(f"âœ“ å¸‚åœºæ•°æ®å·²è·å–: {len(df_market)} æ¡è®°å½•")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = PROJECT_ROOT / 'data' / 'prediction' / 'evaluation'
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # åŠ è½½v2.1.0æ¨¡å‹
    log.info("\nåŠ è½½æ¨¡å‹...")
    booster, feature_names = load_model('v2.1.0')
    log.success(f"âœ“ v2.1.0 æ¨¡å‹åŠ è½½æˆåŠŸ: {len(feature_names)} ä¸ªç‰¹å¾")
    
    # ========== æ— è¿‡æ»¤è¯„ä¼° ==========
    log.info("\n" + "="*80)
    log.info("v2.1.0 æ— é£é™©è¿‡æ»¤ï¼ˆåŸå§‹ç‰ˆæœ¬ï¼‰")
    log.info("="*80)
    
    # è¯„åˆ†ï¼ˆæ— è¿‡æ»¤ï¼Œç”¨äºå¯¹æ¯”ï¼‰
    df_scores_raw = score_stocks_with_risk_filter(dm, stock_list, booster, feature_names, args.predict_date, df_market)
    
    # æŒ‰åŸå§‹æ¦‚ç‡æ’åºå–Top N
    df_top_raw = df_scores_raw.nlargest(args.top_n, 'raw_probability')
    
    log.info(f"\næ— è¿‡æ»¤ Top {args.top_n} è‚¡ç¥¨ï¼ˆæŒ‰åŸå§‹æ¦‚ç‡ï¼‰:")
    for i, (_, row) in enumerate(df_top_raw.head(10).iterrows()):
        log.info(f"  {row['ts_code']} {row['name']}: åŸå§‹{row['raw_probability']:.4f}, é£é™©ç³»æ•°{row['risk_score']:.2f}")
    
    # è¯„ä¼°æ— è¿‡æ»¤ç»“æœ
    df_eval_raw = evaluate_predictions(dm, df_top_raw, args.eval_date, 'v2.1.0_raw')
    stats_raw = print_evaluation_summary(df_eval_raw, 'v2.1.0', 'æ— é£é™©è¿‡æ»¤')
    
    # ========== å¸¦é£é™©è¿‡æ»¤è¯„ä¼° ==========
    log.info("\n" + "="*80)
    log.info("v2.1.0 å¸¦é£é™©è¿‡æ»¤")
    log.info("="*80)
    
    # æŒ‰è°ƒæ•´åæ¦‚ç‡æ’åºå–Top N
    df_top_filtered = df_scores_raw.nlargest(args.top_n, 'adjusted_probability')
    
    log.info(f"\nå¸¦è¿‡æ»¤ Top {args.top_n} è‚¡ç¥¨ï¼ˆæŒ‰è°ƒæ•´åæ¦‚ç‡ï¼‰:")
    for i, (_, row) in enumerate(df_top_filtered.head(10).iterrows()):
        risk_info = f"[{row['risk_reasons']}]" if row['risk_reasons'] else ""
        log.info(f"  {row['ts_code']} {row['name']}: è°ƒæ•´å{row['adjusted_probability']:.4f} (åŸå§‹{row['raw_probability']:.4f}, é£é™©{row['risk_score']:.2f}) {risk_info}")
    
    # è¯„ä¼°å¸¦è¿‡æ»¤ç»“æœ
    df_eval_filtered = evaluate_predictions(dm, df_top_filtered, args.eval_date, 'v2.1.0_filtered')
    stats_filtered = print_evaluation_summary(df_eval_filtered, 'v2.1.0', 'å¸¦é£é™©è¿‡æ»¤')
    
    # ========== å¯¹æ¯”åˆ†æ ==========
    log.info("\n" + "="*80)
    log.info("é£é™©è¿‡æ»¤æ•ˆæœå¯¹æ¯”")
    log.info("="*80)
    
    log.info("\n| æŒ‡æ ‡ | æ— è¿‡æ»¤ | å¸¦é£é™©è¿‡æ»¤ | å˜åŒ– |")
    log.info("|------|--------|------------|------|")
    
    if stats_raw and stats_filtered:
        avg_diff = stats_filtered['avg_return'] - stats_raw['avg_return']
        median_diff = stats_filtered['median_return'] - stats_raw['median_return']
        win_diff = stats_filtered['win_rate'] - stats_raw['win_rate']
        
        log.info(f"| å¹³å‡æ”¶ç›Šç‡ | {stats_raw['avg_return']:.2f}% | {stats_filtered['avg_return']:.2f}% | {avg_diff:+.2f}% |")
        log.info(f"| ä¸­ä½æ•°æ”¶ç›Š | {stats_raw['median_return']:.2f}% | {stats_filtered['median_return']:.2f}% | {median_diff:+.2f}% |")
        log.info(f"| èƒœç‡ | {stats_raw['win_rate']:.1f}% | {stats_filtered['win_rate']:.1f}% | {win_diff:+.1f}% |")
        log.info(f"| æœ€é«˜æ”¶ç›Š | {stats_raw['max_return']:.2f}% | {stats_filtered['max_return']:.2f}% | - |")
        log.info(f"| æœ€ä½æ”¶ç›Š | {stats_raw['min_return']:.2f}% | {stats_filtered['min_return']:.2f}% | - |")
    
    # ========== ä¿å­˜ç»“æœ ==========
    # ä¿å­˜æ— è¿‡æ»¤ç»“æœ
    raw_file = output_dir / f'v2.1.0_raw_top{args.top_n}_{args.predict_date}.csv'
    df_eval_raw.to_csv(raw_file, index=False, encoding='utf-8-sig')
    
    # ä¿å­˜å¸¦è¿‡æ»¤ç»“æœ
    filtered_file = output_dir / f'v2.1.0_filtered_top{args.top_n}_{args.predict_date}.csv'
    df_eval_filtered.to_csv(filtered_file, index=False, encoding='utf-8-sig')
    
    # ä¿å­˜å…¨é‡è¯„åˆ†æ•°æ®
    all_scores_file = output_dir / f'v2.1.0_all_scores_{args.predict_date}.csv'
    df_scores_raw.to_csv(all_scores_file, index=False, encoding='utf-8-sig')
    
    log.success(f"\nâœ“ ç»“æœå·²ä¿å­˜åˆ° {output_dir}")
    
    # è¢«è¿‡æ»¤æ‰çš„é«˜é£é™©è‚¡ç¥¨åˆ†æ
    log.info("\n" + "="*80)
    log.info("è¢«é£é™©è¿‡æ»¤å½±å“çš„è‚¡ç¥¨åˆ†æ")
    log.info("="*80)
    
    # æ‰¾å‡ºåŸå§‹Top50ä¸­è¢«è°ƒæ•´åæ’é™¤çš„è‚¡ç¥¨
    raw_top_codes = set(df_top_raw['ts_code'])
    filtered_top_codes = set(df_top_filtered['ts_code'])
    
    filtered_out = raw_top_codes - filtered_top_codes
    filtered_in = filtered_top_codes - raw_top_codes
    
    log.info(f"\nåŸå§‹Top50ä¸­è¢«è¿‡æ»¤æ‰çš„è‚¡ç¥¨: {len(filtered_out)} åª")
    if filtered_out:
        df_filtered_out = df_eval_raw[df_eval_raw['ts_code'].isin(filtered_out)]
        if len(df_filtered_out) > 0:
            avg_return_out = df_filtered_out['return_pct'].mean()
            log.info(f"  è¿™äº›è‚¡ç¥¨çš„å¹³å‡æ”¶ç›Š: {avg_return_out:.2f}%")
            for _, row in df_filtered_out.iterrows():
                log.info(f"    {row['ts_code']} {row['name']}: æ”¶ç›Š{row['return_pct']:.2f}%, é£é™©[{row.get('risk_reasons', '')}]")
    
    log.info(f"\nè¿‡æ»¤åæ–°è¿›å…¥Top50çš„è‚¡ç¥¨: {len(filtered_in)} åª")
    if filtered_in:
        df_filtered_in = df_eval_filtered[df_eval_filtered['ts_code'].isin(filtered_in)]
        if len(df_filtered_in) > 0:
            avg_return_in = df_filtered_in['return_pct'].mean()
            log.info(f"  è¿™äº›è‚¡ç¥¨çš„å¹³å‡æ”¶ç›Š: {avg_return_in:.2f}%")
            for _, row in df_filtered_in.head(10).iterrows():
                log.info(f"    {row['ts_code']} {row['name']}: æ”¶ç›Š{row['return_pct']:.2f}%")


if __name__ == '__main__':
    main()

