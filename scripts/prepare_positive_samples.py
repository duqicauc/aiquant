"""
å‡†å¤‡æ­£æ ·æœ¬æ•°æ®çš„ä¸»è„šæœ¬

è¿è¡Œæ­¥éª¤ï¼š
1. ç­›é€‰ç¬¦åˆæ¡ä»¶çš„æ­£æ ·æœ¬
2. æå–æ¯ä¸ªæ ·æœ¬T1å‰34å¤©çš„ç‰¹å¾æ•°æ®
3. ä¿å­˜ç»“æœ
"""
import sys
from pathlib import Path
import warnings

# è¿‡æ»¤ pandas FutureWarningï¼ˆæ¥è‡ª tushare åº“å†…éƒ¨ï¼Œä¸å½±å“åŠŸèƒ½ï¼‰
warnings.filterwarnings('ignore', category=FutureWarning, module='tushare')

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

from src.data.data_manager import DataManager
from src.models.screening.positive_sample_screener import PositiveSampleScreener
from src.utils.logger import log
from src.utils.human_intervention import HumanInterventionChecker, require_human_confirmation
from config.settings import settings
import pandas as pd
from datetime import datetime


def main():
    """ä¸»å‡½æ•°"""
    
    log.info("="*80)
    log.info("æ­£æ ·æœ¬æ•°æ®å‡†å¤‡æµç¨‹")
    log.info("="*80)
    
    # å®šä¹‰æ–‡ä»¶è·¯å¾„
    samples_file = PROJECT_ROOT / 'data' / 'training' / 'samples' / 'positive_samples.csv'
    features_file = PROJECT_ROOT / 'data' / 'training' / 'processed' / 'feature_data_34d.csv'
    
    # ä»é…ç½®æ–‡ä»¶è¯»å–æ—¥æœŸèŒƒå›´
    START_DATE = settings.get('data.sample_preparation.start_date', '20000101')
    END_DATE = settings.get('data.sample_preparation.end_date', None)
    
    # æ£€æŸ¥æ˜¯å¦å·²æœ‰æ­£æ ·æœ¬æ•°æ®
    if samples_file.exists():
        log.info(f"\nğŸ“‚ æ£€æµ‹åˆ°å·²æœ‰æ­£æ ·æœ¬æ–‡ä»¶: {samples_file}")
        df_samples = pd.read_csv(samples_file)
        log.success(f"âœ“ å·²åŠ è½½æœ¬åœ°æ­£æ ·æœ¬æ•°æ®ï¼Œå…± {len(df_samples)} æ¡è®°å½•")
        
        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        log.info("\n" + "="*80)
        log.info("æ­£æ ·æœ¬ç»Ÿè®¡ï¼ˆæœ¬åœ°æ•°æ®ï¼‰")
        log.info("="*80)
        log.info(f"æ ·æœ¬æ€»æ•°: {len(df_samples)}")
        log.info(f"è‚¡ç¥¨æ•°é‡: {df_samples['ts_code'].nunique()}")
        if 'total_return' in df_samples.columns:
            log.info(f"å¹³å‡æ€»æ¶¨å¹…: {df_samples['total_return'].mean():.2f}%")
        if 'max_return' in df_samples.columns:
            log.info(f"å¹³å‡æœ€é«˜æ¶¨å¹…: {df_samples['max_return'].mean():.2f}%")
        
        # è¯¢é—®æ˜¯å¦é‡æ–°ç­›é€‰
        use_existing = require_human_confirmation(
            "æ˜¯å¦ä½¿ç”¨å·²æœ‰çš„æ­£æ ·æœ¬æ•°æ®ï¼Ÿï¼ˆé€‰æ‹©Nå°†é‡æ–°ç­›é€‰ï¼‰",
            default=True
        )
        
        if not use_existing:
            df_samples = None  # æ ‡è®°éœ€è¦é‡æ–°ç­›é€‰
            log.info("å°†é‡æ–°ç­›é€‰æ­£æ ·æœ¬...")
    else:
        df_samples = None
        log.info(f"\nğŸ“‚ æœªæ£€æµ‹åˆ°æ­£æ ·æœ¬æ–‡ä»¶ï¼Œå°†è¿›è¡Œç­›é€‰...")
    
    # 1. åˆå§‹åŒ–æ•°æ®ç®¡ç†å™¨
    log.info("\n[æ­¥éª¤1] åˆå§‹åŒ–æ•°æ®ç®¡ç†å™¨...")
    dm = DataManager(source='tushare')
    
    # 2. åˆå§‹åŒ–ç­›é€‰å™¨
    log.info("\n[æ­¥éª¤2] åˆå§‹åŒ–æ­£æ ·æœ¬ç­›é€‰å™¨...")
    screener = PositiveSampleScreener(dm)
    
    # 3. ç­›é€‰æ­£æ ·æœ¬ï¼ˆä»…å½“æ²¡æœ‰æœ¬åœ°æ•°æ®æ—¶ï¼‰
    if df_samples is None:
        log.info("\n[æ­¥éª¤3] å¼€å§‹ç­›é€‰æ­£æ ·æœ¬ï¼ˆè¿™å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´ï¼‰...")
        log.info("ç­›é€‰æ¡ä»¶ï¼š")
        log.info("  - å‘¨Kè¿ç»­ä¸‰å‘¨æ”¶é˜³çº¿")
        log.info("  - æ€»æ¶¨å¹…è¶…50%")
        log.info("  - æœ€é«˜æ¶¨å¹…è¶…70%")
        log.info("  - å‰”é™¤STè‚¡ç¥¨")
        log.info("  - ä¸Šå¸‚è¶…è¿‡åŠå¹´")
        log.info("")
        
        log.info(f"ğŸ“… æ•°æ®èŒƒå›´é…ç½®ï¼š{START_DATE} - {END_DATE or 'ä»Šå¤©'}")
        log.info(f"ğŸ’¡ å¦‚éœ€ä¿®æ”¹ï¼Œè¯·ç¼–è¾‘ config/settings.yaml")
        
        # ğŸ‘¤ äººå·¥ä»‹å…¥æ£€æŸ¥ï¼šæ­£æ ·æœ¬ç­›é€‰æ¡ä»¶
        checker = HumanInterventionChecker()
        criteria_check = checker.check_positive_sample_criteria()
        needs_intervention = checker.print_intervention_reminder("æ­£æ ·æœ¬ç­›é€‰æ¡ä»¶", criteria_check)
        
        if needs_intervention:
            confirmed = require_human_confirmation(
                "âš ï¸  æ£€æµ‹åˆ°æ­£æ ·æœ¬ç­›é€‰æ¡ä»¶å¯èƒ½éœ€è¦è°ƒæ•´ã€‚\n"
                "è¯·æ£€æŸ¥ä¸Šè¿°è­¦å‘Šå’Œå»ºè®®ï¼Œç¡®è®¤æ˜¯å¦ç»§ç»­ä½¿ç”¨å½“å‰é…ç½®ã€‚",
                default=True  # é»˜è®¤ç»§ç»­ï¼Œåœ¨è‡ªåŠ¨æ¨¡å¼ä¸‹ä¼šè‡ªåŠ¨ç¡®è®¤
            )
            if not confirmed:
                log.warning("ç”¨æˆ·å–æ¶ˆæ“ä½œã€‚è¯·ä¿®æ”¹ config/settings.yaml åé‡æ–°è¿è¡Œã€‚")
                return
        
        try:
            df_samples = screener.screen_all_stocks(
                start_date=START_DATE,
                end_date=END_DATE
            )
            
            if df_samples.empty:
                log.error("æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„æ­£æ ·æœ¬ï¼è¯·æ£€æŸ¥ç­›é€‰æ¡ä»¶æˆ–æ•°æ®è´¨é‡")
                return
            
            # ä¿å­˜æ­£æ ·æœ¬åˆ—è¡¨
            samples_file.parent.mkdir(parents=True, exist_ok=True)
            df_samples.to_csv(samples_file, index=False, encoding='utf-8-sig')
            log.success(f"âœ“ æ­£æ ·æœ¬åˆ—è¡¨å·²ä¿å­˜: {samples_file}")
            
            # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
            log.info("\n" + "="*80)
            log.info("æ­£æ ·æœ¬ç»Ÿè®¡")
            log.info("="*80)
            log.info(f"æ ·æœ¬æ€»æ•°: {len(df_samples)}")
            log.info(f"è‚¡ç¥¨æ•°é‡: {df_samples['ts_code'].nunique()}")
            log.info(f"å¹³å‡æ€»æ¶¨å¹…: {df_samples['total_return'].mean():.2f}%")
            log.info(f"å¹³å‡æœ€é«˜æ¶¨å¹…: {df_samples['max_return'].mean():.2f}%")
            log.info(f"\nå‰5ä¸ªæ ·æœ¬:")
            print(df_samples.head())
            
            # ğŸ‘¤ äººå·¥ä»‹å…¥æé†’ï¼šæ£€æŸ¥æ­£æ ·æœ¬è´¨é‡
            log.warning("\n" + "="*80)
            log.warning("ğŸ‘¤ äººå·¥ä»‹å…¥æé†’ï¼šè¯·æ£€æŸ¥æ­£æ ·æœ¬è´¨é‡")
            log.warning("="*80)
            log.warning("è¯·ç¡®è®¤ï¼š")
            log.warning("  1. æ ·æœ¬æ•°é‡æ˜¯å¦åˆç†ï¼ˆå»ºè®®ï¼š1000-5000ä¸ªï¼‰")
            log.warning("  2. å¹³å‡æ¶¨å¹…æ˜¯å¦ç¬¦åˆé¢„æœŸ")
            log.warning("  3. æ ·æœ¬åˆ†å¸ƒæ˜¯å¦åˆç†")
            log.warning("  4. æ˜¯å¦éœ€è¦è°ƒæ•´ç­›é€‰æ¡ä»¶")
            log.warning("="*80)
            
        except Exception as e:
            log.error(f"æ­£æ ·æœ¬ç­›é€‰å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return
    else:
        log.info("\n[æ­¥éª¤3] è·³è¿‡æ­£æ ·æœ¬ç­›é€‰ï¼ˆä½¿ç”¨æœ¬åœ°æ•°æ®ï¼‰")
    
    try:
        # 4. æå–ç‰¹å¾æ•°æ®
        log.info("\n[æ­¥éª¤4] æå–ç‰¹å¾æ•°æ®ï¼ˆT1å‰34å¤©ï¼‰...")
        
        df_features = screener.extract_features(
            df_samples,
            lookback_days=34
        )
        
        if df_features.empty:
            log.error("ç‰¹å¾æå–å¤±è´¥ï¼")
            return
        
        # 4.1 æ•°æ®è´¨é‡å¤„ç†
        log.info("\n[æ­¥éª¤4.1] æ•°æ®è´¨é‡å¤„ç†...")
        
        # ç»Ÿè®¡åŸå§‹ç¼ºå¤±å€¼
        missing_before = df_features.isnull().sum()
        total_missing_before = missing_before.sum()
        log.info(f"åŸå§‹ç¼ºå¤±å€¼æ€»æ•°: {total_missing_before}")
        if total_missing_before > 0:
            for col, count in missing_before.items():
                if count > 0:
                    log.info(f"  - {col}: {count} ({count/len(df_features)*100:.2f}%)")
        
        # å®šä¹‰éœ€è¦å¡«å……çš„æ•°å€¼åˆ—
        numeric_cols = ['close', 'pct_chg', 'total_mv', 'circ_mv', 'ma5', 'ma10', 
                        'volume_ratio', 'macd_dif', 'macd_dea', 'macd', 
                        'rsi_6', 'rsi_12', 'rsi_24']
        numeric_cols = [col for col in numeric_cols if col in df_features.columns]
        
        # æŒ‰æ ·æœ¬åˆ†ç»„è¿›è¡Œå‰å‘å¡«å……+åå‘å¡«å……
        log.info("æ‰§è¡Œç¼ºå¤±å€¼å¡«å……ï¼ˆæŒ‰æ ·æœ¬åˆ†ç»„ï¼šå‰å‘å¡«å…… + åå‘å¡«å……ï¼‰...")
        df_features[numeric_cols] = df_features.groupby('sample_id')[numeric_cols].transform(
            lambda x: x.ffill().bfill()
        )
        
        # æ£€æŸ¥å¡«å……åçš„ç¼ºå¤±å€¼
        missing_after = df_features.isnull().sum()
        total_missing_after = missing_after.sum()
        log.info(f"å¡«å……åç¼ºå¤±å€¼æ€»æ•°: {total_missing_after}")
        
        # 4.2 è¿‡æ»¤æ•°æ®ä¸è¶³çš„æ ·æœ¬
        log.info("\n[æ­¥éª¤4.2] è¿‡æ»¤æ•°æ®ä¸è¶³çš„æ ·æœ¬...")
        min_days = 30  # æœ€å°‘éœ€è¦30å¤©æ•°æ®
        
        days_per_sample = df_features.groupby('sample_id').size()
        valid_samples = days_per_sample[days_per_sample >= min_days].index
        invalid_samples = days_per_sample[days_per_sample < min_days]
        
        if len(invalid_samples) > 0:
            log.warning(f"å‘ç° {len(invalid_samples)} ä¸ªæ ·æœ¬æ•°æ®ä¸è¶³{min_days}å¤©ï¼Œå°†è¢«è¿‡æ»¤:")
            for sample_id, days in invalid_samples.items():
                sample_info = df_features[df_features['sample_id'] == sample_id].iloc[0]
                log.warning(f"  - æ ·æœ¬{sample_id}: {sample_info['ts_code']} {sample_info['name']} - ä»…{days}å¤©")
            
            df_features = df_features[df_features['sample_id'].isin(valid_samples)]
            log.info(f"è¿‡æ»¤åå‰©ä½™æ ·æœ¬æ•°: {df_features['sample_id'].nunique()}")
            log.info(f"è¿‡æ»¤åå‰©ä½™è®°å½•æ•°: {len(df_features)}")
        else:
            log.success(f"âœ“ æ‰€æœ‰æ ·æœ¬æ•°æ®å®Œæ•´ï¼ˆå‡â‰¥{min_days}å¤©ï¼‰")
        
        # 4.3 æœ€ç»ˆæ•°æ®è´¨é‡æ£€æŸ¥
        log.info("\n[æ­¥éª¤4.3] æœ€ç»ˆæ•°æ®è´¨é‡æ£€æŸ¥...")
        final_missing = df_features.isnull().sum().sum()
        if final_missing > 0:
            log.warning(f"ä»æœ‰ {final_missing} ä¸ªç¼ºå¤±å€¼ï¼Œå°†ä½¿ç”¨åˆ—å‡å€¼å¡«å……...")
            df_features[numeric_cols] = df_features[numeric_cols].fillna(df_features[numeric_cols].mean())
        log.success(f"âœ“ æ•°æ®è´¨é‡å¤„ç†å®Œæˆï¼Œæœ€ç»ˆç¼ºå¤±å€¼: {df_features.isnull().sum().sum()}")
        
        # ä¿å­˜ç‰¹å¾æ•°æ®
        features_file.parent.mkdir(parents=True, exist_ok=True)
        df_features.to_csv(features_file, index=False, encoding='utf-8-sig')
        log.success(f"âœ“ ç‰¹å¾æ•°æ®å·²ä¿å­˜: {features_file}")
        
        # ç‰¹å¾ç»Ÿè®¡
        log.info("\n" + "="*80)
        log.info("ç‰¹å¾æ•°æ®ç»Ÿè®¡")
        log.info("="*80)
        log.info(f"æ€»è®°å½•æ•°: {len(df_features)}")
        log.info(f"æ ·æœ¬æ•°: {df_features['sample_id'].nunique()}")
        log.info(f"æ¯æ ·æœ¬å¤©æ•°: {len(df_features) / df_features['sample_id'].nunique():.1f}")
        log.info(f"\næ•°æ®å­—æ®µ:")
        for col in df_features.columns:
            log.info(f"  - {col}")
        log.info(f"\næ•°æ®é¢„è§ˆ:")
        print(df_features.head(10))
        
        # 5. ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
        log.info("\n[æ­¥éª¤5] ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š...")
        
        stats = {
            'generation_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'date_range': f"{START_DATE} - {END_DATE or 'today'}",
            'total_samples': int(len(df_samples)),
            'unique_stocks': int(df_samples['ts_code'].nunique()),
            'avg_total_return': float(df_samples['total_return'].mean()),
            'avg_max_return': float(df_samples['max_return'].mean()),
            'feature_records': int(len(df_features)),
            'feature_samples': int(df_features['sample_id'].nunique()),
            'lookback_days': 34,
            'min_days_required': min_days,
            'data_quality': {
                'missing_values_before': int(total_missing_before),
                'missing_values_after': int(df_features.isnull().sum().sum()),
                'filtered_samples': int(len(invalid_samples)) if len(invalid_samples) > 0 else 0,
                'avg_days_per_sample': float(df_features.groupby('sample_id').size().mean())
            },
            'sample_files': {
                'positive_samples': str(samples_file),
                'feature_data': str(features_file)
            }
        }
        
        import json
        stats_file = PROJECT_ROOT / 'data' / 'training' / 'processed' / 'sample_statistics.json'
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(stats, f, indent=2, ensure_ascii=False)
        
        log.success(f"âœ“ ç»Ÿè®¡æŠ¥å‘Šå·²ä¿å­˜: {stats_file}")
        
        # å®Œæˆ
        log.info("\n" + "="*80)
        log.success("âœ… æ­£æ ·æœ¬æ•°æ®å‡†å¤‡å®Œæˆï¼")
        log.info("="*80)
        log.info("\nç”Ÿæˆçš„æ–‡ä»¶:")
        log.info(f"  1. æ­£æ ·æœ¬åˆ—è¡¨: {samples_file}")
        log.info(f"  2. ç‰¹å¾æ•°æ®: {features_file}")
        log.info(f"  3. ç»Ÿè®¡æŠ¥å‘Š: {stats_file}")
        log.info("\nä¸‹ä¸€æ­¥:")
        log.info("  - æŸ¥çœ‹æ•°æ®è´¨é‡")
        log.info("  - å‡†å¤‡è´Ÿæ ·æœ¬")
        log.info("  - å¼€å§‹æ¨¡å‹è®­ç»ƒ")
        
    except Exception as e:
        log.error(f"æ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == '__main__':
    main()

