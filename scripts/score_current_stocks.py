"""
å¯¹å½“å‰å¸‚åœºæ‰€æœ‰è‚¡ç¥¨è¿›è¡Œè¯„åˆ†å’Œç­›é€‰

åŸºäºè®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œå¯¹æ‰€æœ‰Aè‚¡è¿›è¡Œè¯„åˆ†ï¼Œæ‰¾å‡ºæœ€æœ‰å¯èƒ½æˆä¸ºç‰›è‚¡çš„è‚¡ç¥¨
- åŠ è½½æœ€æ–°çš„æ¨¡å‹
- è·å–æ‰€æœ‰è‚¡ç¥¨çš„æœ€æ–°æ•°æ®ï¼ˆè¿‡å»34å¤©ï¼‰
- è®¡ç®—ç‰¹å¾å¹¶é¢„æµ‹æ¦‚ç‡
- æŒ‰ç…§æ¦‚ç‡æ’åºï¼Œè¾“å‡ºTop N
- å‚è€ƒæ­£æ ·æœ¬å‰”é™¤è§„åˆ™ï¼ˆSTã€æ–°è‚¡ã€åœç‰Œç­‰ï¼‰

æ”¯æŒæŒ‡å®šæ—¥æœŸè¿›è¡Œå†å²å›æµ‹ï¼š
  python scripts/score_current_stocks.py --date 20250919
"""
import sys
import os
import warnings
import time
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import json
import argparse
import xgboost as xgb

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

# å¿½ç•¥FutureWarning
warnings.filterwarnings('ignore', category=FutureWarning)

from src.data.data_manager import DataManager
from src.utils.logger import log


def load_model(model_path=None, version=None):
    """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹ï¼ˆæ—§ç‰ˆæœ¬ï¼šä»…æ”¯æŒxgboost_timeseriesæ¨¡å‹ï¼‰
    
    Args:
        model_path: ç›´æ¥æŒ‡å®šæ¨¡å‹æ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨æŸ¥æ‰¾æœ€æ–°æ¨¡å‹
        version: å·²åºŸå¼ƒï¼Œä¿ç•™ä»¥å…¼å®¹æ—§ä»£ç 
    """
    # å¦‚æœæœªæŒ‡å®šè·¯å¾„ï¼ŒæŸ¥æ‰¾æ—§è·¯å¾„çš„æ¨¡å‹
    if model_path is None:
        model_dir = 'data/training/models'
        if os.path.exists(model_dir):
            import glob
            # æŸ¥æ‰¾ xgboost_timeseries_v2_*.json æ–‡ä»¶
            model_files = glob.glob(os.path.join(model_dir, 'xgboost_timeseries_v2_*.json'))
            if model_files:
                # ä½¿ç”¨æœ€æ–°çš„æ¨¡å‹æ–‡ä»¶
                model_path = max(model_files, key=os.path.getmtime)
                log.info(f"è‡ªåŠ¨æ‰¾åˆ°æ¨¡å‹: {model_path}")
            else:
                raise FileNotFoundError(f"æœªæ‰¾åˆ°æ¨¡å‹æ–‡ä»¶ï¼Œè¯·æ£€æŸ¥ {model_dir} ç›®å½•")
        else:
            raise FileNotFoundError(f"æ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {model_dir}")
    
    if not os.path.exists(model_path):
        raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
    
    log.info(f"åŠ è½½æ¨¡å‹: {model_path}")
    
    # åŠ è½½XGBoost Booster
    booster = xgb.Booster()
    booster.load_model(str(model_path))
    
    # ä»metricsæ–‡ä»¶è·å–ç‰¹å¾åç§°
    metrics_file = 'data/training/metrics/xgboost_timeseries_v2_metrics.json'
    feature_names = None
    
    if os.path.exists(metrics_file):
        try:
            with open(metrics_file, 'r', encoding='utf-8') as f:
                metrics = json.load(f)
            
            if 'feature_importance' in metrics:
                feature_names = [item['feature'] for item in metrics['feature_importance']]
                log.info(f"âœ“ ä»metricsæ–‡ä»¶åŠ è½½ç‰¹å¾åç§°: {len(feature_names)} ä¸ªç‰¹å¾")
        except Exception as e:
            log.warning(f"ä»metricsæ–‡ä»¶åŠ è½½ç‰¹å¾åç§°å¤±è´¥: {e}")
    
    # å¦‚æœæ— æ³•ä»metricsè·å–ï¼Œå°è¯•ä»æ¨¡å‹è·å–
    if feature_names is None:
        if hasattr(booster, 'feature_names'):
            feature_names = booster.feature_names
        elif hasattr(booster, 'feature_names_'):
            feature_names = booster.feature_names_
    
    # å¦‚æœä»ç„¶æ— æ³•è·å–ï¼Œä½¿ç”¨é»˜è®¤ç‰¹å¾é¡ºåº
    if feature_names is None:
        log.warning("æ— æ³•è·å–ç‰¹å¾åç§°ï¼Œä½¿ç”¨é»˜è®¤ç‰¹å¾é¡ºåº")
        feature_names = [
            'close_mean', 'close_std', 'close_max', 'close_min', 'close_trend',
            'pct_chg_mean', 'pct_chg_std', 'pct_chg_sum',
            'positive_days', 'negative_days', 'max_gain', 'max_loss',
            'volume_ratio_mean', 'volume_ratio_max', 'volume_ratio_gt_2', 'volume_ratio_gt_4',
            'macd_mean', 'macd_positive_days', 'macd_max',
            'ma5_mean', 'price_above_ma5', 'ma10_mean', 'price_above_ma10',
            'total_mv_mean', 'circ_mv_mean',
            'return_1w', 'return_2w'
        ]
        log.info(f"ä½¿ç”¨é»˜è®¤ç‰¹å¾é¡ºåº: {len(feature_names)} ä¸ªç‰¹å¾")
    
    # ä»æ–‡ä»¶åæå–æ¨¡å‹ä¿¡æ¯
    model_filename = os.path.basename(model_path)
    model_name = 'xgboost_timeseries'
    model_version = None
    
    # å°è¯•ä»æ–‡ä»¶åæå–ç‰ˆæœ¬ä¿¡æ¯ï¼ˆä¾‹å¦‚ï¼šxgboost_timeseries_v2_20251225_205905.jsonï¼‰
    if '_v' in model_filename:
        # æå–ç‰ˆæœ¬å·éƒ¨åˆ†ï¼ˆv2_20251225_205905ï¼‰
        parts = model_filename.split('_v')
        if len(parts) > 1:
            version_part = parts[1].replace('.json', '')
            model_version = f'v{version_part}'
    
    # è¿”å›æ¨¡å‹å’Œç‰¹å¾åç§°
    class ModelWrapper:
        def __init__(self, booster, feature_names, model_name, model_version, model_path):
            self.booster = booster
            self.feature_names = feature_names
            self.model_name = model_name
            self.model_version = model_version
            self.model_path = model_path
        
        def predict(self, dmatrix):
            """é¢„æµ‹æ¦‚ç‡"""
            return self.booster.predict(dmatrix, output_margin=False, validate_features=False)
    
    log.success("âœ“ æ¨¡å‹åŠ è½½æˆåŠŸ")
    return ModelWrapper(booster, feature_names, model_name, model_version, model_path)


def get_all_stocks(dm, target_date=None):
    """è·å–æ‰€æœ‰Aè‚¡åˆ—è¡¨ï¼Œå¹¶å‰”é™¤ä¸ç¬¦åˆæ¡ä»¶çš„è‚¡ç¥¨"""
    log.info("="*80)
    log.info("è·å–è‚¡ç¥¨åˆ—è¡¨")
    log.info("="*80)
    
    # è·å–æ‰€æœ‰è‚¡ç¥¨
    stock_list = dm.get_stock_list()
    log.info(f"âœ“ è·å–åˆ° {len(stock_list)} åªè‚¡ç¥¨")
    
    # å‰”é™¤è§„åˆ™ï¼ˆå‚è€ƒæ­£æ ·æœ¬ç­›é€‰è§„åˆ™ï¼‰
    excluded_count = {
        'st': 0,
        'new_stock': 0,
        'delisted': 0,
        'bj': 0,  # åŒ—äº¤æ‰€
    }
    
    valid_stocks = []
    
    for _, stock in stock_list.iterrows():
        ts_code = stock['ts_code']
        name = stock['name']
        list_date = stock.get('list_date', '')
        
        # 1. å‰”é™¤STè‚¡ç¥¨
        if 'ST' in name or 'st' in name.lower() or '*' in name:
            excluded_count['st'] += 1
            continue
        
        # 2. å‰”é™¤æ¬¡æ–°è‚¡ï¼ˆä¸Šå¸‚ä¸è¶³120å¤©ï¼‰
        if list_date:
            try:
                # ä½¿ç”¨ç›®æ ‡æ—¥æœŸè®¡ç®—ï¼Œå¦‚æœæœªæŒ‡å®šåˆ™ä½¿ç”¨å½“å‰æ—¥æœŸ
                if target_date is None:
                    target_date = datetime.now()
                elif isinstance(target_date, str):
                    target_date = datetime.strptime(target_date, '%Y%m%d')
                
                days_since_list = (target_date - pd.to_datetime(list_date)).days
                if days_since_list < 120:
                    excluded_count['new_stock'] += 1
                    continue
            except:
                pass
        
        # 3. å‰”é™¤é€€å¸‚è‚¡
        if 'é€€' in name:
            excluded_count['delisted'] += 1
            continue
        
        # 4. å‰”é™¤åŒ—äº¤æ‰€ï¼ˆå¯é€‰ï¼‰
        if ts_code.endswith('.BJ'):
            excluded_count['bj'] += 1
            continue
        
        valid_stocks.append(stock)
    
    df_valid_stocks = pd.DataFrame(valid_stocks)
    
    log.info(f"\nå‰”é™¤ç»Ÿè®¡:")
    log.info(f"  STè‚¡ç¥¨: {excluded_count['st']} åª")
    log.info(f"  æ¬¡æ–°è‚¡: {excluded_count['new_stock']} åª")
    log.info(f"  é€€å¸‚è‚¡: {excluded_count['delisted']} åª")
    log.info(f"  åŒ—äº¤æ‰€: {excluded_count['bj']} åª")
    log.info(f"  æ€»å‰”é™¤: {sum(excluded_count.values())} åª")
    log.info(f"\nâœ“ ç¬¦åˆæ¡ä»¶çš„è‚¡ç¥¨: {len(df_valid_stocks)} åª")
    log.info("")
    
    return df_valid_stocks


def get_stock_features(dm, ts_code, name, lookback_days=34, target_date=None):
    """
    è·å–å•åªè‚¡ç¥¨çš„ç‰¹å¾
    
    Args:
        dm: DataManagerå®ä¾‹
        ts_code: è‚¡ç¥¨ä»£ç 
        name: è‚¡ç¥¨åç§°
        lookback_days: å›çœ‹å¤©æ•°
    
    Returns:
        feature_dict: ç‰¹å¾å­—å…¸ï¼Œå¦‚æœæ•°æ®ä¸è¶³è¿”å›None
    """
    try:
        # ç¡®å®šç›®æ ‡æ—¥æœŸ
        if target_date is None:
            target_date = datetime.now()
        elif isinstance(target_date, str):
            target_date = datetime.strptime(target_date, '%Y%m%d')
        
        # è·å–æœ€è¿‘çš„æ—¥çº¿æ•°æ®
        end_date = target_date.strftime('%Y%m%d')
        start_date = (target_date - timedelta(days=lookback_days*2)).strftime('%Y%m%d')  # å¤šå–ä¸€äº›ä»¥ç¡®ä¿æœ‰è¶³å¤Ÿæ•°æ®
        
        df = dm.get_daily_data(
            stock_code=ts_code,
            start_date=start_date,
            end_date=end_date
        )
        
        if df is None or len(df) < 20:  # è‡³å°‘éœ€è¦20å¤©æ•°æ®
            return None
        
        # å–æœ€è¿‘çš„lookback_dayså¤©
        df = df.tail(lookback_days).sort_values('trade_date')
        
        if len(df) < 20:
            return None
        
        feature_dict = {
            'ts_code': ts_code,
            'name': name,
            'latest_date': df['trade_date'].iloc[-1],
            'latest_close': df['close'].iloc[-1],
        }
        
        # ä»·æ ¼ç‰¹å¾
        feature_dict['close_mean'] = df['close'].mean()
        feature_dict['close_std'] = df['close'].std()
        feature_dict['close_max'] = df['close'].max()
        feature_dict['close_min'] = df['close'].min()
        feature_dict['close_trend'] = (
            (df['close'].iloc[-1] - df['close'].iloc[0]) / 
            df['close'].iloc[0] * 100
        )
        
        # æ¶¨è·Œå¹…ç‰¹å¾
        feature_dict['pct_chg_mean'] = df['pct_chg'].mean()
        feature_dict['pct_chg_std'] = df['pct_chg'].std()
        feature_dict['pct_chg_sum'] = df['pct_chg'].sum()
        feature_dict['positive_days'] = (df['pct_chg'] > 0).sum()
        feature_dict['negative_days'] = (df['pct_chg'] < 0).sum()
        feature_dict['max_gain'] = df['pct_chg'].max()
        feature_dict['max_loss'] = df['pct_chg'].min()
        
        # è®¡ç®—æŠ€æœ¯æŒ‡æ ‡
        # MA
        df['ma5'] = df['close'].rolling(window=5, min_periods=1).mean()
        df['ma10'] = df['close'].rolling(window=10, min_periods=1).mean()
        
        # é‡æ¯”ï¼ˆç®€åŒ–ç‰ˆï¼šå½“æ—¥æˆäº¤é‡/5æ—¥å¹³å‡æˆäº¤é‡ï¼‰
        df['vol_ma5'] = df['vol'].rolling(window=5, min_periods=1).mean()
        df['volume_ratio'] = df['vol'] / df['vol_ma5']
        
        # MACD
        ema12 = df['close'].ewm(span=12, adjust=False).mean()
        ema26 = df['close'].ewm(span=26, adjust=False).mean()
        df['macd_dif'] = ema12 - ema26
        df['macd_dea'] = df['macd_dif'].ewm(span=9, adjust=False).mean()
        df['macd'] = (df['macd_dif'] - df['macd_dea']) * 2
        
        # é‡æ¯”ç‰¹å¾
        feature_dict['volume_ratio_mean'] = df['volume_ratio'].mean()
        feature_dict['volume_ratio_max'] = df['volume_ratio'].max()
        feature_dict['volume_ratio_gt_2'] = (df['volume_ratio'] > 2).sum()
        feature_dict['volume_ratio_gt_4'] = (df['volume_ratio'] > 4).sum()
        
        # MACDç‰¹å¾
        macd_data = df['macd'].dropna()
        if len(macd_data) > 0:
            feature_dict['macd_mean'] = macd_data.mean()
            feature_dict['macd_positive_days'] = (macd_data > 0).sum()
            feature_dict['macd_max'] = macd_data.max()
        else:
            feature_dict['macd_mean'] = 0
            feature_dict['macd_positive_days'] = 0
            feature_dict['macd_max'] = 0
        
        # MAç‰¹å¾
        feature_dict['ma5_mean'] = df['ma5'].mean()
        feature_dict['price_above_ma5'] = (df['close'] > df['ma5']).sum()
        feature_dict['ma10_mean'] = df['ma10'].mean()
        feature_dict['price_above_ma10'] = (df['close'] > df['ma10']).sum()
        
        # å¸‚å€¼ç‰¹å¾ï¼ˆå¦‚æœæœ‰ï¼‰
        if 'total_mv' in df.columns:
            mv_data = df['total_mv'].dropna()
            if len(mv_data) > 0:
                feature_dict['total_mv_mean'] = mv_data.mean()
            else:
                feature_dict['total_mv_mean'] = 0
        else:
            feature_dict['total_mv_mean'] = 0
        
        if 'circ_mv' in df.columns:
            circ_mv_data = df['circ_mv'].dropna()
            if len(circ_mv_data) > 0:
                feature_dict['circ_mv_mean'] = circ_mv_data.mean()
            else:
                feature_dict['circ_mv_mean'] = 0
        else:
            feature_dict['circ_mv_mean'] = 0
        
        # åŠ¨é‡ç‰¹å¾
        days = len(df)
        if days >= 7:
            feature_dict['return_1w'] = (
                (df['close'].iloc[-1] - df['close'].iloc[-7]) /
                df['close'].iloc[-7] * 100
            )
        else:
            feature_dict['return_1w'] = 0
        
        if days >= 14:
            feature_dict['return_2w'] = (
                (df['close'].iloc[-1] - df['close'].iloc[-14]) /
                df['close'].iloc[-14] * 100
            )
        else:
            feature_dict['return_2w'] = 0
        
        return feature_dict
        
    except Exception as e:
        log.warning(f"è·å– {ts_code} {name} ç‰¹å¾å¤±è´¥: {e}")
        return None


def score_all_stocks(dm, model, valid_stocks, batch_size=50, max_stocks=None, target_date=None):
    """
    å¯¹æ‰€æœ‰è‚¡ç¥¨è¿›è¡Œè¯„åˆ†ï¼ˆä¼˜åŒ–ç‰ˆï¼šä½¿ç”¨æ‰¹é‡è·å–ï¼‰
    
    ä¼˜åŒ–ç­–ç•¥ï¼š
    1. å…ˆæ‰¹é‡è·å–æ‰€æœ‰è‚¡ç¥¨çš„æœ€æ–°daily_basicæ•°æ®ï¼ˆä¸€æ¬¡APIè°ƒç”¨ï¼‰
    2. æ‰¹é‡è·å–æ—¥çº¿æ•°æ®ï¼ˆå¹¶å‘ï¼Œæé«˜æ•ˆç‡ï¼‰
    3. å‡å°‘APIè°ƒç”¨æ¬¡æ•°ï¼Œæé«˜é€Ÿåº¦
    """
    log.info("="*80)
    log.info("å¼€å§‹å¯¹æ‰€æœ‰è‚¡ç¥¨è¿›è¡Œè¯„åˆ†ï¼ˆä¼˜åŒ–ç‰ˆï¼šæ‰¹é‡è·å–ï¼‰")
    log.info("="*80)
    
    # å¦‚æœæŒ‡å®šäº†max_stocksï¼Œåªè¯„åˆ†å‰max_stocksåª
    if max_stocks is not None:
        valid_stocks = valid_stocks.head(max_stocks)
        log.info(f"âš ï¸  æµ‹è¯•æ¨¡å¼ï¼šä»…è¯„åˆ†å‰ {max_stocks} åªè‚¡ç¥¨")
    
    total = len(valid_stocks)
    log.info(f"æ€»è‚¡ç¥¨æ•°: {total} åª")
    log.info("")
    
    # ç¡®å®šç›®æ ‡æ—¥æœŸ
    if target_date is None:
        target_date = datetime.now()
    elif isinstance(target_date, str):
        target_date = datetime.strptime(target_date, '%Y%m%d')
    
    target_date_str = target_date.strftime('%Y%m%d')
    
    # ä¼˜åŒ–ï¼šå…ˆæ‰¹é‡è·å–æ‰€æœ‰è‚¡ç¥¨çš„æœ€æ–°daily_basicæ•°æ®ï¼ˆä¸€æ¬¡APIè°ƒç”¨ï¼‰
    log.info("="*80)
    log.info("ä¼˜åŒ–æ­¥éª¤1ï¼šæ‰¹é‡è·å–æ‰€æœ‰è‚¡ç¥¨çš„æœ€æ–°æ¯æ—¥æŒ‡æ ‡")
    log.info("="*80)
    log.info(f"ğŸ“… ç›®æ ‡æ—¥æœŸ: {target_date_str}")
    stock_codes = valid_stocks['ts_code'].tolist()
    
    try:
        df_all_daily_basic = dm.batch_get_daily_basic(target_date_str, stock_codes)
        log.success(f"âœ“ æ‰¹é‡è·å–å®Œæˆ: {len(df_all_daily_basic)} åªè‚¡ç¥¨çš„æœ€æ–°æŒ‡æ ‡")
        
        # åˆ›å»ºå­—å…¸ä¾¿äºå¿«é€ŸæŸ¥æ‰¾ï¼ˆæŒ‰è‚¡ç¥¨ä»£ç ç´¢å¼•ï¼Œå–æœ€æ–°ä¸€æ¡ï¼‰
        daily_basic_dict = {}
        if not df_all_daily_basic.empty:
            # æŒ‰è‚¡ç¥¨ä»£ç åˆ†ç»„ï¼Œå–æœ€æ–°çš„æ•°æ®
            for ts_code, group_df in df_all_daily_basic.groupby('ts_code'):
                latest_row = group_df.iloc[-1]  # å–æœ€æ–°çš„ä¸€æ¡
                daily_basic_dict[ts_code] = latest_row.to_dict()
    except Exception as e:
        log.warning(f"æ‰¹é‡è·å–daily_basicå¤±è´¥ï¼Œå°†ä½¿ç”¨å•è‚¡ç¥¨è·å–: {e}")
        daily_basic_dict = {}
    
    log.info("")
    
    # ä¼˜åŒ–ï¼šæ‰¹é‡è·å–æ—¥çº¿æ•°æ®ï¼ˆå¹¶å‘ï¼‰
    log.info("="*80)
    log.info("ä¼˜åŒ–æ­¥éª¤2ï¼šæ‰¹é‡è·å–æ—¥çº¿æ•°æ®ï¼ˆå¹¶å‘ï¼‰")
    log.info("="*80)
    
    end_date = target_date_str
    start_date = (target_date - timedelta(days=34*2)).strftime('%Y%m%d')
    
    log.info(f"æ‰¹é‡è·å–æ—¥æœŸèŒƒå›´: {start_date} è‡³ {end_date}")
    log.info("ä½¿ç”¨å¹¶å‘è·å–ï¼Œæé«˜æ•ˆç‡...")
    
    # æ‰¹é‡è·å–æ—¥çº¿æ•°æ®
    daily_data_dict = dm.batch_get_daily_data(stock_codes, start_date, end_date)
    log.success(f"âœ“ æ‰¹é‡è·å–å®Œæˆ: {len([k for k, v in daily_data_dict.items() if not v.empty])}/{total} åªè‚¡ç¥¨æˆåŠŸ")
    log.info("")
    
    # æ­¥éª¤3ï¼šè®¡ç®—ç‰¹å¾å¹¶è¯„åˆ†
    log.info("="*80)
    log.info("ä¼˜åŒ–æ­¥éª¤3ï¼šè®¡ç®—ç‰¹å¾å¹¶è¯„åˆ†")
    log.info("="*80)
    
    results = []
    # ä»æ¨¡å‹è·å–ç‰¹å¾åç§°ï¼ˆå¦‚æœå¯ç”¨ï¼‰ï¼Œå¦åˆ™ä½¿ç”¨é»˜è®¤ç‰¹å¾åˆ—è¡¨
    if hasattr(model, 'feature_names') and model.feature_names is not None:
        feature_cols = model.feature_names
        log.info(f"ä½¿ç”¨æ¨¡å‹ä¿å­˜çš„ç‰¹å¾åç§°: {len(feature_cols)} ä¸ªç‰¹å¾")
    else:
        # é»˜è®¤ç‰¹å¾åˆ—è¡¨ï¼ˆä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´ï¼Œå…±21ä¸ªç‰¹å¾ï¼‰
        feature_cols = [
            'close_mean', 'close_std', 'close_max', 'close_min', 'close_trend',
            'pct_chg_mean', 'pct_chg_std', 'pct_chg_sum', 
            'positive_days', 'negative_days', 'max_gain', 'max_loss',
            'volume_ratio_mean', 'volume_ratio_max',
            'macd_mean', 'macd_positive_days',
            'ma5_mean', 'price_above_ma5', 'ma10_mean', 'price_above_ma10'
        ]
        log.warning(f"æ¨¡å‹æœªä¿å­˜ç‰¹å¾åç§°ï¼Œä½¿ç”¨é»˜è®¤ç‰¹å¾åˆ—è¡¨: {len(feature_cols)} ä¸ªç‰¹å¾")
    
    skipped_count = {
        'no_data': 0,
        'insufficient_data': 0,
        'feature_calc_failed': 0,
        'success': 0
    }
    
    # ä¼˜åŒ–ï¼šæ‰¹é‡å¤„ç†ç‰¹å¾å’Œé¢„æµ‹ï¼ˆæå‡10-20å€é€Ÿåº¦ï¼‰
    all_features_list = []
    valid_stock_info = []
    
    log.info("å¼€å§‹è®¡ç®—ç‰¹å¾...")
    
    # ä¼˜åŒ–ï¼šå‡å°‘è°ƒè¯•æ—¥å¿—è¾“å‡ºï¼Œæå‡æ€§èƒ½
    for i, (_, stock) in enumerate(valid_stocks.iterrows()):
        if (i + 1) % 100 == 0 or i == 0 or (i + 1) == total:  # æ¯100åªè¾“å‡ºä¸€æ¬¡ï¼Œä¾¿äºè§‚å¯Ÿè¿›åº¦
            log.info(f"ç‰¹å¾è®¡ç®—è¿›åº¦: {i+1}/{total} ({(i+1)/total*100:.1f}%)")
        
        try:
            ts_code = stock['ts_code']
            name = stock['name']
        except Exception as e:
            if (i + 1) % 100 == 0 or i < 10:
                log.warning(f"æ— æ³•æå–è‚¡ç¥¨ä¿¡æ¯ (i={i}): {e}")
            continue
        
        # ä»æ‰¹é‡è·å–çš„æ•°æ®ä¸­æå–ç‰¹å¾
        df = daily_data_dict.get(ts_code, pd.DataFrame())
        
        if df is None or df.empty:
            skipped_count['no_data'] += 1
            continue
        
        # ç¡®ä¿trade_dateæ˜¯datetimeç±»å‹
        if 'trade_date' in df.columns:
            if not pd.api.types.is_datetime64_any_dtype(df['trade_date']):
                df['trade_date'] = pd.to_datetime(df['trade_date'])
        
        # ç¡®ä¿å¿…è¦çš„åˆ—å­˜åœ¨
        required_cols = ['close', 'pct_chg', 'vol']
        if not all(col in df.columns for col in required_cols):
            skipped_count['insufficient_data'] += 1
            continue
        
        if len(df) < 20:
            skipped_count['insufficient_data'] += 1
            continue
        
        # å–æœ€è¿‘çš„34å¤©
        df = df.tail(34).sort_values('trade_date')
        if len(df) < 20:
            skipped_count['insufficient_data'] += 1
            continue
        
        # åˆå¹¶daily_basicæ•°æ®ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if ts_code in daily_basic_dict:
            basic_row = daily_basic_dict[ts_code]
            # å¦‚æœdfä¸­æ²¡æœ‰è¿™äº›å­—æ®µï¼Œä»daily_basicè¡¥å……
            if 'total_mv' not in df.columns and 'total_mv' in basic_row:
                df['total_mv'] = basic_row['total_mv']
            if 'circ_mv' not in df.columns and 'circ_mv' in basic_row:
                df['circ_mv'] = basic_row['circ_mv']
            # volume_ratioä¼˜å…ˆä½¿ç”¨daily_basicçš„ï¼ˆæ›´å‡†ç¡®ï¼‰
            if 'volume_ratio' in basic_row and pd.notna(basic_row['volume_ratio']):
                # ç”¨daily_basicçš„volume_ratioå¡«å……ç¼ºå¤±å€¼
                if 'volume_ratio' not in df.columns:
                    df['volume_ratio'] = basic_row['volume_ratio']
                else:
                    df['volume_ratio'] = df['volume_ratio'].fillna(basic_row['volume_ratio'])
        
        # å°è¯•è·å–TushareæŠ€æœ¯å› å­ï¼ˆä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼‰
        try:
            # è®¡ç®—æ—¥æœŸèŒƒå›´
            end_date = df['trade_date'].max()
            start_date = df['trade_date'].min()
            if pd.api.types.is_datetime64_any_dtype(df['trade_date']):
                end_date_str = end_date.strftime('%Y%m%d')
                start_date_str = start_date.strftime('%Y%m%d')
            else:
                end_date_str = str(end_date).replace('-', '')
                start_date_str = str(start_date).replace('-', '')
            
            df_factor = dm.get_stk_factor(ts_code, start_date_str, end_date_str)
            if not df_factor.empty:
                # ç¡®ä¿trade_dateæ ¼å¼ä¸€è‡´
                if 'trade_date' in df_factor.columns:
                    if pd.api.types.is_datetime64_any_dtype(df['trade_date']):
                        df_factor['trade_date'] = pd.to_datetime(df_factor['trade_date'])
                    else:
                        df_factor['trade_date'] = pd.to_datetime(df_factor['trade_date'])
                
                # åˆå¹¶æŠ€æœ¯å› å­ï¼ˆä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼‰
                df = pd.merge(
                    df,
                    df_factor[['trade_date', 'macd_dif', 'macd_dea', 'macd', 'rsi_6', 'rsi_12', 'rsi_24']],
                    on='trade_date',
                    how='left'
                )
        except Exception as e:
            # å¦‚æœè·å–æŠ€æœ¯å› å­å¤±è´¥ï¼Œç»§ç»­ä½¿ç”¨æœ¬åœ°è®¡ç®—
            pass
        
        # è®¡ç®—ç‰¹å¾ï¼ˆå¤ç”¨åŸæœ‰é€»è¾‘ï¼Œä½†ä¼˜å…ˆä½¿ç”¨TushareæŠ€æœ¯å› å­ï¼‰
        try:
            features = _calculate_features_from_df(df, ts_code, name, debug_log=None)  # å…³é—­è°ƒè¯•æ—¥å¿—ä»¥æå‡æ€§èƒ½
        except Exception as e:
            skipped_count['feature_calc_failed'] += 1
            if (i + 1) % 100 == 0 or i < 10:  # å‰10åªæˆ–æ¯100åªè®°å½•ä¸€æ¬¡é”™è¯¯
                log.warning(f"ç‰¹å¾è®¡ç®—å¤±è´¥ {ts_code} ({i+1}/{total}): {e}")
            continue
        
        if features is None:
            skipped_count['feature_calc_failed'] += 1
            continue
        
        # ä¿å­˜ç‰¹å¾å’Œè‚¡ç¥¨ä¿¡æ¯ï¼Œç”¨äºæ‰¹é‡é¢„æµ‹
        all_features_list.append(features)
        valid_stock_info.append({
            'ts_code': ts_code,
            'name': name,
            'features': features
        })
    
    log.info(f"ç‰¹å¾è®¡ç®—å®Œæˆ: {len(all_features_list)} åªè‚¡ç¥¨")
    log.info("å¼€å§‹æ‰¹é‡é¢„æµ‹...")
    
    # ä¼˜åŒ–ï¼šæ‰¹é‡é¢„æµ‹ï¼ˆæå‡10-20å€é€Ÿåº¦ï¼‰
    if all_features_list:
        try:
            # æ‰¹é‡æå–ç‰¹å¾å€¼ï¼ˆä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼šå¦‚æœç‰¹å¾ä¸å­˜åœ¨ï¼Œä½¿ç”¨0å¡«å……ï¼‰
            all_feature_values = []
            for features in all_features_list:
                feature_values = []
                for col in feature_cols:
                    # å¦‚æœç‰¹å¾ä¸å­˜åœ¨ï¼Œä½¿ç”¨0ï¼ˆä¸è®­ç»ƒæ—¶DataFrameçš„fillnaè¡Œä¸ºä¸€è‡´ï¼‰
                    value = features.get(col, 0)
                    if pd.isna(value):
                        value = 0
                    feature_values.append(value)
                all_feature_values.append(feature_values)
            
            # æ‰¹é‡æ„å»ºDMatrixå¹¶é¢„æµ‹
            dmatrix = xgb.DMatrix(all_feature_values, feature_names=feature_cols)
            all_probs = model.predict(dmatrix)  # æ‰¹é‡é¢„æµ‹ï¼Œä¸€æ¬¡å®Œæˆ
            
            # æ„å»ºç»“æœ
            for i, stock_info in enumerate(valid_stock_info):
                features = stock_info['features']
                prob = float(all_probs[i])
                
                results.append({
                    'è‚¡ç¥¨ä»£ç ': stock_info['ts_code'],
                    'è‚¡ç¥¨åç§°': stock_info['name'],
                    'ç‰›è‚¡æ¦‚ç‡': prob,
                    'æ•°æ®æ—¥æœŸ': features['latest_date'],
                    'æœ€æ–°ä»·æ ¼': features['latest_close'],
                    '34æ—¥æ¶¨å¹…%': round(features['close_trend'], 2),
                    'ç´¯è®¡æ¶¨è·Œ%': round(features['pct_chg_sum'], 2),
                    '1å‘¨æ¶¨å¹…%': round(features['return_1w'], 2),
                    '2å‘¨æ¶¨å¹…%': round(features['return_2w'], 2),
                })
                skipped_count['success'] += 1
                
        except Exception as e:
            log.error(f"æ‰¹é‡é¢„æµ‹å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            # å›é€€åˆ°é€ä¸ªé¢„æµ‹
            log.warning("å›é€€åˆ°é€ä¸ªé¢„æµ‹æ¨¡å¼...")
            for stock_info in valid_stock_info:
                try:
                    features = stock_info['features']
                    feature_values = []
                    for col in feature_cols:
                        value = features.get(col, 0)
                        if pd.isna(value):
                            value = 0
                        feature_values.append(value)
                    
                    dmatrix = xgb.DMatrix([feature_values], feature_names=feature_cols)
                    prob = model.predict(dmatrix)[0]
                    
                    results.append({
                        'è‚¡ç¥¨ä»£ç ': stock_info['ts_code'],
                        'è‚¡ç¥¨åç§°': stock_info['name'],
                        'ç‰›è‚¡æ¦‚ç‡': float(prob),
                        'æ•°æ®æ—¥æœŸ': features['latest_date'],
                        'æœ€æ–°ä»·æ ¼': features['latest_close'],
                        '34æ—¥æ¶¨å¹…%': round(features['close_trend'], 2),
                        'ç´¯è®¡æ¶¨è·Œ%': round(features['pct_chg_sum'], 2),
                        '1å‘¨æ¶¨å¹…%': round(features['return_1w'], 2),
                        '2å‘¨æ¶¨å¹…%': round(features['return_2w'], 2),
                    })
                    skipped_count['success'] += 1
                except Exception as e:
                    skipped_count['feature_calc_failed'] += 1
                    continue
    
    log.success(f"\nâœ“ è¯„åˆ†å®Œæˆï¼å…±è¯„åˆ† {len(results)} åªè‚¡ç¥¨")
    log.info("\nè·³è¿‡ç»Ÿè®¡:")
    log.info(f"  - æ— æ•°æ®: {skipped_count['no_data']} åª")
    log.info(f"  - æ•°æ®ä¸è¶³: {skipped_count['insufficient_data']} åª")
    log.info(f"  - ç‰¹å¾è®¡ç®—å¤±è´¥: {skipped_count['feature_calc_failed']} åª")
    log.info(f"  - æˆåŠŸè¯„åˆ†: {skipped_count['success']} åª")
    log.info("")
    
    if len(results) == 0:
        log.error("âš ï¸  æ²¡æœ‰æˆåŠŸè¯„åˆ†ä»»ä½•è‚¡ç¥¨ï¼")
        log.error("   å¯èƒ½åŸå› ï¼š")
        log.error("   1. æ‰¹é‡è·å–çš„æ•°æ®æ ¼å¼ä¸æ­£ç¡®")
        log.error("   2. ç‰¹å¾è®¡ç®—å‡½æ•°æœ‰é—®é¢˜")
        log.error("   3. æ•°æ®åˆ—åä¸åŒ¹é…")
        log.error(f"   è¯·æ£€æŸ¥ï¼šdaily_data_dictä¸­æœ‰ {len([k for k, v in daily_data_dict.items() if not v.empty])} åªè‚¡ç¥¨æœ‰æ•°æ®")
    
    return pd.DataFrame(results)


def _calculate_features_from_df(df, ts_code, name, debug_log=None):
    """
    ä»DataFrameè®¡ç®—ç‰¹å¾ï¼ˆä»get_stock_featuresä¸­æå–çš„é€»è¾‘ï¼‰
    
    Args:
        df: æ—¥çº¿æ•°æ®DataFrameï¼ˆå·²åŒ…å«34å¤©æ•°æ®ï¼‰
        ts_code: è‚¡ç¥¨ä»£ç 
        name: è‚¡ç¥¨åç§°
        debug_log: è°ƒè¯•æ—¥å¿—å‡½æ•°ï¼ˆå¯é€‰ï¼‰
        
    Returns:
        feature_dict: ç‰¹å¾å­—å…¸
    """
    try:
        if df is None or len(df) < 20:
            return None
        
        # ç¡®ä¿æ•°æ®æ˜¯æ•°å€¼ç±»å‹ï¼Œé¿å…è®¡ç®—å¡ä½ï¼ˆä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼šåªè½¬æ¢å¿…è¦åˆ—ï¼‰
        numeric_cols = ['close', 'pct_chg', 'vol']
        for col in numeric_cols:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors='coerce')
        
        # æ³¨æ„ï¼šä¸åœ¨è¿™é‡Œfillna(0)ï¼Œä¸è®­ç»ƒæ—¶ä¸€è‡´
        # è®­ç»ƒæ—¶æ˜¯åœ¨ç‰¹å¾æå–å®Œæˆåï¼Œæ„å»ºDataFrameæ—¶æ‰fillna(0)
        # è¿™é‡Œåªå¯¹å¿…è¦çš„åŸºç¡€åˆ—è¿›è¡Œæ•°å€¼è½¬æ¢ï¼Œå…¶ä»–åˆ—ä¿æŒåŸæ ·
        
        feature_dict = {
            'ts_code': ts_code,
            'name': name,
            'latest_date': df['trade_date'].iloc[-1],
            'latest_close': df['close'].iloc[-1],
        }
        
        # ä»·æ ¼ç‰¹å¾
        feature_dict['close_mean'] = df['close'].mean()
        feature_dict['close_std'] = df['close'].std()
        feature_dict['close_max'] = df['close'].max()
        feature_dict['close_min'] = df['close'].min()
        feature_dict['close_trend'] = (
            (df['close'].iloc[-1] - df['close'].iloc[0]) / 
            df['close'].iloc[0] * 100
        )
        
        # æ¶¨è·Œå¹…ç‰¹å¾
        feature_dict['pct_chg_mean'] = df['pct_chg'].mean()
        feature_dict['pct_chg_std'] = df['pct_chg'].std()
        feature_dict['pct_chg_sum'] = df['pct_chg'].sum()
        feature_dict['positive_days'] = (df['pct_chg'] > 0).sum()
        feature_dict['negative_days'] = (df['pct_chg'] < 0).sum()
        feature_dict['max_gain'] = df['pct_chg'].max()
        feature_dict['max_loss'] = df['pct_chg'].min()
        
        # è®¡ç®—æŠ€æœ¯æŒ‡æ ‡ï¼ˆä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼šä¼˜å…ˆä½¿ç”¨Tushareæ•°æ®ï¼Œç¼ºå¤±æ—¶å†è®¡ç®—ï¼‰
        # MA5å’ŒMA10ï¼ˆå¦‚æœTushareæ²¡æœ‰æä¾›ï¼Œåˆ™æœ¬åœ°è®¡ç®—ï¼‰
        if 'ma5' not in df.columns:
            df['ma5'] = df['close'].rolling(window=5, min_periods=1).mean()
        if 'ma10' not in df.columns:
            df['ma10'] = df['close'].rolling(window=10, min_periods=1).mean()
        
        # é‡æ¯”ï¼ˆå¦‚æœdaily_basicæ²¡æœ‰ï¼Œåˆ™è®¡ç®—ï¼‰
        if 'volume_ratio' not in df.columns:
            df['vol_ma5'] = df['vol'].rolling(window=5, min_periods=1).mean()
            df['volume_ratio'] = df['vol'] / df['vol_ma5']
        
        # MACDï¼ˆä¼˜å…ˆä½¿ç”¨TushareæŠ€æœ¯å› å­ï¼Œç¼ºå¤±æ—¶å†è®¡ç®—ï¼Œä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼‰
        if 'macd' not in df.columns:
            try:
                ema12 = df['close'].ewm(span=12, adjust=False).mean()
                ema26 = df['close'].ewm(span=26, adjust=False).mean()
                df['macd_dif'] = ema12 - ema26
                df['macd_dea'] = df['macd_dif'].ewm(span=9, adjust=False).mean()
                df['macd'] = (df['macd_dif'] - df['macd_dea']) * 2
            except Exception as e:
                # å¦‚æœMACDè®¡ç®—å¤±è´¥ï¼Œä¸è®¾ç½®macdåˆ—ï¼ˆä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼‰
                pass
        
        # é‡æ¯”ç‰¹å¾ï¼ˆä¸è®­ç»ƒæ—¶å®Œå…¨ä¸€è‡´ï¼šå¦‚æœåˆ—å­˜åœ¨æ‰è®¾ç½®ç‰¹å¾ï¼‰
        if 'volume_ratio' in df.columns:
            feature_dict['volume_ratio_mean'] = df['volume_ratio'].mean()
            feature_dict['volume_ratio_max'] = df['volume_ratio'].max()
            feature_dict['volume_ratio_gt_2'] = (df['volume_ratio'] > 2).sum()
            feature_dict['volume_ratio_gt_4'] = (df['volume_ratio'] > 4).sum()
        
        # MACDç‰¹å¾ï¼ˆä¸è®­ç»ƒæ—¶å®Œå…¨ä¸€è‡´ï¼šå¦‚æœåˆ—å­˜åœ¨æ‰è®¾ç½®ç‰¹å¾ï¼‰
        if 'macd' in df.columns:
            macd_data = df['macd'].dropna()
            if len(macd_data) > 0:
                feature_dict['macd_mean'] = macd_data.mean()
                feature_dict['macd_positive_days'] = (macd_data > 0).sum()
                feature_dict['macd_max'] = macd_data.max()
        
        # MAç‰¹å¾ï¼ˆä¸è®­ç»ƒæ—¶å®Œå…¨ä¸€è‡´ï¼šå¦‚æœåˆ—å­˜åœ¨æ‰è®¾ç½®ç‰¹å¾ï¼‰
        if 'ma5' in df.columns:
            feature_dict['ma5_mean'] = df['ma5'].mean()
            feature_dict['price_above_ma5'] = (df['close'] > df['ma5']).sum()
        
        if 'ma10' in df.columns:
            feature_dict['ma10_mean'] = df['ma10'].mean()
            feature_dict['price_above_ma10'] = (df['close'] > df['ma10']).sum()
        
        # å¸‚å€¼ç‰¹å¾ï¼ˆä¸è®­ç»ƒæ—¶å®Œå…¨ä¸€è‡´ï¼šå¦‚æœåˆ—å­˜åœ¨ä¸”æ•°æ®æœ‰æ•ˆæ‰è®¾ç½®ç‰¹å¾ï¼‰
        if 'total_mv' in df.columns:
            mv_data = df['total_mv'].dropna()
            if len(mv_data) > 0:
                feature_dict['total_mv_mean'] = mv_data.mean()
        
        if 'circ_mv' in df.columns:
            circ_mv_data = df['circ_mv'].dropna()
            if len(circ_mv_data) > 0:
                feature_dict['circ_mv_mean'] = circ_mv_data.mean()
        
        # åŠ¨é‡ç‰¹å¾ï¼ˆä¸è®­ç»ƒæ—¶å®Œå…¨ä¸€è‡´ï¼šå¦‚æœæ•°æ®è¶³å¤Ÿæ‰è®¾ç½®ç‰¹å¾ï¼‰
        days = len(df)
        if days >= 7:
            feature_dict['return_1w'] = (
                (df['close'].iloc[-1] - df['close'].iloc[-7]) /
                df['close'].iloc[-7] * 100
            )
        if days >= 14:
            feature_dict['return_2w'] = (
                (df['close'].iloc[-1] - df['close'].iloc[-14]) /
                df['close'].iloc[-14] * 100
            )
        
        if debug_log:
            debug_log("A", f"score_current_stocks.py:{970}", "Function exit success", {
                "ts_code": ts_code,
                "features_count": len(feature_dict)
            })
        
        return feature_dict
        
    except Exception as e:
        if debug_log:
            debug_log("D", f"score_current_stocks.py:{975}", "Function exit exception", {
                "ts_code": ts_code,
                "error": str(e)
            })
        log.warning(f"è®¡ç®— {ts_code} {name} ç‰¹å¾å¤±è´¥: {e}")
        return None


def analyze_and_output_results(df_scores, top_n=50):
    """åˆ†æå’Œè¾“å‡ºè¯„åˆ†ç»“æœ"""
    log.info("="*80)
    log.info("è¯„åˆ†ç»“æœåˆ†æ")
    log.info("="*80)
    
    # æŒ‰æ¦‚ç‡æ’åº
    df_scores = df_scores.sort_values('ç‰›è‚¡æ¦‚ç‡', ascending=False).reset_index(drop=True)
    
    # ç»Ÿè®¡ä¿¡æ¯
    log.info(f"\næ¦‚ç‡åˆ†å¸ƒ:")
    log.info(f"  æœ€é«˜: {df_scores['ç‰›è‚¡æ¦‚ç‡'].max():.4f}")
    log.info(f"  æœ€ä½: {df_scores['ç‰›è‚¡æ¦‚ç‡'].min():.4f}")
    log.info(f"  å¹³å‡: {df_scores['ç‰›è‚¡æ¦‚ç‡'].mean():.4f}")
    log.info(f"  ä¸­ä½æ•°: {df_scores['ç‰›è‚¡æ¦‚ç‡'].median():.4f}")
    
    # Top N æ¨è
    log.info(f"\n{'='*80}")
    log.info(f"Top {top_n} æ¨èè‚¡ç¥¨ï¼ˆæœ€æœ‰å¯èƒ½æˆä¸ºç‰›è‚¡ï¼‰")
    log.info(f"{'='*80}")
    
    df_top = df_scores.head(top_n)
    
    log.info(f"\n{'åºå·':<4} {'ä»£ç ':<12} {'åç§°':<10} {'æ¦‚ç‡':<8} {'æœ€æ–°ä»·':<8} {'34æ—¥%':<8} {'1å‘¨%':<8} {'2å‘¨%':<8}")
    log.info("-" * 80)
    
    for i, row in df_top.iterrows():
        log.info(
            f"{i+1:<4} {row['è‚¡ç¥¨ä»£ç ']:<12} {row['è‚¡ç¥¨åç§°']:<10} "
            f"{row['ç‰›è‚¡æ¦‚ç‡']:<8.4f} {row['æœ€æ–°ä»·æ ¼']:<8.2f} "
            f"{row['34æ—¥æ¶¨å¹…%']:<8.2f} {row['1å‘¨æ¶¨å¹…%']:<8.2f} {row['2å‘¨æ¶¨å¹…%']:<8.2f}"
        )
    
    return df_top


def generate_prediction_report(df_scores, df_top, top_n=50, model_path=None, target_date=None):
    """ç”Ÿæˆé¢„æµ‹æŠ¥å‘Š"""
    if target_date is None:
        timestamp = datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M')
        date_str = datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥')
    else:
        if isinstance(target_date, str):
            target_date = datetime.strptime(target_date, '%Y%m%d')
        timestamp = target_date.strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M')
        date_str = target_date.strftime('%Yå¹´%mæœˆ%dæ—¥')
    
    report = []
    report.append("=" * 80)
    report.append("ğŸ“Š é‡åŒ–é€‰è‚¡é¢„æµ‹æŠ¥å‘Š")
    report.append("=" * 80)
    report.append(f"\nğŸ“… æŠ¥å‘Šæ—¶é—´: {timestamp}")
    if target_date is not None:
        report.append(f"ğŸ“… æ•°æ®æ—¥æœŸ: {date_str}ï¼ˆå†å²å›æµ‹ï¼‰")
    
    # è·å–æ¨¡å‹ä¿¡æ¯
    if model_path is None:
        import glob
        model_files = glob.glob('models/breakout_launch_scorer_*.json')
        if model_files:
            model_path = max(model_files, key=os.path.getmtime)
    
    model_version = "çªç ´èµ·çˆ†è¯„åˆ†æ¨¡å‹"
    if model_path:
        model_name = os.path.basename(model_path)
        model_version = f"çªç ´èµ·çˆ†è¯„åˆ†æ¨¡å‹ ({model_name})"
    
    report.append(f"ğŸ¤– æ¨¡å‹ç‰ˆæœ¬: {model_version}")
    report.append(f"ğŸ“ˆ è¯„åˆ†è‚¡ç¥¨: {len(df_scores)} åª")
    report.append(f"ğŸ¯ æ¨èæ•°é‡: {top_n} åª")
    report.append(f"ğŸ” ç­›é€‰æ–¹å¼: ä»…æ¨¡å‹è¯„åˆ†ï¼ˆå·²ç§»é™¤è´¢åŠ¡ç­›é€‰ï¼‰")
    
    # æ•´ä½“å¸‚åœºåˆ†æ
    report.append("\n" + "=" * 80)
    report.append("ä¸€ã€æ•´ä½“å¸‚åœºåˆ†æ")
    report.append("=" * 80)
    
    high_prob_count = len(df_scores[df_scores['ç‰›è‚¡æ¦‚ç‡'] > 0.8])
    mid_prob_count = len(df_scores[(df_scores['ç‰›è‚¡æ¦‚ç‡'] >= 0.6) & (df_scores['ç‰›è‚¡æ¦‚ç‡'] <= 0.8)])
    low_prob_count = len(df_scores[df_scores['ç‰›è‚¡æ¦‚ç‡'] < 0.6])
    
    report.append(f"\n1. æ¦‚ç‡åˆ†å¸ƒç»Ÿè®¡")
    report.append(f"   - é«˜æ½œåŠ›è‚¡ç¥¨ï¼ˆæ¦‚ç‡>80%ï¼‰: {high_prob_count} åª ({high_prob_count/len(df_scores)*100:.1f}%)")
    report.append(f"   - ä¸­æ½œåŠ›è‚¡ç¥¨ï¼ˆæ¦‚ç‡60-80%ï¼‰: {mid_prob_count} åª ({mid_prob_count/len(df_scores)*100:.1f}%)")
    report.append(f"   - ä½æ½œåŠ›è‚¡ç¥¨ï¼ˆæ¦‚ç‡<60%ï¼‰: {low_prob_count} åª ({low_prob_count/len(df_scores)*100:.1f}%)")
    
    report.append(f"\n2. å¸‚åœºæƒ…ç»ªæŒ‡æ ‡")
    avg_34d = df_scores['34æ—¥æ¶¨å¹…%'].mean()
    avg_1w = df_scores['1å‘¨æ¶¨å¹…%'].mean()
    avg_2w = df_scores['2å‘¨æ¶¨å¹…%'].mean()
    
    report.append(f"   - å¹³å‡34æ—¥æ¶¨å¹…: {avg_34d:.2f}%")
    report.append(f"   - å¹³å‡1å‘¨æ¶¨å¹…: {avg_1w:.2f}%")
    report.append(f"   - å¹³å‡2å‘¨æ¶¨å¹…: {avg_2w:.2f}%")
    
    if avg_1w > avg_2w > 0:
        market_trend = "ğŸ“ˆ å¸‚åœºå¤„äºåŠ é€Ÿä¸Šæ¶¨é˜¶æ®µ"
    elif avg_1w > 0 and avg_2w > 0:
        market_trend = "ğŸ“Š å¸‚åœºä¿æŒä¸Šæ¶¨è¶‹åŠ¿"
    elif avg_1w < 0 and avg_2w < 0:
        market_trend = "ğŸ“‰ å¸‚åœºå¤„äºè°ƒæ•´é˜¶æ®µ"
    else:
        market_trend = "ğŸ”„ å¸‚åœºéœ‡è¡æ•´ç†ä¸­"
    
    report.append(f"\n3. å¸‚åœºè¶‹åŠ¿åˆ¤æ–­")
    report.append(f"   {market_trend}")
    
    # Top 10 è¯¦ç»†åˆ†æ
    report.append("\n" + "=" * 80)
    report.append("äºŒã€Top 10 é‡ç‚¹æ¨è")
    report.append("=" * 80)
    
    for i, row in df_top.head(10).iterrows():
        report.append(f"\nã€ç¬¬ {i+1} åã€‘{row['è‚¡ç¥¨åç§°']}ï¼ˆ{row['è‚¡ç¥¨ä»£ç ']}ï¼‰")
        report.append(f"  ğŸ¯ ç‰›è‚¡æ¦‚ç‡: {row['ç‰›è‚¡æ¦‚ç‡']*100:.2f}%")
        report.append(f"  ğŸ’° æœ€æ–°ä»·æ ¼: {row['æœ€æ–°ä»·æ ¼']:.2f} å…ƒ")
        report.append(f"  ğŸ“Š 34æ—¥æ¶¨å¹…: {row['34æ—¥æ¶¨å¹…%']:.2f}%")
        report.append(f"  ğŸ“ˆ 1å‘¨æ¶¨å¹…: {row['1å‘¨æ¶¨å¹…%']:.2f}%")
        report.append(f"  ğŸ“ˆ 2å‘¨æ¶¨å¹…: {row['2å‘¨æ¶¨å¹…%']:.2f}%")
        
        # æ¨èç†ç”±
        prob = row['ç‰›è‚¡æ¦‚ç‡']
        trend_34d = row['34æ—¥æ¶¨å¹…%']
        trend_1w = row['1å‘¨æ¶¨å¹…%']
        trend_2w = row['2å‘¨æ¶¨å¹…%']
        
        reasons = []
        if prob > 0.9:
            reasons.append("âœ… æ¨¡å‹æåº¦çœ‹å¥½ï¼Œå†å²ç›¸ä¼¼æƒ…å†µæˆåŠŸç‡>90%")
        elif prob > 0.8:
            reasons.append("âœ… æ¨¡å‹å¼ºçƒˆçœ‹å¥½ï¼Œå†å²ç›¸ä¼¼æƒ…å†µæˆåŠŸç‡>80%")
        
        if trend_1w > trend_2w and trend_1w > 5:
            reasons.append("âœ… è¿‘æœŸå‘ˆåŠ é€Ÿä¸Šæ¶¨è¶‹åŠ¿ï¼ŒåŠ¨èƒ½å¼ºåŠ²")
        elif trend_1w > 0 and trend_2w > 0:
            reasons.append("âœ… çŸ­æœŸèµ°åŠ¿ç¨³å¥ï¼Œä¿æŒä¸Šæ¶¨åŠ¨èƒ½")
        elif trend_1w > 0 and trend_34d < 0:
            reasons.append("âœ… ç»è¿‡è°ƒæ•´åå¼€å§‹åå¼¹ï¼Œå¯èƒ½å¤„äºåº•éƒ¨åŒºåŸŸ")
        
        if trend_34d > 50:
            reasons.append("âš ï¸ 34æ—¥æ¶¨å¹…è¾ƒå¤§ï¼Œæ³¨æ„å›è°ƒé£é™©")
        
        if reasons:
            report.append(f"  ğŸ“ æ¨èç†ç”±:")
            for reason in reasons:
                report.append(f"     {reason}")
    
    # æŠ•èµ„å»ºè®®
    report.append("\n" + "=" * 80)
    report.append("ä¸‰ã€æŠ•èµ„å»ºè®®")
    report.append("=" * 80)
    
    report.append("\n1. é€‰è‚¡ç­–ç•¥")
    report.append("   âœ… ä¼˜å…ˆå…³æ³¨æ¦‚ç‡>80%çš„é«˜æ½œåŠ›è‚¡ç¥¨")
    report.append("   âœ… ç»“åˆä¸ªè‚¡æŠ€æœ¯é¢å’ŒåŸºæœ¬é¢è¿›è¡ŒäºŒæ¬¡ç­›é€‰")
    report.append("   âœ… å…³æ³¨æˆäº¤é‡é…åˆï¼Œé¿å…æ— é‡ä¸Šæ¶¨")
    
    report.append("\n2. ä»“ä½ç®¡ç†")
    report.append("   ğŸ’° å•åªè‚¡ç¥¨ä¸è¶…è¿‡æ€»èµ„é‡‘çš„5-10%")
    report.append("   ğŸ’° å»ºè®®åˆ†æ‰¹å»ºä»“ï¼Œä¸è¦ä¸€æ¬¡æ€§æ»¡ä»“")
    report.append("   ğŸ’° Top 10ä¸­é€‰æ‹©3-5åªåˆ†æ•£é…ç½®")
    
    report.append("\n3. é£é™©æ§åˆ¶")
    report.append("   ğŸ›¡ï¸ è®¾ç½®æ­¢æŸä½ï¼šå»ºè®®-15%æ­¢æŸ")
    report.append("   ğŸ›¡ï¸ è®¾ç½®æ­¢ç›ˆä½ï¼šè¾¾åˆ°+50%åˆ†æ‰¹æ­¢ç›ˆ")
    report.append("   ğŸ›¡ï¸ æŒä»“æ—¶é—´ï¼šå»ºè®®æŒæœ‰3-6å‘¨è§‚å¯Ÿ")
    
    report.append("\n4. è·Ÿè¸ªä¸è°ƒæ•´")
    report.append("   ğŸ“Š æ¯å‘¨é‡æ–°è¿è¡Œè¯„åˆ†ï¼Œæ›´æ–°æ¨èåˆ—è¡¨")
    report.append("   ğŸ“Š è·Ÿè¸ªæ¨èè‚¡ç¥¨å®é™…è¡¨ç°ï¼ŒéªŒè¯æ¨¡å‹å‡†ç¡®æ€§")
    report.append("   ğŸ“Š æ ¹æ®å¸‚åœºå˜åŒ–åŠæ—¶è°ƒæ•´æŒä»“")
    
    # é£é™©æç¤º
    report.append("\n" + "=" * 80)
    report.append("å››ã€é£é™©æç¤º")
    report.append("=" * 80)
    
    report.append("\nâš ï¸  é‡è¦å£°æ˜:")
    report.append("   1. æœ¬æŠ¥å‘ŠåŸºäºå†å²æ•°æ®è®­ç»ƒçš„é‡åŒ–æ¨¡å‹ç”Ÿæˆï¼Œä¸æ„æˆæŠ•èµ„å»ºè®®")
    report.append("   2. è‚¡å¸‚æœ‰é£é™©ï¼ŒæŠ•èµ„éœ€è°¨æ…ï¼Œå†å²è¡¨ç°ä¸ä»£è¡¨æœªæ¥æ”¶ç›Š")
    report.append("   3. å»ºè®®ç»“åˆåŸºæœ¬é¢åˆ†æã€å¸‚åœºç¯å¢ƒç­‰å¤šæ–¹é¢å› ç´ ç»¼åˆåˆ¤æ–­")
    report.append("   4. è¯·æ ¹æ®è‡ªèº«é£é™©æ‰¿å—èƒ½åŠ›åˆç†é…ç½®èµ„é‡‘")
    report.append("   5. å¦‚æœ‰ç–‘é—®ï¼Œå»ºè®®å’¨è¯¢ä¸“ä¸šæŠ•èµ„é¡¾é—®")
    
    report.append("\n" + "=" * 80)
    report.append("æŠ¥å‘Šç»“æŸ")
    report.append("=" * 80)
    
    return "\n".join(report)


def save_results(df_scores, df_top, top_n=50, model_path=None, model_name=None, model_version=None, target_date=None):
    """ä¿å­˜ç»“æœï¼ˆåŒ…å«å…ƒæ•°æ®ï¼Œç”¨äºåç»­å‡†ç¡®ç‡åˆ†æï¼‰
    
    Args:
        df_scores: å®Œæ•´è¯„åˆ†ç»“æœDataFrame
        df_top: Top Næ¨èç»“æœDataFrame
        top_n: Top Næ•°é‡
        model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾„
        model_name: æ¨¡å‹åç§°ï¼ˆå¦‚ 'breakout_launch_scorer'ï¼‰
        model_version: æ¨¡å‹ç‰ˆæœ¬ï¼ˆå¦‚ 'v1.0.0'ï¼‰
        target_date: ç›®æ ‡æ—¥æœŸ
    """
    # ç¡®å®šé¢„æµ‹æ—¥æœŸ
    if target_date is None:
        prediction_date = datetime.now()
        is_backtest = False
    else:
        if isinstance(target_date, str):
            prediction_date = datetime.strptime(target_date, '%Y%m%d')
        else:
            prediction_date = target_date
        is_backtest = True
    
    prediction_date_str = prediction_date.strftime('%Y%m%d')
    timestamp = prediction_date.strftime('%Y%m%d_%H%M%S') if not is_backtest else prediction_date_str
    
    # æ„å»ºæ–‡ä»¶ååç¼€ï¼ˆåŒ…å«æ¨¡å‹åç§°å’Œç‰ˆæœ¬ï¼‰
    model_suffix = ''
    if model_name:
        model_suffix = f'_{model_name}'
    if model_version:
        model_suffix += f'_{model_version}'
    
    # ä¿å­˜å®Œæ•´è¯„åˆ†ç»“æœï¼ˆä½¿ç”¨æ–°çš„ç›®å½•ç»“æ„ï¼‰
    output_dir = 'data/prediction/results'
    metadata_dir = 'data/prediction/metadata'
    os.makedirs(output_dir, exist_ok=True)
    os.makedirs(metadata_dir, exist_ok=True)
    
    scores_file = f'{output_dir}/stock_scores_{prediction_date_str}{model_suffix}.csv'
    df_scores.to_csv(scores_file, index=False, encoding='utf-8-sig')
    log.success(f"\nâœ“ å®Œæ•´è¯„åˆ†ç»“æœå·²ä¿å­˜: {scores_file}")
    
    # ä¿å­˜Top Næ¨è
    top_file = f'{output_dir}/top_{top_n}_stocks_{prediction_date_str}{model_suffix}.csv'
    df_top.to_csv(top_file, index=False, encoding='utf-8-sig')
    log.success(f"âœ“ Top {top_n} æ¨èå·²ä¿å­˜: {top_file}")
    
    # ç”Ÿæˆé¢„æµ‹æŠ¥å‘Š
    report_content = generate_prediction_report(df_scores, df_top, top_n, model_path=model_path, target_date=target_date)
    report_file = f'{output_dir}/prediction_report_{prediction_date_str}{model_suffix}.txt'
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(report_content)
    log.success(f"âœ“ é¢„æµ‹æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
    
    # ä¿å­˜é¢„æµ‹å…ƒæ•°æ®ï¼ˆç”¨äºåç»­å‡†ç¡®ç‡åˆ†æï¼‰
    metadata = {
        'prediction_date': prediction_date_str,
        'prediction_timestamp': prediction_date.strftime('%Y-%m-%d %H:%M:%S'),
        'is_backtest': is_backtest,
        'model_name': model_name,
        'model_version': model_version,
        'model_path': str(model_path) if model_path else None,
        'total_scored': len(df_scores),
        'top_n': top_n,
        'top_stocks': [
            {
                'rank': i + 1,
                'code': row['è‚¡ç¥¨ä»£ç '],
                'name': row['è‚¡ç¥¨åç§°'],
                'probability': float(row['ç‰›è‚¡æ¦‚ç‡']),
                'price': float(row['æœ€æ–°ä»·æ ¼']),
                'date': str(row.get('æ•°æ®æ—¥æœŸ', ''))
            }
            for i, row in df_top.iterrows()
        ],
        'scores_file': scores_file,
        'top_file': top_file,
        'report_file': report_file
    }
    
    metadata_file = f'{metadata_dir}/prediction_metadata_{prediction_date_str}{model_suffix}.json'
    with open(metadata_file, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, indent=2, ensure_ascii=False)
    log.success(f"âœ“ é¢„æµ‹å…ƒæ•°æ®å·²ä¿å­˜: {metadata_file}")
    
    # åŒæ—¶æ‰“å°æŠ¥å‘Šå†…å®¹
    log.info("\n" + report_content)
    
    return scores_file, top_file, report_file, metadata_file


def main():
    """ä¸»å‡½æ•°"""
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description='è‚¡ç¥¨è¯„åˆ†ç³»ç»Ÿ')
    parser.add_argument('--date', type=str, default=None,
                        help='æŒ‡å®šæ—¥æœŸï¼ˆæ ¼å¼ï¼šYYYYMMDDï¼‰ï¼Œç”¨äºå†å²å›æµ‹ã€‚ä¾‹å¦‚ï¼š--date 20250919')
    parser.add_argument('--max-stocks', type=int, default=None,
                        help='æœ€å¤§è¯„åˆ†è‚¡ç¥¨æ•°é‡ï¼ˆç”¨äºæµ‹è¯•ï¼‰ï¼Œé»˜è®¤Noneè¡¨ç¤ºè¯„åˆ†æ‰€æœ‰è‚¡ç¥¨')
    parser.add_argument('--version', type=str, default=None,
                        help='[å·²åºŸå¼ƒ] æ­¤å‚æ•°å·²ä¸å†ä½¿ç”¨ï¼Œå°†è‡ªåŠ¨ä½¿ç”¨æœ€æ–°çš„xgboost_timeseriesæ¨¡å‹')
    
    args = parser.parse_args()
    
    # å¦‚æœæŒ‡å®šäº†versionå‚æ•°ï¼Œç»™å‡ºè­¦å‘Š
    if args.version:
        log.warning(f"âš ï¸  --version å‚æ•°å·²åºŸå¼ƒï¼Œå°†è‡ªåŠ¨ä½¿ç”¨æœ€æ–°çš„ xgboost_timeseries æ¨¡å‹")
    
    # è§£æç›®æ ‡æ—¥æœŸ
    target_date = None
    if args.date:
        try:
            target_date = datetime.strptime(args.date, '%Y%m%d')
            log.info("="*80)
            log.info("ğŸ“… å†å²å›æµ‹æ¨¡å¼")
            log.info("="*80)
            log.info(f"ç›®æ ‡æ—¥æœŸ: {target_date.strftime('%Yå¹´%mæœˆ%dæ—¥')}")
            log.info("")
        except ValueError:
            log.error(f"âŒ æ—¥æœŸæ ¼å¼é”™è¯¯: {args.date}ï¼Œè¯·ä½¿ç”¨ YYYYMMDD æ ¼å¼ï¼Œä¾‹å¦‚ï¼š20250919")
            return
    
    log.info("="*80)
    log.info("å½“å‰å¸‚åœºè‚¡ç¥¨è¯„åˆ†ç³»ç»Ÿ")
    log.info("="*80)
    log.info("")
    log.info("ğŸ“Š ä½¿ç”¨æœ€æ–°è®­ç»ƒçš„ xgboost_timeseries æ¨¡å‹å¯¹æ‰€æœ‰Aè‚¡è¿›è¡Œè¯„åˆ†")
    log.info("ğŸ¯ è¾“å‡ºTop 50æ¨èè‚¡ç¥¨åŠè¯¦ç»†æŠ•èµ„æŠ¥å‘Š")
    if target_date:
        log.info(f"ğŸ“… æ¨¡æ‹Ÿæ—¥æœŸ: {target_date.strftime('%Yå¹´%mæœˆ%dæ—¥')} æ”¶ç›˜åçš„è¯„åˆ†ç»“æœ")
    log.info("")
    
    TOP_N = 50  # æ¨èTop 50
    MAX_STOCKS = args.max_stocks  # ä»å‘½ä»¤è¡Œå‚æ•°è·å–
    
    try:
        # 1. åŠ è½½æ¨¡å‹
        log.info("="*80)
        log.info("ç¬¬ä¸€æ­¥ï¼šåŠ è½½æ¨¡å‹")
        log.info("="*80)
        model = load_model()
        log.success(f"âœ“ æ¨¡å‹åŠ è½½æˆåŠŸ")
        log.info("")
        
        # 2. åˆå§‹åŒ–æ•°æ®ç®¡ç†å™¨
        log.info("="*80)
        log.info("ç¬¬äºŒæ­¥ï¼šåˆå§‹åŒ–æ•°æ®ç®¡ç†å™¨")
        log.info("="*80)
        dm = DataManager()
        log.success("âœ“ æ•°æ®ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
        log.info("")
        
        # 3. è·å–æ‰€æœ‰ç¬¦åˆæ¡ä»¶çš„è‚¡ç¥¨
        valid_stocks = get_all_stocks(dm, target_date=target_date.strftime('%Y%m%d') if target_date else None)
        
        # 4. å¯¹æ‰€æœ‰è‚¡ç¥¨è¿›è¡Œè¯„åˆ†
        df_scores = score_all_stocks(dm, model, valid_stocks, max_stocks=MAX_STOCKS, 
                                     target_date=target_date.strftime('%Y%m%d') if target_date else None)
        
        # æ£€æŸ¥è¯„åˆ†ç»“æœæ˜¯å¦ä¸ºç©º
        if df_scores is None or len(df_scores) == 0:
            log.error("âœ— è¯„åˆ†ç»“æœä¸ºç©ºï¼Œæ²¡æœ‰æˆåŠŸè¯„åˆ†çš„è‚¡ç¥¨")
            log.error("   å¯èƒ½åŸå› ï¼š")
            log.error("   1. æ•°æ®è·å–å¤±è´¥")
            log.error("   2. ç‰¹å¾è®¡ç®—å¤±è´¥")
            log.error("   3. æ¨¡å‹é¢„æµ‹å¤±è´¥")
            log.error("   è¯·æ£€æŸ¥æ—¥å¿—äº†è§£è¯¦ç»†é”™è¯¯ä¿¡æ¯")
            return
        
        log.info(f"\nâœ“ æˆåŠŸè¯„åˆ† {len(df_scores)} åªè‚¡ç¥¨")
        
        # 5. ç›´æ¥ä½¿ç”¨è¯„åˆ†ç»“æœï¼ˆå·²ç§»é™¤è´¢åŠ¡ç­›é€‰ï¼‰
        log.info("\n" + "="*80)
        log.info("ç¬¬äº”æ­¥ï¼šç”Ÿæˆæ¨èç»“æœ")
        log.info("="*80)
        log.info("âœ“ å·²ç§»é™¤è´¢åŠ¡ç­›é€‰ï¼Œç›´æ¥ä½¿ç”¨æ¨¡å‹è¯„åˆ†ç»“æœ")
        log.info("")
        
        df_filtered = df_scores  # ç›´æ¥ä½¿ç”¨è¯„åˆ†ç»“æœï¼Œä¸è¿›è¡Œè´¢åŠ¡ç­›é€‰
        
        # 6. åˆ†æå’Œè¾“å‡ºç»“æœ
        df_top = analyze_and_output_results(df_filtered, top_n=min(TOP_N, len(df_filtered)))
        
        # 7. ä¿å­˜ç»“æœï¼ˆåŒ…å«å…ƒæ•°æ®ï¼Œç”¨äºåç»­å‡†ç¡®ç‡åˆ†æï¼‰
        # ä»æ¨¡å‹å¯¹è±¡è·å–æ¨¡å‹ä¿¡æ¯
        scores_file, top_file, report_file, metadata_file = save_results(
            df_filtered, df_top, top_n=min(TOP_N, len(df_filtered)), 
            model_path=model.model_path,
            model_name=model.model_name,
            model_version=model.model_version,
            target_date=target_date.strftime('%Y%m%d') if target_date else None
        )
        
        log.info("\n" + "="*80)
        log.success("âœ… è‚¡ç¥¨è¯„åˆ†å®Œæˆï¼")
        log.info("="*80)
        log.info("\nğŸ’¡ ä½¿ç”¨å»ºè®®:")
        log.info("  1. Top 50 æ˜¯æ¨¡å‹é¢„æµ‹æœ€æœ‰å¯èƒ½æˆä¸ºç‰›è‚¡çš„å€™é€‰")
        log.info("  2. å»ºè®®ç»“åˆåŸºæœ¬é¢åˆ†æè¿›ä¸€æ­¥ç­›é€‰")
        log.info("  3. æ³¨æ„æ§åˆ¶ä»“ä½å’Œé£é™©")
        log.info("  4. å®šæœŸé‡æ–°è¯„åˆ†ï¼ˆå»ºè®®æ¯å‘¨ä¸€æ¬¡ï¼‰")
        log.info("")
        
    except Exception as e:
        log.error(f"âœ— è¯„åˆ†è¿‡ç¨‹å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()


if __name__ == '__main__':
    main()

