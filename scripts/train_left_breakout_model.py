#!/usr/bin/env python3
"""
å·¦ä¾§æ½œåŠ›ç‰›è‚¡æ¨¡å‹ - æ¨¡å‹è®­ç»ƒè„šæœ¬

è®­ç»ƒå·¦ä¾§æ½œåŠ›ç‰›è‚¡æ¨¡å‹

æ¨èä½¿ç”¨æµç¨‹:
1. python scripts/prepare_left_breakout_data.py    # æå‰å‡†å¤‡æ•°æ®
2. python scripts/train_left_breakout_model.py --load-prepared-data  # åŠ è½½å·²å‡†å¤‡æ•°æ®è®­ç»ƒ

æˆ–è€…ç›´æ¥è®­ç»ƒï¼ˆå®æ—¶å‡†å¤‡æ•°æ®ï¼‰:
python scripts/train_left_breakout_model.py

å¯é€‰å‚æ•°:
--load-prepared-data  åŠ è½½å·²å‡†å¤‡å¥½çš„æ•°æ®ï¼ˆæ¨èï¼‰
--force-refresh       å¼ºåˆ¶é‡æ–°å‡†å¤‡æ ·æœ¬
--skip-validation     è·³è¿‡æ¨¡å‹éªŒè¯
--config-file         æŒ‡å®šé…ç½®æ–‡ä»¶è·¯å¾„
"""

import sys
import os
import argparse
import pandas as pd
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.data.data_manager import DataManager
from src.models.stock_selection.left_breakout import LeftBreakoutModel
from src.models.stock_selection.left_breakout.left_validation import LeftBreakoutValidator
from config.settings import settings
from src.utils.logger import log


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='è®­ç»ƒå·¦ä¾§æ½œåŠ›ç‰›è‚¡æ¨¡å‹')
    parser.add_argument('--force-refresh', action='store_true',
                       help='å¼ºåˆ¶é‡æ–°å‡†å¤‡æ ·æœ¬')
    parser.add_argument('--skip-validation', action='store_true',
                       help='è·³è¿‡æ¨¡å‹éªŒè¯')
    parser.add_argument('--config-file', type=str, default='config/settings.yaml',
                       help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--load-prepared-data', action='store_true',
                       help='åŠ è½½å·²å‡†å¤‡å¥½çš„æ•°æ®ï¼ˆè¿è¡Œ prepare_left_breakout_data.py åçš„æ•°æ®ï¼‰')

    args = parser.parse_args()

    try:
        log.info("="*60)
        log.info("ğŸš€ å·¦ä¾§æ½œåŠ›ç‰›è‚¡æ¨¡å‹ - æ¨¡å‹è®­ç»ƒ")
        log.info("="*60)

        # 1. åŠ è½½é…ç½®
        log.info("ğŸ“‹ åŠ è½½é…ç½®...")
        if args.config_file != 'config/settings.yaml':
            # å¦‚æœæŒ‡å®šäº†ä¸åŒçš„é…ç½®æ–‡ä»¶ï¼Œé‡æ–°åŠ è½½
            from config.settings import Settings
            settings_obj = Settings(args.config_file)
            config = settings_obj._config
        else:
            config = settings._config

        # æ£€æŸ¥å·¦ä¾§æ¨¡å‹æ˜¯å¦å¯ç”¨
        if not config.get('left_breakout', {}).get('model', {}).get('enabled', True):
            log.warning("âš ï¸  å·¦ä¾§æ¨¡å‹æœªå¯ç”¨ï¼Œè¯·åœ¨é…ç½®æ–‡ä»¶ä¸­è®¾ç½® left_breakout.model.enabled = true")
            return

        # 2. åˆå§‹åŒ–æ•°æ®ç®¡ç†å™¨
        log.info("ğŸ”§ åˆå§‹åŒ–æ•°æ®ç®¡ç†å™¨...")
        dm = DataManager(config.get('data', {}).get('source', 'tushare'))

        # 3. åˆå§‹åŒ–å·¦ä¾§æ¨¡å‹
        log.info("ğŸ¤– åˆå§‹åŒ–å·¦ä¾§æ½œåŠ›ç‰›è‚¡æ¨¡å‹...")
        left_model = LeftBreakoutModel(dm, config.get('left_breakout', {}))

        # 4. å‡†å¤‡æ•°æ®
        if args.load_prepared_data:
            # åŠ è½½å·²å‡†å¤‡å¥½çš„æ•°æ®
            log.info("ğŸ“‚ åŠ è½½å·²å‡†å¤‡å¥½çš„è®­ç»ƒæ•°æ®...")
            data_dir = 'data/training/features'
            feature_file = f'{data_dir}/left_breakout_features_latest.csv'

            if not os.path.exists(feature_file):
                log.error(f"âŒ æœªæ‰¾åˆ°å·²å‡†å¤‡çš„æ•°æ®æ–‡ä»¶: {feature_file}")
                log.error("è¯·å…ˆè¿è¡Œ: python scripts/prepare_left_breakout_data.py")
                return

            features_df = pd.read_csv(feature_file)
            log.info(f"âœ… åŠ è½½ç‰¹å¾æ•°æ®: {len(features_df)} æ ·æœ¬ Ã— {features_df.shape[1]} ç‰¹å¾")

            # ä»æ•°æ®ä¸­æå–æ ·æœ¬ç»Ÿè®¡ä¿¡æ¯
            label_counts = features_df['label'].value_counts()
            positive_count = label_counts.get(1, 0)
            negative_count = label_counts.get(0, 0)
            log.info(f"âœ… æ­£æ ·æœ¬: {positive_count} ä¸ª")
            log.info(f"âœ… è´Ÿæ ·æœ¬: {negative_count} ä¸ª")

        else:
            # å®æ—¶å‡†å¤‡æ•°æ®ï¼ˆåŸæœ‰é€»è¾‘ï¼‰
            log.info("ğŸ“Š å‡†å¤‡æ ·æœ¬æ•°æ®...")
            positive_samples, negative_samples = left_model.prepare_samples(
                force_refresh=args.force_refresh
            )

            if positive_samples.empty:
                log.error("âŒ æ­£æ ·æœ¬ä¸ºç©ºï¼Œæ— æ³•è®­ç»ƒæ¨¡å‹")
                return

            if negative_samples.empty:
                log.error("âŒ è´Ÿæ ·æœ¬ä¸ºç©ºï¼Œæ— æ³•è®­ç»ƒæ¨¡å‹")
                return

            log.info(f"âœ… æ­£æ ·æœ¬: {len(positive_samples)} ä¸ª")
            log.info(f"âœ… è´Ÿæ ·æœ¬: {len(negative_samples)} ä¸ª")

            # 5. ç‰¹å¾æå–
            log.info("ğŸ” æå–ç‰¹å¾...")
            features_df = left_model.extract_features(positive_samples, negative_samples)

            if features_df.empty:
                log.error("âŒ ç‰¹å¾æå–å¤±è´¥")
                return

            log.info(f"âœ… ç‰¹å¾ç»´åº¦: {features_df.shape[0]} æ ·æœ¬ Ã— {features_df.shape[1]} ç‰¹å¾")

        # 6. è®­ç»ƒæ¨¡å‹
        log.info("ğŸ¯ è®­ç»ƒæ¨¡å‹...")
        training_results = left_model.train_model(features_df)

        if not training_results:
            log.error("âŒ æ¨¡å‹è®­ç»ƒå¤±è´¥")
            return

        # 7. è¾“å‡ºè®­ç»ƒç»“æœ
        log.info("\n" + "="*60)
        log.info("ğŸ“ˆ æ¨¡å‹è®­ç»ƒç»“æœ")
        log.info("="*60)

        log.info(f"ğŸ¯ æ¨¡å‹ç‰ˆæœ¬: {training_results.get('model_path', 'N/A').split('/')[-1]}")
        log.info(f"ğŸ“Š è®­ç»ƒæ ·æœ¬: {training_results.get('train_samples', 0)}")
        log.info(f"ğŸ“Š æµ‹è¯•æ ·æœ¬: {training_results.get('test_samples', 0)}")

        train_metrics = training_results.get('train_metrics', {})
        test_metrics = training_results.get('test_metrics', {})

        log.info("è®­ç»ƒé›†æ€§èƒ½:")
        log.info(f"å‡†ç¡®ç‡: {train_metrics.get('accuracy', 0):.4f}")
        log.info(f"ç²¾ç¡®ç‡: {train_metrics.get('precision', 0):.4f}")
        log.info(f"å¬å›ç‡: {train_metrics.get('recall', 0):.4f}")
        log.info(f"F1åˆ†æ•°: {train_metrics.get('f1', 0):.4f}")
        log.info(f"AUC: {train_metrics.get('auc', 0):.4f}")
        log.info("\næµ‹è¯•é›†æ€§èƒ½:")
        log.info(f"å‡†ç¡®ç‡: {test_metrics.get('accuracy', 0):.4f}")
        log.info(f"ç²¾ç¡®ç‡: {test_metrics.get('precision', 0):.4f}")
        log.info(f"å¬å›ç‡: {test_metrics.get('recall', 0):.4f}")
        log.info(f"F1åˆ†æ•°: {test_metrics.get('f1', 0):.4f}")
        log.info(f"AUC: {test_metrics.get('auc', 0):.4f}")
        # 8. æ¨¡å‹éªŒè¯ï¼ˆå¯é€‰ï¼‰
        if not args.skip_validation:
            log.info("\nğŸ”¬ å¼€å§‹æ¨¡å‹éªŒè¯...")

            # åˆå§‹åŒ–éªŒè¯å™¨
            validator = LeftBreakoutValidator(left_model)

            # Walk-ForwardéªŒè¯
            log.info("ğŸ“Š æ‰§è¡ŒWalk-ForwardéªŒè¯...")
            wf_results = validator.walk_forward_validation(
                features_df,
                n_splits=config.get('left_breakout', {}).get('validation', {}).get('walk_forward', {}).get('n_splits', 5)
            )

            if wf_results:
                summary = wf_results.get('summary', {})
                log.info(f"AUCå‡å€¼: {summary.get('auc_mean', 0):.4f}")
                log.info(f"ğŸ“ˆ éªŒè¯è¯„çº§: {summary.get('overall_rating', 'N/A')}")

                # é²æ£’æ€§æµ‹è¯•
                log.info("ğŸ›¡ï¸  æ‰§è¡Œé²æ£’æ€§æµ‹è¯•...")
                robustness_results = validator.robustness_test(
                    features_df,
                    n_bootstraps=config.get('left_breakout', {}).get('validation', {}).get('robustness_test', {}).get('n_bootstraps', 50)
                )

                if robustness_results:
                    rb_stats = robustness_results.get('statistics', {})
                    log.info(f"æ ‡å‡†å·®: {rb_stats.get('std', 0):.4f}")
        # 9. ä¿å­˜è®­ç»ƒæŠ¥å‘Š
        log.info("ğŸ’¾ ä¿å­˜è®­ç»ƒæŠ¥å‘Š...")
        report_saved = save_training_summary_report(
            training_results,
            wf_results if not args.skip_validation and 'wf_results' in locals() else None,
            robustness_results if not args.skip_validation and 'robustness_results' in locals() else None
        )

        if report_saved:
            log.info("âœ… è®­ç»ƒæŠ¥å‘Šå·²ä¿å­˜")

        # 10. è¾“å‡ºä½¿ç”¨å»ºè®®
        log.info("\n" + "="*60)
        log.info("ğŸ‰ å·¦ä¾§æ½œåŠ›ç‰›è‚¡æ¨¡å‹è®­ç»ƒå®Œæˆï¼")
        log.info("="*60)
        log.info("ğŸ’¡ ä½¿ç”¨å»ºè®®:")
        log.info("   1. æŸ¥çœ‹è®­ç»ƒæŠ¥å‘Š: data/models/left_breakout/training_report_*.txt")
        log.info("   2. è¿è¡Œé¢„æµ‹è„šæœ¬: python scripts/predict_left_breakout.py")
        log.info("   3. å®šæœŸé‡æ–°è®­ç»ƒä»¥ä¿æŒæ¨¡å‹æ—¶æ•ˆæ€§")
        log.info("="*60)

    except Exception as e:
        log.error(f"âŒ æ¨¡å‹è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        log.error(traceback.format_exc())
        sys.exit(1)


def save_training_summary_report(training_results, wf_results=None, robustness_results=None):
    """
    ä¿å­˜è®­ç»ƒæ€»ç»“æŠ¥å‘Š

    Args:
        training_results: è®­ç»ƒç»“æœ
        wf_results: Walk-ForwardéªŒè¯ç»“æœ
        robustness_results: é²æ£’æ€§æµ‹è¯•ç»“æœ

    Returns:
        æ˜¯å¦ä¿å­˜æˆåŠŸ
    """
    try:
        report_dir = "data/models/left_breakout"
        os.makedirs(report_dir, exist_ok=True)

        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        report_file = os.path.join(report_dir, f"training_summary_{timestamp}.txt")

        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("="*80 + "\n")
            f.write("å·¦ä¾§æ½œåŠ›ç‰›è‚¡æ¨¡å‹ - è®­ç»ƒæ€»ç»“æŠ¥å‘Š\n")
            f.write("="*80 + "\n\n")

            f.write(f"ğŸ“… è®­ç»ƒæ—¶é—´: {training_results.get('training_time', 'N/A')}\n")
            f.write(f"ğŸ¯ æ¨¡å‹ç‰ˆæœ¬: {training_results.get('model_path', 'N/A').split('/')[-1]}\n\n")

            # åŸºæœ¬ä¿¡æ¯
            f.write("ğŸ“Š åŸºæœ¬ä¿¡æ¯\n")
            f.write("-"*40 + "\n")
            f.write(f"è®­ç»ƒæ ·æœ¬: {training_results.get('train_samples', 0)}\n")
            f.write(f"æµ‹è¯•æ ·æœ¬: {training_results.get('test_samples', 0)}\n")
            f.write(f"ç‰¹å¾æ•°é‡: {len(training_results.get('feature_columns', []))}\n\n")

            # æ€§èƒ½æŒ‡æ ‡
            f.write("ğŸ¯ æ€§èƒ½æŒ‡æ ‡\n")
            f.write("-"*40 + "\n")

            train_metrics = training_results.get('train_metrics', {})
            test_metrics = training_results.get('test_metrics', {})

            f.write("<12")
            f.write("-"*40 + "\n")
            f.write("<12")
            f.write("<12")
            f.write("<12")
            f.write("<12")
            f.write("<12")
            f.write("\n")

            # Walk-ForwardéªŒè¯ç»“æœ
            if wf_results:
                f.write("\nğŸ“ˆ Walk-ForwardéªŒè¯\n")
                f.write("-"*40 + "\n")

                summary = wf_results.get('summary', {})
                metrics = ['accuracy', 'precision', 'recall', 'f1_score', 'auc_roc']

                f.write("<12")
                f.write("-"*40 + "\n")

                for metric in metrics:
                    mean_val = summary.get(f'{metric}_mean', 0)
                    std_val = summary.get(f'{metric}_std', 0)
                    stability = summary.get(f'{metric}_stability', 0)

                    f.write("<12")
                f.write(f"\næ•´ä½“è¯„çº§: {summary.get('overall_rating', 'N/A')}\n")

            # é²æ£’æ€§æµ‹è¯•ç»“æœ
            if robustness_results:
                f.write("\nğŸ›¡ï¸  é²æ£’æ€§æµ‹è¯•\n")
                f.write("-"*40 + "\n")

                rb_stats = robustness_results.get('statistics', {})
                f.write(f"å‡†ç¡®ç‡: {rb_stats.get('accuracy_mean', 0):.4f}\n")
                f.write(f"ç²¾ç¡®ç‡: {rb_stats.get('precision_mean', 0):.4f}\n")
                f.write(f"å¬å›ç‡: {rb_stats.get('recall_mean', 0):.4f}\n")
                f.write(f"F1åˆ†æ•°: {rb_stats.get('f1_mean', 0):.4f}\n")
                f.write(f"AUC: {rb_stats.get('auc_mean', 0):.4f}\n")
                f.write(f"æ ‡å‡†å·®: {rb_stats.get('std', 0):.4f}\n")
            # ç‰¹å¾é‡è¦æ€§ï¼ˆå¦‚æœæœ‰ï¼‰
            feature_importance = training_results.get('feature_importance', [])
            if feature_importance:
                f.write("\nğŸ” é‡è¦ç‰¹å¾\n")
                f.write("-"*40 + "\n")

                # æ˜¾ç¤ºå‰10ä¸ªé‡è¦ç‰¹å¾
                for i, (_, row) in enumerate(feature_importance.head(10).iterrows(), 1):
                    f.write(f"{i:2d}. {row['feature']}: {row['importance']:.4f}\n")
            # ä½¿ç”¨å»ºè®®
            f.write("\nğŸ’¡ ä½¿ç”¨å»ºè®®\n")
            f.write("-"*40 + "\n")

            test_auc = test_metrics.get('auc_roc', 0)
            if test_auc > 0.8:
                f.write("ğŸ‰ æ¨¡å‹æ€§èƒ½ä¼˜ç§€ï¼Œå»ºè®®ç«‹å³ç”¨äºé¢„æµ‹\n")
            elif test_auc > 0.7:
                f.write("âœ… æ¨¡å‹æ€§èƒ½è‰¯å¥½ï¼Œå¯ä»¥ç”¨äºé¢„æµ‹\n")
            elif test_auc > 0.6:
                f.write("âš ï¸  æ¨¡å‹æ€§èƒ½ä¸€èˆ¬ï¼Œå»ºè®®è¿›ä¸€æ­¥ä¼˜åŒ–\n")
            else:
                f.write("âŒ æ¨¡å‹æ€§èƒ½ä¸ä½³ï¼Œéœ€è¦é‡æ–°è®­ç»ƒæˆ–è°ƒæ•´å‚æ•°\n")

            f.write("\nğŸ“ æ³¨æ„äº‹é¡¹:\n")
            f.write("â€¢ å·¦ä¾§äº¤æ˜“å…·æœ‰è¾ƒé«˜é£é™©ï¼Œè¯·è°¨æ…ä½¿ç”¨\n")
            f.write("â€¢ å»ºè®®ä»å°ä»“ä½å¼€å§‹è¯•æ°´\n")
            f.write("â€¢ å®šæœŸç›‘æ§æ¨¡å‹è¡¨ç°å¹¶é‡æ–°è®­ç»ƒ\n")
            f.write("â€¢ ç»“åˆæŠ€æœ¯åˆ†æå’ŒåŸºæœ¬é¢åˆ†æ\n")

        log.info(f"è®­ç»ƒæ€»ç»“æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
        return True

    except Exception as e:
        log.error(f"ä¿å­˜è®­ç»ƒæ€»ç»“æŠ¥å‘Šå¤±è´¥: {e}")
        return False


if __name__ == "__main__":
    main()
