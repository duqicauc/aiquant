"""
è´Ÿæ ·æœ¬æ•°æ®å‡†å¤‡è„šæœ¬ V2 - åŒå‘¨æœŸå…¶ä»–è‚¡ç¥¨æ³•

æ›´ç®€å•ç›´æ¥çš„è´Ÿæ ·æœ¬ç­›é€‰æ–¹æ³•ï¼š
- å¯¹æ¯ä¸ªæ­£æ ·æœ¬ï¼Œåœ¨åŒä¸€T1æ—¥æœŸé€‰æ‹©å…¶ä»–è‚¡ç¥¨ä½œä¸ºè´Ÿæ ·æœ¬
- æ›´å¿«é€Ÿï¼Œæ•°æ®é‡æ›´å……è¶³
- æ›´æ¥è¿‘å®é™…åº”ç”¨åœºæ™¯
"""
import sys
import os
import warnings
import pandas as pd
import json
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

# å¿½ç•¥FutureWarning
warnings.filterwarnings('ignore', category=FutureWarning)

from src.data.data_manager import DataManager
from src.models.screening.negative_sample_screener_v2 import NegativeSampleScreenerV2
from src.utils.logger import log


def main():
    """ä¸»å‡½æ•°"""
    log.info("="*80)
    log.info("è´Ÿæ ·æœ¬æ•°æ®å‡†å¤‡ V2 - åŒå‘¨æœŸå…¶ä»–è‚¡ç¥¨æ³•")
    log.info("="*80)
    
    # é…ç½®å‚æ•°
    SAMPLES_PER_POSITIVE = 2  # æ¯ä¸ªæ­£æ ·æœ¬å¯¹åº”çš„è´Ÿæ ·æœ¬æ•°é‡ï¼ˆå¢åŠ ä»¥å¹³è¡¡æ ·æœ¬ï¼‰
    RANDOM_SEED = 42
    
    POSITIVE_SAMPLES_FILE = 'data/training/samples/positive_samples.csv'
    
    OUTPUT_NEGATIVE_SAMPLES = 'data/training/samples/negative_samples_v2.csv'
    OUTPUT_NEGATIVE_FEATURES = 'data/training/features/negative_feature_data_v2_34d.csv'
    OUTPUT_STATS = 'data/training/samples/negative_sample_statistics_v2.json'
    
    log.info(f"\nå½“å‰è®¾ç½®ï¼š")
    log.info(f"  æ–¹æ³•: åŒå‘¨æœŸå…¶ä»–è‚¡ç¥¨æ³•")
    log.info(f"  æ­£æ ·æœ¬æ–‡ä»¶: {POSITIVE_SAMPLES_FILE}")
    log.info(f"  æ¯æ­£æ ·æœ¬å¯¹åº”è´Ÿæ ·æœ¬æ•°: {SAMPLES_PER_POSITIVE}")
    log.info(f"  éšæœºç§å­: {RANDOM_SEED}")
    log.info("")
    
    # 1. åŠ è½½æ­£æ ·æœ¬æ•°æ®
    log.info("="*80)
    log.info("ç¬¬ä¸€æ­¥ï¼šåŠ è½½æ­£æ ·æœ¬æ•°æ®")
    log.info("="*80)
    
    try:
        df_positive_samples = pd.read_csv(POSITIVE_SAMPLES_FILE)
        log.success(f"âœ“ æ­£æ ·æœ¬åŠ è½½æˆåŠŸ: {len(df_positive_samples)} ä¸ª")
    except Exception as e:
        log.error(f"âœ— åŠ è½½æ­£æ ·æœ¬æ•°æ®å¤±è´¥: {e}")
        log.error("è¯·å…ˆè¿è¡Œ prepare_positive_samples.py ç”Ÿæˆæ­£æ ·æœ¬æ•°æ®")
        return
    
    # 2. åˆå§‹åŒ–æ•°æ®ç®¡ç†å™¨å’Œè´Ÿæ ·æœ¬ç­›é€‰å™¨
    log.info("\n" + "="*80)
    log.info("ç¬¬äºŒæ­¥ï¼šåˆå§‹åŒ–ç­›é€‰å™¨ V2")
    log.info("="*80)
    
    dm = DataManager()
    screener = NegativeSampleScreenerV2(dm)
    
    log.success("âœ“ ç­›é€‰å™¨ V2 åˆå§‹åŒ–å®Œæˆ")
    
    # 3. ç­›é€‰è´Ÿæ ·æœ¬
    log.info("\n" + "="*80)
    log.info("ç¬¬ä¸‰æ­¥ï¼šç­›é€‰è´Ÿæ ·æœ¬ï¼ˆåŒå‘¨æœŸå…¶ä»–è‚¡ç¥¨æ³•ï¼‰")
    log.info("="*80)
    
    df_negative_samples = screener.screen_negative_samples(
        positive_samples_df=df_positive_samples,
        samples_per_positive=SAMPLES_PER_POSITIVE,
        random_seed=RANDOM_SEED
    )
    
    if df_negative_samples.empty:
        log.error("âœ— æœªæ‰¾åˆ°è´Ÿæ ·æœ¬")
        return
    
    # 4. æå–è´Ÿæ ·æœ¬ç‰¹å¾
    log.info("\n" + "="*80)
    log.info("ç¬¬å››æ­¥ï¼šæå–è´Ÿæ ·æœ¬ç‰¹å¾æ•°æ®")
    log.info("="*80)
    
    df_negative_features = screener.extract_features(df_negative_samples)
    
    if df_negative_features.empty:
        log.error("âœ— ç‰¹å¾æå–å¤±è´¥")
        return
    
    # 4.1 æ•°æ®è´¨é‡å¤„ç†
    log.info("\n[æ­¥éª¤4.1] æ•°æ®è´¨é‡å¤„ç†...")
    
    # ç»Ÿè®¡åŸå§‹ç¼ºå¤±å€¼
    missing_before = df_negative_features.isnull().sum()
    total_missing_before = missing_before.sum()
    log.info(f"åŸå§‹ç¼ºå¤±å€¼æ€»æ•°: {total_missing_before}")
    if total_missing_before > 0:
        for col, count in missing_before.items():
            if count > 0:
                log.info(f"  - {col}: {count} ({count/len(df_negative_features)*100:.2f}%)")
    
    # å®šä¹‰éœ€è¦å¡«å……çš„æ•°å€¼åˆ—
    numeric_cols = ['close', 'pct_chg', 'total_mv', 'circ_mv', 'ma5', 'ma10', 
                    'volume_ratio', 'macd_dif', 'macd_dea', 'macd', 
                    'rsi_6', 'rsi_12', 'rsi_24']
    numeric_cols = [col for col in numeric_cols if col in df_negative_features.columns]
    
    # æŒ‰æ ·æœ¬åˆ†ç»„è¿›è¡Œå‰å‘å¡«å……+åå‘å¡«å……
    log.info("æ‰§è¡Œç¼ºå¤±å€¼å¡«å……ï¼ˆæŒ‰æ ·æœ¬åˆ†ç»„ï¼šå‰å‘å¡«å…… + åå‘å¡«å……ï¼‰...")
    df_negative_features[numeric_cols] = df_negative_features.groupby('sample_id')[numeric_cols].transform(
        lambda x: x.ffill().bfill()
    )
    
    # æ£€æŸ¥å¡«å……åçš„ç¼ºå¤±å€¼
    missing_after = df_negative_features.isnull().sum()
    total_missing_after = missing_after.sum()
    log.info(f"å¡«å……åç¼ºå¤±å€¼æ€»æ•°: {total_missing_after}")
    
    # 4.2 è¿‡æ»¤æ•°æ®ä¸è¶³çš„æ ·æœ¬
    log.info("\n[æ­¥éª¤4.2] è¿‡æ»¤æ•°æ®ä¸è¶³çš„æ ·æœ¬...")
    min_days = 30  # æœ€å°‘éœ€è¦30å¤©æ•°æ®
    
    days_per_sample = df_negative_features.groupby('sample_id').size()
    valid_samples = days_per_sample[days_per_sample >= min_days].index
    invalid_samples = days_per_sample[days_per_sample < min_days]
    
    if len(invalid_samples) > 0:
        log.warning(f"å‘ç° {len(invalid_samples)} ä¸ªæ ·æœ¬æ•°æ®ä¸è¶³{min_days}å¤©ï¼Œå°†è¢«è¿‡æ»¤")
        df_negative_features = df_negative_features[df_negative_features['sample_id'].isin(valid_samples)]
        # åŒæ­¥è¿‡æ»¤è´Ÿæ ·æœ¬åˆ—è¡¨
        valid_sample_ids = df_negative_features['sample_id'].unique()
        df_negative_samples = df_negative_samples[df_negative_samples.index.isin(valid_sample_ids)]
        log.info(f"è¿‡æ»¤åå‰©ä½™æ ·æœ¬æ•°: {df_negative_features['sample_id'].nunique()}")
    else:
        log.success(f"âœ“ æ‰€æœ‰æ ·æœ¬æ•°æ®å®Œæ•´ï¼ˆå‡â‰¥{min_days}å¤©ï¼‰")
    
    # 4.3 æœ€ç»ˆæ•°æ®è´¨é‡æ£€æŸ¥
    log.info("\n[æ­¥éª¤4.3] æœ€ç»ˆæ•°æ®è´¨é‡æ£€æŸ¥...")
    final_missing = df_negative_features.isnull().sum().sum()
    if final_missing > 0:
        log.warning(f"ä»æœ‰ {final_missing} ä¸ªç¼ºå¤±å€¼ï¼Œå°†ä½¿ç”¨åˆ—å‡å€¼å¡«å……...")
        df_negative_features[numeric_cols] = df_negative_features[numeric_cols].fillna(
            df_negative_features[numeric_cols].mean()
        )
    log.success(f"âœ“ æ•°æ®è´¨é‡å¤„ç†å®Œæˆï¼Œæœ€ç»ˆç¼ºå¤±å€¼: {df_negative_features.isnull().sum().sum()}")
    
    # 5. ä¿å­˜ç»“æœ
    log.info("\n" + "="*80)
    log.info("ç¬¬äº”æ­¥ï¼šä¿å­˜ç»“æœ")
    log.info("="*80)
    
    # ä¿å­˜è´Ÿæ ·æœ¬åˆ—è¡¨
    df_negative_samples.to_csv(OUTPUT_NEGATIVE_SAMPLES, index=False)
    log.success(f"âœ“ è´Ÿæ ·æœ¬åˆ—è¡¨å·²ä¿å­˜: {OUTPUT_NEGATIVE_SAMPLES}")
    
    # ä¿å­˜è´Ÿæ ·æœ¬ç‰¹å¾æ•°æ®
    df_negative_features.to_csv(OUTPUT_NEGATIVE_FEATURES, index=False)
    log.success(f"âœ“ è´Ÿæ ·æœ¬ç‰¹å¾æ•°æ®å·²ä¿å­˜: {OUTPUT_NEGATIVE_FEATURES}")
    
    # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
    stats = {
        'generation_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'method': 'V2 - åŒå‘¨æœŸå…¶ä»–è‚¡ç¥¨æ³•',
        'total_negative_samples': len(df_negative_samples),
        'total_positive_samples': len(df_positive_samples),
        'samples_per_positive': SAMPLES_PER_POSITIVE,
        'negative_feature_records': len(df_negative_features),
        'feature_samples': int(df_negative_features['sample_id'].nunique()),
        'random_seed': RANDOM_SEED,
        'min_days_required': min_days,
        'data_quality': {
            'missing_values_before': int(total_missing_before),
            'missing_values_after': int(df_negative_features.isnull().sum().sum()),
            'filtered_samples': int(len(invalid_samples)) if len(invalid_samples) > 0 else 0,
            'avg_days_per_sample': float(df_negative_features.groupby('sample_id').size().mean())
        },
        'files': {
            'negative_samples': OUTPUT_NEGATIVE_SAMPLES,
            'negative_features': OUTPUT_NEGATIVE_FEATURES,
            'positive_samples': POSITIVE_SAMPLES_FILE
        }
    }
    
    with open(OUTPUT_STATS, 'w', encoding='utf-8') as f:
        json.dump(stats, f, indent=2, ensure_ascii=False)
    
    log.success(f"âœ“ ç»Ÿè®¡æŠ¥å‘Šå·²ä¿å­˜: {OUTPUT_STATS}")
    
    # 6. æ˜¾ç¤ºæ ·æœ¬é¢„è§ˆ
    log.info("\n" + "="*80)
    log.info("è´Ÿæ ·æœ¬æ•°æ®é¢„è§ˆï¼ˆå‰10æ¡ï¼‰")
    log.info("="*80)
    print(df_negative_samples.head(10))
    
    log.info("\n" + "="*80)
    log.info("è´Ÿæ ·æœ¬ç‰¹å¾æ•°æ®é¢„è§ˆï¼ˆå‰10æ¡ï¼‰")
    log.info("="*80)
    available_columns = [col for col in [
        'sample_id', 'trade_date', 'name', 'ts_code', 'close', 'pct_chg',
        'total_mv', 'circ_mv', 'ma5', 'ma10', 'volume_ratio',
        'macd_dif', 'macd_dea', 'macd', 'rsi_6', 'rsi_12', 'rsi_24', 
        'days_to_t1', 'label'
    ] if col in df_negative_features.columns]
    
    log.info("\nå¯ç”¨å­—æ®µ:")
    for col in available_columns:
        log.info(f"  - {col}")
    
    log.info("")
    print(df_negative_features[available_columns].head(10))
    
    # 7. æœ€ç»ˆæ€»ç»“
    log.info("\n" + "="*80)
    log.success("âœ… è´Ÿæ ·æœ¬æ•°æ®å‡†å¤‡å®Œæˆï¼ï¼ˆV2 - åŒå‘¨æœŸå…¶ä»–è‚¡ç¥¨æ³•ï¼‰")
    log.info("="*80)
    log.info("")
    log.info(f"  1. è´Ÿæ ·æœ¬åˆ—è¡¨: {OUTPUT_NEGATIVE_SAMPLES}")
    log.info(f"  2. è´Ÿæ ·æœ¬ç‰¹å¾: {OUTPUT_NEGATIVE_FEATURES}")
    log.info(f"  3. ç»Ÿè®¡æŠ¥å‘Š: {OUTPUT_STATS}")
    log.info("")
    log.info("ğŸ“Š æ•°æ®å¯¹æ¯”ï¼š")
    log.info(f"  æ­£æ ·æœ¬æ•°: {len(df_positive_samples)}")
    log.info(f"  è´Ÿæ ·æœ¬æ•°: {len(df_negative_samples)}")
    log.info(f"  è´Ÿæ ·æœ¬ç‰¹å¾: {len(df_negative_features)} æ¡")
    log.info("")
    log.info("ğŸ’¡ ä¼˜åŠ¿ï¼š")
    log.info("  - ç­›é€‰é€Ÿåº¦å¿«ï¼ˆä¸éœ€è¦ç‰¹å¾è®¡ç®—ï¼‰")
    log.info("  - æ•°æ®é‡å……è¶³")
    log.info("  - çœŸå®åæ˜ å¸‚åœºè‚¡ç¥¨åˆ†å¸ƒ")
    log.info("  - æ¥è¿‘å®é™…åº”ç”¨åœºæ™¯")
    log.info("")
    log.info("ğŸ”¬ å¯¹æ¯”å®éªŒï¼š")
    log.info("  æ–¹æ¡ˆ1ï¼ˆV1ï¼‰ï¼šåŸºäºç‰¹å¾ç»Ÿè®¡ç­›é€‰")
    log.info("    æ–‡ä»¶: negative_samples.csv")
    log.info("    ç‰¹ç‚¹: è´Ÿæ ·æœ¬ç‰¹å¾ä¸æ­£æ ·æœ¬ç›¸ä¼¼")
    log.info("")
    log.info("  æ–¹æ¡ˆ2ï¼ˆV2ï¼‰ï¼šåŒå‘¨æœŸå…¶ä»–è‚¡ç¥¨")
    log.info("    æ–‡ä»¶: negative_samples_v2.csv")
    log.info("    ç‰¹ç‚¹: éšæœºé€‰æ‹©ï¼Œæ›´çœŸå®")
    log.info("")
    log.info("ä¸‹ä¸€æ­¥ï¼š")
    log.info("  - åˆ†åˆ«è®­ç»ƒä¸¤ä¸ªæ¨¡å‹")
    log.info("  - å¯¹æ¯”æ¨¡å‹æ•ˆæœï¼ˆå‡†ç¡®ç‡ã€å¬å›ç‡ã€F1ï¼‰")
    log.info("  - é€‰æ‹©æœ€ä½³æ–¹æ¡ˆæˆ–ç»„åˆä½¿ç”¨")
    log.info("")


if __name__ == '__main__':
    main()

