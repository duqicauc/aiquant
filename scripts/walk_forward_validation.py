"""
Walk-ForwardéªŒè¯è„šæœ¬

åœ¨å¤šä¸ªæ—¶é—´çª—å£ä¸Šæµ‹è¯•æ¨¡å‹çš„ç¨³å®šæ€§å’Œæ³›åŒ–èƒ½åŠ›
- æ»‘åŠ¨çª—å£æ–¹å¼è®­ç»ƒå’Œæµ‹è¯•
- éªŒè¯æ¨¡å‹åœ¨ä¸åŒå¸‚åœºç¯å¢ƒä¸‹çš„è¡¨ç°
- é¿å…è¿‡æ‹Ÿåˆï¼Œç¡®ä¿æ¨¡å‹é²æ£’æ€§
"""
import sys
import os
import warnings
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import json

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

# å¿½ç•¥FutureWarning
warnings.filterwarnings('ignore', category=FutureWarning)

from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, 
    f1_score, roc_auc_score, confusion_matrix
)
import xgboost as xgb
from src.utils.logger import log


def safe_to_datetime(date_value):
    """
    å®‰å…¨åœ°å°†æ—¥æœŸå€¼è½¬æ¢ä¸ºdatetimeç±»å‹
    
    å¤„ç†ä»¥ä¸‹æƒ…å†µï¼š
    - æ•´æ•°ï¼šå¦‚ 20230101 -> è¢«é”™è¯¯è§£æä¸ºçº³ç§’æ—¶é—´æˆ³
    - å­—ç¬¦ä¸²ï¼šå¦‚ '20230101' -> æ­£å¸¸è§£æ
    - datetimeï¼šç›´æ¥è¿”å›
    """
    if pd.isna(date_value):
        return pd.NaT
    if isinstance(date_value, (int, np.integer, float, np.floating)):
        return pd.to_datetime(str(int(date_value)), format='%Y%m%d', errors='coerce')
    return pd.to_datetime(date_value, errors='coerce')


def load_and_prepare_data(neg_version='v2', use_market_factors=True, use_tech_factors=False, use_advanced_factors=False):
    """åŠ è½½æ•°æ®"""
    log.info("="*80)
    log.info("åŠ è½½æ•°æ®")
    log.info("="*80)
    
    # åŠ è½½æ­£æ ·æœ¬ï¼ˆä½¿ç”¨æ–°çš„ç›®å½•ç»“æ„ï¼‰
    if use_advanced_factors:
        pos_file = 'data/training/processed/feature_data_34d_advanced.csv'
        log.info("ğŸ“Š ä½¿ç”¨å¸¦é«˜çº§æŠ€æœ¯å› å­çš„ç‰¹å¾æ–‡ä»¶(advanced)")
    elif use_tech_factors:
        pos_file = 'data/training/processed/feature_data_34d_full.csv'
        log.info("ğŸ“Š ä½¿ç”¨å¸¦æ–°æŠ€æœ¯å› å­çš„ç‰¹å¾æ–‡ä»¶(full)")
    elif use_market_factors:
        pos_file = 'data/training/processed/feature_data_34d_with_market.csv'
        log.info("ğŸ“Š ä½¿ç”¨å¸¦å¸‚åœºå› å­çš„ç‰¹å¾æ–‡ä»¶")
    else:
        pos_file = 'data/training/processed/feature_data_34d.csv'
        log.info("ğŸ“Š ä½¿ç”¨åŸºç¡€ç‰¹å¾æ–‡ä»¶")
    
    df_pos = pd.read_csv(pos_file)
    df_pos['label'] = 1
    log.success(f"âœ“ æ­£æ ·æœ¬åŠ è½½å®Œæˆ: {len(df_pos)} æ¡")
    
    # åŠ è½½è´Ÿæ ·æœ¬ï¼ˆä½¿ç”¨æ–°çš„ç›®å½•ç»“æ„ï¼‰
    if neg_version == 'v2':
        if use_advanced_factors:
            neg_file = 'data/training/features/negative_feature_data_v2_34d_advanced.csv'
        elif use_tech_factors:
            neg_file = 'data/training/features/negative_feature_data_v2_34d_full.csv'
        elif use_market_factors:
            neg_file = 'data/training/features/negative_feature_data_v2_34d_with_market.csv'
        else:
            neg_file = 'data/training/features/negative_feature_data_v2_34d.csv'
    else:
        neg_file = 'data/training/features/negative_feature_data_34d.csv'
    
    df_neg = pd.read_csv(neg_file)
    log.success(f"âœ“ è´Ÿæ ·æœ¬åŠ è½½å®Œæˆ: {len(df_neg)} æ¡ (ç‰ˆæœ¬: {neg_version})")
    
    # åˆå¹¶
    df = pd.concat([df_pos, df_neg])
    log.info(f"âœ“ æ•°æ®åˆå¹¶å®Œæˆ: {len(df)} æ¡")
    log.info("")
    
    return df


def extract_features_with_time(df):
    """ä»34å¤©çš„æ—¶åºæ•°æ®ä¸­æå–ç»Ÿè®¡ç‰¹å¾ï¼ˆä¿ç•™æ—¶é—´ä¿¡æ¯ï¼‰"""
    log.info("="*80)
    log.info("ç‰¹å¾å·¥ç¨‹")
    log.info("="*80)
    log.info("å°†34å¤©æ—¶åºæ•°æ®è½¬æ¢ä¸ºç»Ÿè®¡ç‰¹å¾...")
    
    # é‡æ–°åˆ†é…å”¯ä¸€çš„sample_id
    df['unique_sample_id'] = df.groupby(['ts_code', 'label']).ngroup()
    
    features = []
    sample_ids = df['unique_sample_id'].unique()
    
    for i, sample_id in enumerate(sample_ids):
        if (i + 1) % 500 == 0:
            log.info(f"è¿›åº¦: {i+1}/{len(sample_ids)}")
        
        sample_data = df[df['unique_sample_id'] == sample_id].sort_values('days_to_t1')
        
        if len(sample_data) < 20:  # è‡³å°‘20å¤©æ•°æ®
            continue
        
        # ä»æ•°æ®ä¸­è·å–T1æ—¥æœŸ
        t1_row = sample_data.iloc[sample_data['days_to_t1'].abs().argmin()]
        t1_date = safe_to_datetime(t1_row['trade_date'])
        
        feature_dict = {
            'sample_id': sample_id,
            'ts_code': sample_data['ts_code'].iloc[0],
            'name': sample_data['name'].iloc[0],
            'label': int(sample_data['label'].iloc[0]),
            't1_date': t1_date,
        }
        
        # ä»·æ ¼ç‰¹å¾
        feature_dict['close_mean'] = sample_data['close'].mean()
        feature_dict['close_std'] = sample_data['close'].std()
        feature_dict['close_max'] = sample_data['close'].max()
        feature_dict['close_min'] = sample_data['close'].min()
        feature_dict['close_trend'] = (
            (sample_data['close'].iloc[-1] - sample_data['close'].iloc[0]) / 
            sample_data['close'].iloc[0] * 100
        )
        
        # æ¶¨è·Œå¹…ç‰¹å¾
        feature_dict['pct_chg_mean'] = sample_data['pct_chg'].mean()
        feature_dict['pct_chg_std'] = sample_data['pct_chg'].std()
        feature_dict['pct_chg_sum'] = sample_data['pct_chg'].sum()
        feature_dict['positive_days'] = (sample_data['pct_chg'] > 0).sum()
        feature_dict['negative_days'] = (sample_data['pct_chg'] < 0).sum()
        feature_dict['max_gain'] = sample_data['pct_chg'].max()
        feature_dict['max_loss'] = sample_data['pct_chg'].min()
        
        # é‡æ¯”ç‰¹å¾
        if 'volume_ratio' in sample_data.columns:
            feature_dict['volume_ratio_mean'] = sample_data['volume_ratio'].mean()
            feature_dict['volume_ratio_max'] = sample_data['volume_ratio'].max()
            feature_dict['volume_ratio_gt_2'] = (sample_data['volume_ratio'] > 2).sum()
            feature_dict['volume_ratio_gt_4'] = (sample_data['volume_ratio'] > 4).sum()
        
        # MACDç‰¹å¾
        if 'macd' in sample_data.columns:
            macd_data = sample_data['macd'].dropna()
            if len(macd_data) > 0:
                feature_dict['macd_mean'] = macd_data.mean()
                feature_dict['macd_positive_days'] = (macd_data > 0).sum()
                feature_dict['macd_max'] = macd_data.max()
        
        # MAç‰¹å¾
        if 'ma5' in sample_data.columns:
            feature_dict['ma5_mean'] = sample_data['ma5'].mean()
            feature_dict['price_above_ma5'] = (
                sample_data['close'] > sample_data['ma5']
            ).sum()
        
        if 'ma10' in sample_data.columns:
            feature_dict['ma10_mean'] = sample_data['ma10'].mean()
            feature_dict['price_above_ma10'] = (
                sample_data['close'] > sample_data['ma10']
            ).sum()
        
        # å¸‚å€¼ç‰¹å¾
        if 'total_mv' in sample_data.columns:
            mv_data = sample_data['total_mv'].dropna()
            if len(mv_data) > 0:
                feature_dict['total_mv_mean'] = mv_data.mean()
        
        if 'circ_mv' in sample_data.columns:
            circ_mv_data = sample_data['circ_mv'].dropna()
            if len(circ_mv_data) > 0:
                feature_dict['circ_mv_mean'] = circ_mv_data.mean()
        
        # åŠ¨é‡ç‰¹å¾
        days = len(sample_data)
        if days >= 7:
            feature_dict['return_1w'] = (
                (sample_data['close'].iloc[-1] - sample_data['close'].iloc[-7]) /
                sample_data['close'].iloc[-7] * 100
            )
        if days >= 14:
            feature_dict['return_2w'] = (
                (sample_data['close'].iloc[-1] - sample_data['close'].iloc[-14]) /
                sample_data['close'].iloc[-14] * 100
            )
        
        # å¸‚åœºå› å­ç‰¹å¾ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if 'market_pct_chg' in sample_data.columns:
            market_data = sample_data['market_pct_chg'].dropna()
            if len(market_data) > 0:
                feature_dict['market_pct_chg_mean'] = market_data.mean()
        
        if 'market_return_34d' in sample_data.columns:
            market_return_data = sample_data['market_return_34d'].dropna()
            if len(market_return_data) > 0:
                feature_dict['market_return_34d_last'] = market_return_data.iloc[-1]
        
        if 'market_volatility_34d' in sample_data.columns:
            market_vol_data = sample_data['market_volatility_34d'].dropna()
            if len(market_vol_data) > 0:
                feature_dict['market_volatility_34d_last'] = market_vol_data.iloc[-1]
        
        if 'market_trend' in sample_data.columns:
            market_trend_data = sample_data['market_trend'].dropna()
            if len(market_trend_data) > 0:
                feature_dict['market_trend_last'] = market_trend_data.iloc[-1]
        
        if 'excess_return' in sample_data.columns:
            excess_data = sample_data['excess_return'].dropna()
            if len(excess_data) > 0:
                feature_dict['excess_return_mean'] = excess_data.mean()
                feature_dict['excess_return_sum'] = excess_data.sum()
                feature_dict['excess_return_positive_days'] = (excess_data > 0).sum()
        
        if 'excess_return_cumsum' in sample_data.columns:
            excess_cumsum_data = sample_data['excess_return_cumsum'].dropna()
            if len(excess_cumsum_data) > 0:
                feature_dict['excess_return_cumsum_last'] = excess_cumsum_data.iloc[-1]
        
        if 'price_vs_hist_mean' in sample_data.columns:
            hist_mean_data = sample_data['price_vs_hist_mean'].dropna()
            if len(hist_mean_data) > 0:
                feature_dict['price_vs_hist_mean_last'] = hist_mean_data.iloc[-1]
        
        # ä»¥ä¸‹ä½æ•ˆç‰¹å¾å·²å‰”é™¤ï¼ˆé‡è¦æ€§ < é˜ˆå€¼ï¼‰:
        # - price_vs_hist_high_last: 0.0088
        # - volatility_vs_hist_last: 0.0064
        
        # ===== æ–°æŠ€æœ¯å› å­ç‰¹å¾ï¼ˆfullï¼‰=====
        # æ¢æ‰‹ç‡ï¼ˆè‡ªç”±æµé€šè‚¡ï¼‰
        if 'turnover_rate_f' in sample_data.columns:
            turnover_data = sample_data['turnover_rate_f'].dropna()
            if len(turnover_data) > 0:
                feature_dict['turnover_rate_f_mean'] = turnover_data.mean()
                feature_dict['turnover_rate_f_max'] = turnover_data.max()
                feature_dict['turnover_rate_f_std'] = turnover_data.std()
        
        # ä¹–ç¦»ç‡BIAS (bias_short/mid/long)
        if 'bias_short' in sample_data.columns:
            bias_short = sample_data['bias_short'].dropna()
            if len(bias_short) > 0:
                feature_dict['bias_short_last'] = bias_short.iloc[-1]
                feature_dict['bias_short_mean'] = bias_short.mean()
        if 'bias_mid' in sample_data.columns:
            bias_mid = sample_data['bias_mid'].dropna()
            if len(bias_mid) > 0:
                feature_dict['bias_mid_last'] = bias_mid.iloc[-1]
        if 'bias_long' in sample_data.columns:
            bias_long = sample_data['bias_long'].dropna()
            if len(bias_long) > 0:
                feature_dict['bias_long_last'] = bias_long.iloc[-1]
        
        # EMA
        if 'ema_5' in sample_data.columns and 'ema_20' in sample_data.columns:
            ema5 = sample_data['ema_5'].dropna()
            ema20 = sample_data['ema_20'].dropna()
            if len(ema5) > 0 and len(ema20) > 0:
                feature_dict['ema_ratio_5_20'] = ema5.iloc[-1] / ema20.iloc[-1] if ema20.iloc[-1] != 0 else 1
                if len(sample_data['close'].dropna()) > 0:
                    close_last = sample_data['close'].dropna().iloc[-1]
                    feature_dict['price_vs_ema5'] = (close_last - ema5.iloc[-1]) / ema5.iloc[-1] * 100 if ema5.iloc[-1] != 0 else 0
                    feature_dict['price_vs_ema20'] = (close_last - ema20.iloc[-1]) / ema20.iloc[-1] * 100 if ema20.iloc[-1] != 0 else 0
        if 'ema_60' in sample_data.columns:
            ema60 = sample_data['ema_60'].dropna()
            if len(ema60) > 0 and len(sample_data['close'].dropna()) > 0:
                close_last = sample_data['close'].dropna().iloc[-1]
                feature_dict['price_vs_ema60'] = (close_last - ema60.iloc[-1]) / ema60.iloc[-1] * 100 if ema60.iloc[-1] != 0 else 0
        
        # KDJ
        if 'kdj_k' in sample_data.columns:
            kdj_k = sample_data['kdj_k'].dropna()
            if len(kdj_k) > 0:
                feature_dict['kdj_k_last'] = kdj_k.iloc[-1]
                feature_dict['kdj_k_mean'] = kdj_k.mean()
        if 'kdj_d' in sample_data.columns:
            kdj_d = sample_data['kdj_d'].dropna()
            if len(kdj_d) > 0:
                feature_dict['kdj_d_last'] = kdj_d.iloc[-1]
        if 'kdj_j' in sample_data.columns:
            kdj_j = sample_data['kdj_j'].dropna()
            if len(kdj_j) > 0:
                feature_dict['kdj_j_last'] = kdj_j.iloc[-1]
                feature_dict['kdj_j_overbought'] = (kdj_j > 80).sum()
                feature_dict['kdj_j_oversold'] = (kdj_j < 20).sum()
        
        # æ¶¨åœç»Ÿè®¡ (is_limit_up)
        if 'is_limit_up' in sample_data.columns:
            is_limit = sample_data['is_limit_up'].dropna()
            if len(is_limit) > 0:
                feature_dict['limit_up_count'] = is_limit.sum()
        
        # OBV
        if 'obv' in sample_data.columns:
            obv = sample_data['obv'].dropna()
            if len(obv) > 0:
                feature_dict['obv_change'] = (obv.iloc[-1] - obv.iloc[0]) / abs(obv.iloc[0]) * 100 if obv.iloc[0] != 0 else 0
                feature_dict['obv_trend'] = 1 if obv.iloc[-1] > obv.mean() else 0
        
        # æˆäº¤é‡ä¸å‡é‡æ¯” (vol_ma5_ratio/vol_ma20_ratio)
        if 'vol_ma5_ratio' in sample_data.columns:
            vol_r5 = sample_data['vol_ma5_ratio'].dropna()
            if len(vol_r5) > 0:
                feature_dict['vol_ma5_ratio_mean'] = vol_r5.mean()
                feature_dict['vol_ma5_ratio_max'] = vol_r5.max()
        if 'vol_ma20_ratio' in sample_data.columns:
            vol_r20 = sample_data['vol_ma20_ratio'].dropna()
            if len(vol_r20) > 0:
                feature_dict['vol_ma20_ratio_mean'] = vol_r20.mean()
                feature_dict['vol_ma20_ratio_max'] = vol_r20.max()
        
        # ===== é«˜çº§æŠ€æœ¯å› å­ï¼ˆadvancedï¼‰=====
        # åŠ¨é‡å› å­
        for period in [5, 10, 20]:
            col = f'momentum_{period}d'
            if col in sample_data.columns:
                data = sample_data[col].dropna()
                if len(data) > 0:
                    feature_dict[f'{col}_last'] = data.iloc[-1]
                    feature_dict[f'{col}_mean'] = data.mean()
        
        if 'momentum_acceleration' in sample_data.columns:
            data = sample_data['momentum_acceleration'].dropna()
            if len(data) > 0:
                feature_dict['momentum_acceleration_last'] = data.iloc[-1]
        
        # é‡ä»·é…åˆåº¦
        if 'volume_price_corr_10d' in sample_data.columns:
            data = sample_data['volume_price_corr_10d'].dropna()
            if len(data) > 0:
                feature_dict['volume_price_corr_last'] = data.iloc[-1]
        if 'volume_price_match_sum_10d' in sample_data.columns:
            data = sample_data['volume_price_match_sum_10d'].dropna()
            if len(data) > 0:
                feature_dict['volume_price_match_sum'] = data.iloc[-1]
        
        # å¤šæ—¶é—´æ¡†æ¶ç‰¹å¾ (8d, 55d)
        for tf in [8, 55]:
            for metric in ['return', 'price_vs_ma', 'volatility', 'price_position', 'trend_slope']:
                col = f'{metric}_{tf}d'
                if col in sample_data.columns:
                    data = sample_data[col].dropna()
                    if len(data) > 0:
                        feature_dict[f'{col}_last'] = data.iloc[-1]
        
        # çªç ´å½¢æ€
        for period in [10, 20, 55]:
            col = f'breakout_high_{period}d'
            if col in sample_data.columns:
                data = sample_data[col].dropna()
                if len(data) > 0:
                    feature_dict[f'{col}_sum'] = data.sum()
        
        for ma in [5, 10, 20, 55]:
            col = f'breakout_ma{ma}'
            if col in sample_data.columns:
                data = sample_data[col].dropna()
                if len(data) > 0:
                    feature_dict[f'{col}_sum'] = data.sum()
        
        if 'high_volume_breakout' in sample_data.columns:
            data = sample_data['high_volume_breakout'].dropna()
            if len(data) > 0:
                feature_dict['high_volume_breakout_sum'] = data.sum()
        
        if 'consecutive_new_high' in sample_data.columns:
            data = sample_data['consecutive_new_high'].dropna()
            if len(data) > 0:
                feature_dict['consecutive_new_high_max'] = data.max()
        
        # æ”¯æ’‘é˜»åŠ›
        for period in [10, 20]:
            for metric in ['dist_to_support', 'dist_to_resistance']:
                col = f'{metric}_{period}d'
                if col in sample_data.columns:
                    data = sample_data[col].dropna()
                    if len(data) > 0:
                        feature_dict[f'{col}_last'] = data.iloc[-1]
            
            for metric in ['support_strength', 'resistance_strength']:
                col = f'{metric}_{period}d'
                if col in sample_data.columns:
                    data = sample_data[col].dropna()
                    if len(data) > 0:
                        feature_dict[f'{col}_last'] = data.iloc[-1]
        
        if 'channel_width_20d' in sample_data.columns:
            data = sample_data['channel_width_20d'].dropna()
            if len(data) > 0:
                feature_dict['channel_width_last'] = data.iloc[-1]
        
        # é«˜çº§æˆäº¤é‡
        for col in ['volume_trend_slope_10d', 'volume_trend_slope_20d']:
            if col in sample_data.columns:
                data = sample_data[col].dropna()
                if len(data) > 0:
                    feature_dict[f'{col}_last'] = data.iloc[-1]
        
        if 'volume_breakout_count_20d' in sample_data.columns:
            data = sample_data['volume_breakout_count_20d'].dropna()
            if len(data) > 0:
                feature_dict['volume_breakout_count'] = data.iloc[-1]
        
        if 'price_up_vol_down_count_10d' in sample_data.columns:
            data = sample_data['price_up_vol_down_count_10d'].dropna()
            if len(data) > 0:
                feature_dict['price_up_vol_down_count'] = data.iloc[-1]
        
        if 'price_down_vol_up_count_10d' in sample_data.columns:
            data = sample_data['price_down_vol_up_count_10d'].dropna()
            if len(data) > 0:
                feature_dict['price_down_vol_up_count'] = data.iloc[-1]
        
        if 'volume_rsv_20d' in sample_data.columns:
            data = sample_data['volume_rsv_20d'].dropna()
            if len(data) > 0:
                feature_dict['volume_rsv_last'] = data.iloc[-1]
        
        if 'obv_trend' in sample_data.columns:
            data = sample_data['obv_trend'].dropna()
            if len(data) > 0:
                feature_dict['obv_trend_sum'] = data.sum()
        
        features.append(feature_dict)
    
    df_features = pd.DataFrame(features)
    log.success(f"âœ“ ç‰¹å¾æå–å®Œæˆ: {len(df_features)} ä¸ªæ ·æœ¬")
    log.info("")
    
    return df_features


def walk_forward_validation(df_features, n_splits=5, train_size=0.6):
    """
    Walk-forwardéªŒè¯
    
    Args:
        df_features: ç‰¹å¾DataFrame
        n_splits: åˆ†å‰²æ•°é‡ï¼ˆæ—¶é—´çª—å£æ•°ï¼‰
        train_size: è®­ç»ƒé›†å æ¯”
    """
    log.info("="*80)
    log.info("Walk-ForwardéªŒè¯")
    log.info("="*80)
    log.info(f"æ—¶é—´çª—å£æ•°: {n_splits}")
    log.info(f"è®­ç»ƒé›†å æ¯”: {train_size*100}%")
    log.info("")
    
    # æŒ‰æ—¶é—´æ’åº
    df_features = df_features.sort_values('t1_date').reset_index(drop=True)
    
    # å‡†å¤‡ç‰¹å¾å’Œæ ‡ç­¾
    feature_cols = [col for col in df_features.columns 
                    if col not in ['sample_id', 'ts_code', 'name', 'label', 't1_date']]
    
    X = df_features[feature_cols]
    y = df_features['label']
    dates = df_features['t1_date']
    
    log.info(f"ç¼ºå¤±å€¼å¤„ç†: æ¯ä¸ªçª—å£ä½¿ç”¨è®­ç»ƒé›†ä¸­ä½æ•°å¡«å……ï¼ˆé¿å…æœªæ¥å‡½æ•°ï¼‰")
    
    # è®¡ç®—æ¯ä¸ªçª—å£çš„å¤§å°
    total_samples = len(df_features)
    window_size = total_samples // n_splits
    
    results = []
    
    for i in range(n_splits):
        log.info(f"\n{'='*80}")
        log.info(f"æ—¶é—´çª—å£ {i+1}/{n_splits}")
        log.info(f"{'='*80}")
        
        # è®¡ç®—è®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„ç´¢å¼•
        start_idx = i * window_size
        end_idx = (i + 1) * window_size if i < n_splits - 1 else total_samples
        
        window_data = df_features.iloc[start_idx:end_idx]
        
        # æŒ‰æ—¶é—´åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
        split_idx = int(len(window_data) * train_size)
        
        train_data = window_data.iloc[:split_idx]
        test_data = window_data.iloc[split_idx:]
        
        if len(test_data) < 10:  # æµ‹è¯•é›†å¤ªå°ï¼Œè·³è¿‡
            log.warning(f"âš ï¸  æµ‹è¯•é›†æ ·æœ¬å¤ªå°‘({len(test_data)}ä¸ª)ï¼Œè·³è¿‡æ­¤çª—å£")
            continue
        
        # å‡†å¤‡è®­ç»ƒé›†å’Œæµ‹è¯•é›†
        X_train = train_data[feature_cols].copy()
        y_train = train_data['label']
        X_test = test_data[feature_cols].copy()
        y_test = test_data['label']
        
        # ç¼ºå¤±å€¼å¤„ç†ï¼šä½¿ç”¨è®­ç»ƒé›†çš„ä¸­ä½æ•°å¡«å……ï¼ˆé¿å…æœªæ¥å‡½æ•°ï¼‰
        train_medians = X_train.median()
        X_train = X_train.fillna(train_medians)
        X_test = X_test.fillna(train_medians)  # ç”¨è®­ç»ƒé›†çš„ç»Ÿè®¡é‡å¡«å……æµ‹è¯•é›†
        
        train_dates = train_data['t1_date']
        test_dates = test_data['t1_date']
        
        log.info(f"\næ—¶é—´èŒƒå›´:")
        log.info(f"  è®­ç»ƒé›†: {train_dates.min().date()} è‡³ {train_dates.max().date()}")
        log.info(f"  æµ‹è¯•é›†: {test_dates.min().date()} è‡³ {test_dates.max().date()}")
        
        log.info(f"\næ ·æœ¬åˆ†å¸ƒ:")
        log.info(f"  è®­ç»ƒé›†: {len(X_train)} ä¸ª (æ­£:{y_train.sum()}, è´Ÿ:{len(y_train)-y_train.sum()})")
        log.info(f"  æµ‹è¯•é›†: {len(X_test)} ä¸ª (æ­£:{y_test.sum()}, è´Ÿ:{len(y_test)-y_test.sum()})")
        
        # è®¡ç®—ç±»åˆ«æƒé‡ï¼ˆå¤„ç†æ ·æœ¬ä¸å‡è¡¡ï¼‰
        neg_count = (y_train == 0).sum()
        pos_count = (y_train == 1).sum()
        raw_weight = neg_count / pos_count if pos_count > 0 else 1.0
        # é™åˆ¶æƒé‡èŒƒå›´åœ¨[0.5, 2.0]ä¹‹é—´ï¼Œé¿å…è¿‡åº¦è¡¥å¿
        scale_pos_weight = max(0.5, min(2.0, raw_weight))
        
        # è®­ç»ƒæ¨¡å‹
        model = xgb.XGBClassifier(
            n_estimators=100,
            max_depth=6,
            learning_rate=0.1,
            subsample=0.8,
            colsample_bytree=0.8,
            scale_pos_weight=scale_pos_weight,  # å¤„ç†æ ·æœ¬ä¸å‡è¡¡
            random_state=42,
            eval_metric='logloss'
        )
        
        model.fit(X_train, y_train, verbose=False)
        
        # é¢„æµ‹
        y_pred = model.predict(X_test)
        y_prob = model.predict_proba(X_test)[:, 1]
        
        # è¯„ä¼°
        accuracy = accuracy_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred, zero_division=0)
        recall = recall_score(y_test, y_pred, zero_division=0)
        f1 = f1_score(y_test, y_pred, zero_division=0)
        
        try:
            auc = roc_auc_score(y_test, y_prob)
        except:
            auc = 0.0
        
        log.info(f"\næ€§èƒ½æŒ‡æ ‡:")
        log.info(f"  å‡†ç¡®ç‡: {accuracy*100:.2f}%")
        log.info(f"  ç²¾ç¡®ç‡: {precision*100:.2f}%")
        log.info(f"  å¬å›ç‡: {recall*100:.2f}%")
        log.info(f"  F1-Score: {f1*100:.2f}%")
        log.info(f"  AUC-ROC: {auc:.4f}")
        
        # è®°å½•ç»“æœ
        results.append({
            'window': i + 1,
            'train_start': train_dates.min().date().isoformat(),
            'train_end': train_dates.max().date().isoformat(),
            'test_start': test_dates.min().date().isoformat(),
            'test_end': test_dates.max().date().isoformat(),
            'train_samples': len(X_train),
            'test_samples': len(X_test),
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1_score': f1,
            'auc': auc
        })
    
    return results


def analyze_results(results):
    """åˆ†æWalk-forwardéªŒè¯ç»“æœ"""
    log.info("\n" + "="*80)
    log.info("Walk-ForwardéªŒè¯ç»“æœæ±‡æ€»")
    log.info("="*80)
    
    df_results = pd.DataFrame(results)
    
    # è®¡ç®—ç»Ÿè®¡é‡
    metrics = ['accuracy', 'precision', 'recall', 'f1_score', 'auc']
    
    log.info("\nå„æ—¶é—´çª—å£è¡¨ç°:")
    for _, row in df_results.iterrows():
        log.info(f"\nçª—å£ {int(row['window'])}: {row['test_start']} è‡³ {row['test_end']}")
        log.info(f"  å‡†ç¡®ç‡: {row['accuracy']*100:.2f}%")
        log.info(f"  ç²¾ç¡®ç‡: {row['precision']*100:.2f}%")
        log.info(f"  å¬å›ç‡: {row['recall']*100:.2f}%")
        log.info(f"  F1-Score: {row['f1_score']*100:.2f}%")
        log.info(f"  AUC: {row['auc']:.4f}")
    
    log.info("\n" + "="*80)
    log.info("ç»Ÿè®¡æ‘˜è¦")
    log.info("="*80)
    
    for metric in metrics:
        mean_val = df_results[metric].mean()
        std_val = df_results[metric].std()
        min_val = df_results[metric].min()
        max_val = df_results[metric].max()
        
        metric_name = {
            'accuracy': 'å‡†ç¡®ç‡',
            'precision': 'ç²¾ç¡®ç‡',
            'recall': 'å¬å›ç‡',
            'f1_score': 'F1-Score',
            'auc': 'AUC-ROC'
        }[metric]
        
        log.info(f"\n{metric_name}:")
        log.info(f"  å¹³å‡å€¼: {mean_val*100:.2f}%")
        log.info(f"  æ ‡å‡†å·®: {std_val*100:.2f}%")
        log.info(f"  æœ€å°å€¼: {min_val*100:.2f}%")
        log.info(f"  æœ€å¤§å€¼: {max_val*100:.2f}%")
    
    # ç¨³å®šæ€§è¯„ä¼°
    f1_std = df_results['f1_score'].std()
    if f1_std < 0.05:
        stability = "éå¸¸ç¨³å®š â­â­â­â­â­"
    elif f1_std < 0.10:
        stability = "ç¨³å®š â­â­â­â­"
    elif f1_std < 0.15:
        stability = "ä¸€èˆ¬ â­â­â­"
    else:
        stability = "ä¸ç¨³å®š â­â­"
    
    log.info(f"\næ¨¡å‹ç¨³å®šæ€§è¯„ä¼°: {stability}")
    log.info(f"  (åŸºäºF1-Scoreæ ‡å‡†å·®: {f1_std*100:.2f}%)")
    
    return df_results


def save_results(results_df):
    """ä¿å­˜éªŒè¯ç»“æœ"""
    output_file = 'data/results/walk_forward_validation_results.json'
    os.makedirs('data/results', exist_ok=True)
    
    results_dict = {
        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'n_windows': len(results_df),
        'summary': {
            'accuracy_mean': float(results_df['accuracy'].mean()),
            'accuracy_std': float(results_df['accuracy'].std()),
            'precision_mean': float(results_df['precision'].mean()),
            'precision_std': float(results_df['precision'].std()),
            'recall_mean': float(results_df['recall'].mean()),
            'recall_std': float(results_df['recall'].std()),
            'f1_score_mean': float(results_df['f1_score'].mean()),
            'f1_score_std': float(results_df['f1_score'].std()),
            'auc_mean': float(results_df['auc'].mean()),
            'auc_std': float(results_df['auc'].std()),
        },
        'windows': results_df.to_dict('records')
    }
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results_dict, f, indent=2, ensure_ascii=False)
    
    log.success(f"\nâœ“ éªŒè¯ç»“æœå·²ä¿å­˜: {output_file}")


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='Walk-ForwardéªŒè¯')
    parser.add_argument('--use-market-factors', action='store_true', 
                       help='ä½¿ç”¨å¸¦å¸‚åœºå› å­çš„ç‰¹å¾æ–‡ä»¶')
    parser.add_argument('--use-tech-factors', action='store_true',
                       help='ä½¿ç”¨å¸¦æ–°æŠ€æœ¯å› å­çš„v2ç‰¹å¾æ–‡ä»¶')
    parser.add_argument('--use-advanced-factors', action='store_true',
                       help='ä½¿ç”¨å¸¦é«˜çº§æŠ€æœ¯å› å­çš„ç‰¹å¾æ–‡ä»¶')
    parser.add_argument('--neg-version', default='v2', choices=['v1', 'v2'],
                       help='è´Ÿæ ·æœ¬ç‰ˆæœ¬')
    args = parser.parse_args()
    
    # é…ç½®
    NEG_VERSION = args.neg_version
    USE_ADVANCED_FACTORS = args.use_advanced_factors
    USE_TECH_FACTORS = args.use_tech_factors and not USE_ADVANCED_FACTORS
    USE_MARKET_FACTORS = args.use_market_factors or (not args.use_tech_factors and not USE_ADVANCED_FACTORS)
    
    log.info("="*80)
    log.info("Walk-ForwardéªŒè¯ - å¤šæ—¶é—´çª—å£æ¨¡å‹ç¨³å®šæ€§æµ‹è¯•")
    log.info("="*80)
    log.info(f"é…ç½®: è´Ÿæ ·æœ¬ç‰ˆæœ¬={NEG_VERSION}, å¸‚åœºå› å­={USE_MARKET_FACTORS}, æŠ€æœ¯å› å­={USE_TECH_FACTORS}, é«˜çº§å› å­={USE_ADVANCED_FACTORS}")
    log.info("")
    
    try:
        # 1. åŠ è½½æ•°æ®
        df = load_and_prepare_data(
            neg_version=NEG_VERSION, 
            use_market_factors=USE_MARKET_FACTORS,
            use_tech_factors=USE_TECH_FACTORS,
            use_advanced_factors=USE_ADVANCED_FACTORS
        )
        
        # 2. ç‰¹å¾å·¥ç¨‹
        df_features = extract_features_with_time(df)
        
        # 3. Walk-forwardéªŒè¯
        results = walk_forward_validation(df_features, n_splits=5, train_size=0.6)
        
        # 4. åˆ†æç»“æœ
        results_df = analyze_results(results)
        
        # 5. ä¿å­˜ç»“æœ
        save_results(results_df)
        
        log.info("\n" + "="*80)
        log.success("âœ… Walk-ForwardéªŒè¯å®Œæˆï¼")
        log.info("="*80)
        
    except Exception as e:
        log.error(f"âœ— Walk-ForwardéªŒè¯å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()


if __name__ == '__main__':
    main()

