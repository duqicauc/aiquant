#!/usr/bin/env python3
"""
è‚¡ç¥¨è¯„åˆ†è„šæœ¬ - é«˜çº§ç‰ˆï¼ˆæ”¯æŒå¸‚åœºå› å­å’Œé«˜çº§æŠ€æœ¯å› å­ï¼‰

ç‰¹ç‚¹ï¼š
1. ä¸ train_xgboost_timeseries.py ç‰¹å¾æå–æ–¹å¼å®Œå…¨ä¸€è‡´
2. æ”¯æŒå¸‚åœºå› å­ï¼ˆmarket_pct_chg, excess_return ç­‰ï¼‰
3. æ”¯æŒé«˜çº§æŠ€æœ¯å› å­ï¼ˆåŠ¨é‡ã€é‡ä»·é…åˆã€çªç ´å½¢æ€ã€æ”¯æ’‘é˜»åŠ›ç­‰ï¼‰
4. æ”¯æŒå†å²å›æµ‹ï¼ˆæŒ‡å®šæ—¥æœŸï¼‰

ä½¿ç”¨æ–¹æ³•ï¼š
    # å¯¹æœ€æ–°æ”¶ç›˜æ•°æ®è¯„åˆ†
    python scripts/score_stocks_advanced.py
    
    # å¯¹20251225æ”¶ç›˜åè¯„åˆ†
    python scripts/score_stocks_advanced.py --date 20251225
    
    # é™åˆ¶è¯„åˆ†æ•°é‡ï¼ˆæµ‹è¯•ç”¨ï¼‰
    python scripts/score_stocks_advanced.py --max-stocks 100
"""
import sys
import os
import argparse
import json
import warnings
from datetime import datetime, timedelta
from pathlib import Path
from scipy import stats
import pandas as pd
import numpy as np
import xgboost as xgb

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))
warnings.filterwarnings('ignore', category=FutureWarning)

from src.data.data_manager import DataManager
from src.utils.logger import log


def load_model_and_features():
    """
    åŠ è½½æœ€æ–°è®­ç»ƒçš„æ¨¡å‹å’Œç‰¹å¾åç§°
    
    Returns:
        booster: XGBoost Boosteræ¨¡å‹
        feature_names: ç‰¹å¾åç§°åˆ—è¡¨
        model_info: æ¨¡å‹ä¿¡æ¯å­—å…¸
    """
    log.info("="*80)
    log.info("åŠ è½½æ¨¡å‹")
    log.info("="*80)
    
    # æ–¹æ¡ˆ1ï¼šä» v1.4.0 ç‰ˆæœ¬ç›®å½•åŠ è½½
    version_model_path = PROJECT_ROOT / 'data' / 'models' / 'breakout_launch_scorer' / 'versions' / 'v1.4.0' / 'model' / 'model.json'
    
    # æ–¹æ¡ˆ2ï¼šä»è®­ç»ƒæ¨¡å‹ç›®å½•åŠ è½½æœ€æ–°æ¨¡å‹
    training_model_dir = PROJECT_ROOT / 'data' / 'training' / 'models'
    metrics_file = PROJECT_ROOT / 'data' / 'training' / 'metrics' / 'xgboost_timeseries_v2_metrics.json'
    
    # åŠ è½½æœ€æ–°çš„è®­ç»ƒæ¨¡å‹
    model_files = list(training_model_dir.glob('xgboost_timeseries_v2_*.json'))
    if model_files:
        model_path = max(model_files, key=lambda x: x.stat().st_mtime)
        log.info(f"åŠ è½½æœ€æ–°è®­ç»ƒæ¨¡å‹: {model_path.name}")
    elif version_model_path.exists():
        model_path = version_model_path
        log.info(f"åŠ è½½ç‰ˆæœ¬æ¨¡å‹: v1.4.0")
    else:
        raise FileNotFoundError("æœªæ‰¾åˆ°ä»»ä½•å¯ç”¨çš„æ¨¡å‹æ–‡ä»¶")
    
    # åŠ è½½ Booster
    booster = xgb.Booster()
    booster.load_model(str(model_path))
    
    # ã€å…³é”®ã€‘ä»æ¨¡å‹å†…éƒ¨è·å–ç‰¹å¾åç§°ï¼Œç¡®ä¿é¡ºåºä¸è®­ç»ƒæ—¶ä¸€è‡´
    feature_names = booster.feature_names
    if feature_names is None:
        # å¦‚æœæ¨¡å‹å†…éƒ¨æ²¡æœ‰ç‰¹å¾åç§°ï¼Œå°è¯•å…¶ä»–æ–¹å¼
        if metrics_file.exists():
            with open(metrics_file, 'r', encoding='utf-8') as f:
                metrics = json.load(f)
            if 'feature_importance' in metrics:
                feature_names = [item['feature'] for item in metrics['feature_importance']]
                log.warning(f"ä» metrics æ–‡ä»¶åŠ è½½ç‰¹å¾åç§°ï¼ˆå¯èƒ½é¡ºåºä¸ä¸€è‡´ï¼‰: {len(feature_names)} ä¸ªç‰¹å¾")
        else:
            raise ValueError("æ— æ³•è·å–ç‰¹å¾åç§°")
    else:
        log.info(f"ä»æ¨¡å‹å†…éƒ¨è·å–ç‰¹å¾åç§°: {len(feature_names)} ä¸ªç‰¹å¾")
    
    model_info = {
        'model_path': str(model_path),
        'model_name': 'breakout_launch_scorer',
        'version': 'v1.4.0',
        'feature_count': len(feature_names)
    }
    
    log.success(f"âœ“ æ¨¡å‹åŠ è½½æˆåŠŸï¼Œç‰¹å¾æ•°: {len(feature_names)}")
    
    return booster, feature_names, model_info


def _vectorized_rolling_slope(y: np.ndarray, window: int) -> np.ndarray:
    """
    å‘é‡åŒ–è®¡ç®—æ»šåŠ¨çª—å£çº¿æ€§å›å½’æ–œç‡ï¼ˆæ¯”å¾ªç¯å¿«50å€ï¼‰
    
    ä½¿ç”¨å…¬å¼: slope = (n*sum(xy) - sum(x)*sum(y)) / (n*sum(x^2) - sum(x)^2)
    å…¶ä¸­ x = [0, 1, 2, ..., window-1]
    """
    n = len(y)
    result = np.full(n, np.nan)
    
    if n < window:
        return result
    
    # é¢„è®¡ç®— x ç›¸å…³å¸¸é‡ (x = [0, 1, ..., window-1])
    x = np.arange(window)
    sum_x = x.sum()  # = window*(window-1)/2
    sum_x2 = (x ** 2).sum()  # = window*(window-1)*(2*window-1)/6
    denom = window * sum_x2 - sum_x ** 2
    
    if denom == 0:
        return result
    
    # ä½¿ç”¨ cumsum æŠ€å·§è®¡ç®—æ»šåŠ¨ sum(y) å’Œ sum(xy)
    y_cumsum = np.cumsum(y)
    xy_cumsum = np.cumsum(np.arange(n) * y)
    
    # å¯¹æ¯ä¸ªçª—å£ä½ç½®è®¡ç®—æ–œç‡
    for i in range(window - 1, n):
        start = i - window + 1
        if start == 0:
            sum_y = y_cumsum[i]
            # sum(xy) éœ€è¦è°ƒæ•´ x çš„åç§»
            sum_xy = np.sum(x * y[start:i+1])
        else:
            sum_y = y_cumsum[i] - y_cumsum[start - 1]
            sum_xy = np.sum(x * y[start:i+1])
        
        result[i] = (window * sum_xy - sum_x * sum_y) / denom
    
    return result


def _vectorized_rolling_slope_fast(y: np.ndarray, window: int) -> np.ndarray:
    """
    æ›´å¿«çš„å‘é‡åŒ–æ»šåŠ¨æ–œç‡è®¡ç®—ï¼ˆä½¿ç”¨ pandas rollingï¼‰
    """
    import pandas as pd
    
    n = len(y)
    if n < window:
        return np.full(n, np.nan)
    
    # åˆ›å»º Series
    s = pd.Series(y)
    
    # x å¸¸é‡
    x = np.arange(window)
    sum_x = x.sum()
    sum_x2 = (x ** 2).sum()
    denom = window * sum_x2 - sum_x ** 2
    
    if denom == 0:
        return np.full(n, np.nan)
    
    # æ»šåŠ¨è®¡ç®— sum(y)
    sum_y = s.rolling(window).sum()
    
    # æ»šåŠ¨è®¡ç®— sum(i*y_i) ç„¶åè°ƒæ•´
    # æŠ€å·§: å¯¹äºçª—å£ [y_{t-w+1}, ..., y_t]ï¼Œsum(x*y) = sum((j - (t-w+1)) * y_j)
    idx = np.arange(n)
    weighted = pd.Series(idx * y)
    sum_idx_y = weighted.rolling(window).sum()
    
    # è°ƒæ•´ä¸º sum(x*y) where x = [0, 1, ..., w-1]
    # sum_xy = sum((j - (t-w+1)) * y_j) = sum_idx_y - (t-w+1) * sum_y
    t_minus_w_plus_1 = pd.Series(idx - window + 1)
    sum_xy = sum_idx_y - t_minus_w_plus_1 * sum_y
    
    # è®¡ç®—æ–œç‡
    slope = (window * sum_xy - sum_x * sum_y) / denom
    
    return slope.values


def calculate_advanced_factors(df: pd.DataFrame) -> pd.DataFrame:
    """
    è®¡ç®—é«˜çº§æŠ€æœ¯å› å­ï¼ˆä¸ add_advanced_factors_v2.py ä¸­çš„é€»è¾‘ä¸€è‡´ï¼‰
    """
    df = df.copy()
    df = df.sort_values('trade_date').reset_index(drop=True)
    
    n = len(df)
    if n < 10:
        return df
    
    # ==================== 1. åŠ¨é‡å› å­ ====================
    for period in [5, 10, 20]:
        if n >= period:
            df[f'momentum_{period}d'] = df['close'].pct_change(period) * 100
    
    if 'momentum_10d' in df.columns and n >= 15:
        df['momentum_acceleration'] = df['momentum_10d'].diff(5)
    
    # ==================== 2. é‡ä»·é…åˆ ====================
    if 'vol' in df.columns:
        df['price_change'] = df['close'].pct_change()
        df['volume_change'] = df['vol'].pct_change()
        
        if n >= 10:
            df['volume_price_corr_10d'] = df['price_change'].rolling(10).corr(df['volume_change'])
        if n >= 20:
            df['volume_price_corr_20d'] = df['price_change'].rolling(20).corr(df['volume_change'])
        
        df['volume_price_match'] = np.where(
            (df['price_change'] > 0) & (df['volume_change'] > 0), 1,
            np.where((df['price_change'] < 0) & (df['volume_change'] < 0), 1, -1)
        )
        if n >= 10:
            df['volume_price_match_sum_10d'] = df['volume_price_match'].rolling(10).sum()
    
    # ==================== 3. å¤šæ—¶é—´æ¡†æ¶ç‰¹å¾ ====================
    for tf in [8, 34, 55]:
        if n >= tf:
            df[f'return_{tf}d'] = (df['close'] - df['close'].shift(tf)) / df['close'].shift(tf) * 100
            df[f'ma_{tf}d'] = df['close'].rolling(tf).mean()
            df[f'price_vs_ma_{tf}d'] = (df['close'] - df[f'ma_{tf}d']) / df[f'ma_{tf}d'] * 100
            df[f'volatility_{tf}d'] = df['pct_chg'].rolling(tf).std()
            df[f'high_{tf}d'] = df['close'].rolling(tf).max()
            df[f'low_{tf}d'] = df['close'].rolling(tf).min()
            df[f'price_position_{tf}d'] = (df['close'] - df[f'low_{tf}d']) / (df[f'high_{tf}d'] - df[f'low_{tf}d'] + 1e-8) * 100
            
            # è¶‹åŠ¿æ–œç‡ï¼ˆçº¯å‘é‡åŒ–è®¡ç®—ï¼Œæ›´å¿«ï¼‰
            df[f'trend_slope_{tf}d'] = _vectorized_rolling_slope_fast(df['close'].values, tf)
    
    # ==================== 4. çªç ´å½¢æ€ ====================
    for period in [10, 20, 55]:
        if n >= period:
            df[f'prev_high_{period}d'] = df['close'].shift(1).rolling(period).max()
            df[f'breakout_high_{period}d'] = (df['close'] > df[f'prev_high_{period}d']).astype(int)
    
    for ma_period in [5, 10, 20, 55]:
        ma_col = f'ma_{ma_period}d'
        if ma_col not in df.columns and n >= ma_period:
            df[ma_col] = df['close'].rolling(ma_period).mean()
        
        if ma_col in df.columns:
            df[f'breakout_ma{ma_period}'] = (
                (df['close'] > df[ma_col]) & 
                (df['close'].shift(1) <= df[ma_col].shift(1))
            ).astype(int)
    
    if 'vol' in df.columns and n >= 20:
        vol_ma20 = df['vol'].rolling(20).mean()
        df['breakout_volume_ratio'] = df['vol'] / (vol_ma20 + 1e-8)
        df['high_volume_breakout'] = (df['breakout_volume_ratio'] > 1.5).astype(int)
    
    if 'breakout_high_20d' in df.columns:
        consecutive = []
        count = 0
        for val in df['breakout_high_20d']:
            if val == 1:
                count += 1
            else:
                count = 0
            consecutive.append(count)
        df['consecutive_new_high'] = consecutive
    
    # ==================== 5. æ”¯æ’‘/é˜»åŠ›ä½ ====================
    for period in [10, 20, 55]:
        if n >= period:
            df[f'resistance_{period}d'] = df['close'].shift(1).rolling(period).max()
            df[f'support_{period}d'] = df['close'].shift(1).rolling(period).min()
            df[f'dist_to_resistance_{period}d'] = (df[f'resistance_{period}d'] - df['close']) / df['close'] * 100
            df[f'dist_to_support_{period}d'] = (df['close'] - df[f'support_{period}d']) / df['close'] * 100
            
            near_support = (abs(df['close'] - df[f'support_{period}d']) / df['close'] < 0.02).astype(int)
            df[f'support_strength_{period}d'] = near_support.rolling(period).sum()
            
            near_resistance = (abs(df['close'] - df[f'resistance_{period}d']) / df['close'] < 0.02).astype(int)
            df[f'resistance_strength_{period}d'] = near_resistance.rolling(period).sum()
    
    if 'resistance_20d' in df.columns and 'support_20d' in df.columns:
        df['channel_width_20d'] = (df['resistance_20d'] - df['support_20d']) / df['close'] * 100
    
    # ==================== 6. æˆäº¤é‡ç‰¹å¾å¢å¼º ====================
    if 'vol' in df.columns:
        for period in [10, 20]:
            if n >= period:
                # çº¯å‘é‡åŒ–è®¡ç®—æˆäº¤é‡è¶‹åŠ¿æ–œç‡
                vol_slope = _vectorized_rolling_slope_fast(df['vol'].values, period)
                vol_ma = df['vol'].rolling(period).mean().values
                df[f'volume_trend_slope_{period}d'] = vol_slope / (vol_ma + 1e-8) * 100
        
        if n >= 20:
            vol_ma20 = df['vol'].rolling(20).mean()
            vol_breakout = (df['vol'] > vol_ma20 * 2).astype(int)
            df['volume_breakout_count_20d'] = vol_breakout.rolling(20).sum()
        
        df['price_up_vol_down'] = (
            (df['close'] > df['close'].shift(1)) & 
            (df['vol'] < df['vol'].shift(1))
        ).astype(int)
        if n >= 10:
            df['price_up_vol_down_count_10d'] = df['price_up_vol_down'].rolling(10).sum()
        
        df['price_down_vol_up'] = (
            (df['close'] < df['close'].shift(1)) & 
            (df['vol'] > df['vol'].shift(1))
        ).astype(int)
        if n >= 10:
            df['price_down_vol_up_count_10d'] = df['price_down_vol_up'].rolling(10).sum()
        
        if n >= 20:
            vol_high_20 = df['vol'].rolling(20).max()
            vol_low_20 = df['vol'].rolling(20).min()
            df['volume_rsv_20d'] = (df['vol'] - vol_low_20) / (vol_high_20 - vol_low_20 + 1e-8) * 100
        
        df['obv_calc'] = (np.sign(df['close'].diff()) * df['vol']).fillna(0).cumsum()
        if n >= 10:
            df['obv_ma10'] = df['obv_calc'].rolling(10).mean()
            df['obv_trend'] = (df['obv_calc'] > df['obv_ma10']).astype(int)
    
    return df


def get_cached_market_data(dm: DataManager, target_date: str, lookback_days: int = 120) -> pd.DataFrame:
    """
    è·å–å¹¶ç¼“å­˜å¸‚åœºæ•°æ®ï¼ˆåªè°ƒç”¨ä¸€æ¬¡APIï¼‰
    
    Args:
        dm: DataManagerå®ä¾‹
        target_date: ç›®æ ‡æ—¥æœŸ
        lookback_days: å›çœ‹å¤©æ•°
        
    Returns:
        df_market: åŒ…å«å¸‚åœºå› å­çš„DataFrame
    """
    try:
        start_date = (datetime.strptime(target_date, '%Y%m%d') - timedelta(days=lookback_days)).strftime('%Y%m%d')
        
        # è·å–æ²ªæ·±300æŒ‡æ•°
        df_index = dm.get_index_daily('000300.SH', start_date, target_date)
        
        if df_index is None or df_index.empty:
            log.warning("è·å–æ²ªæ·±300æŒ‡æ•°æ•°æ®å¤±è´¥")
            return None
        
        df_index['trade_date'] = pd.to_datetime(df_index['trade_date'])
        df_index = df_index.rename(columns={'pct_chg': 'market_pct_chg', 'close': 'market_close'})
        
        # è®¡ç®—å¸‚åœº34æ—¥æ”¶ç›Šç‡å’Œæ³¢åŠ¨ç‡
        df_index = df_index.sort_values('trade_date')
        df_index['market_return_34d'] = df_index['market_close'].pct_change(34) * 100
        df_index['market_volatility_34d'] = df_index['market_pct_chg'].rolling(34).std()
        df_index['market_trend'] = (df_index['market_return_34d'] > 0).astype(int)
        
        log.success(f"âœ“ å¸‚åœºæ•°æ®å·²ç¼“å­˜: {len(df_index)} æ¡è®°å½•")
        return df_index[['trade_date', 'market_pct_chg', 'market_return_34d', 'market_volatility_34d', 'market_trend']]
        
    except Exception as e:
        log.warning(f"è·å–å¸‚åœºæ•°æ®å¤±è´¥: {e}")
        return None


def calculate_market_factors(df: pd.DataFrame, df_market: pd.DataFrame) -> pd.DataFrame:
    """
    è®¡ç®—å¸‚åœºå› å­ï¼ˆä½¿ç”¨ç¼“å­˜çš„å¸‚åœºæ•°æ®ï¼Œé¿å…é‡å¤APIè°ƒç”¨ï¼‰
    
    Args:
        df: è‚¡ç¥¨æ—¥çº¿æ•°æ®
        df_market: ç¼“å­˜çš„å¸‚åœºæ•°æ®ï¼ˆç”±get_cached_market_dataè¿”å›ï¼‰
    """
    df = df.copy()
    
    if df_market is None:
        return df
    
    try:
        # åˆå¹¶å¸‚åœºæ•°æ®
        df['trade_date'] = pd.to_datetime(df['trade_date'])
        df = pd.merge(
            df,
            df_market,
            on='trade_date',
            how='left'
        )
        
        # è®¡ç®—è¶…é¢æ”¶ç›Š
        if 'pct_chg' in df.columns and 'market_pct_chg' in df.columns:
            df['excess_return'] = df['pct_chg'] - df['market_pct_chg']
            df['excess_return_cumsum'] = df['excess_return'].cumsum()
        
        # å†å²ä»·æ ¼ç»Ÿè®¡
        df['price_vs_hist_mean'] = (df['close'] - df['close'].rolling(34).mean()) / df['close'].rolling(34).mean() * 100
    except Exception as e:
        pass  # é™é»˜å¤„ç†ï¼Œä¸å½±å“è¯„åˆ†
    
    return df


def extract_features_from_sample(sample_data: pd.DataFrame, feature_names: list) -> dict:
    """
    ä»34å¤©æ—¶åºæ•°æ®æå–ç‰¹å¾ï¼ˆä¸ train_xgboost_timeseries.py ä¸­çš„ extract_features_with_time ä¸€è‡´ï¼‰
    
    Args:
        sample_data: 34å¤©çš„æ—¥çº¿æ•°æ®
        feature_names: ç‰¹å¾åç§°åˆ—è¡¨
        
    Returns:
        feature_dict: ç‰¹å¾å­—å…¸
    """
    if len(sample_data) < 20:
        return None
    
    feature_dict = {}
    
    # ä»·æ ¼ç‰¹å¾
    feature_dict['close_mean'] = sample_data['close'].mean()
    feature_dict['close_std'] = sample_data['close'].std()
    feature_dict['close_max'] = sample_data['close'].max()
    feature_dict['close_min'] = sample_data['close'].min()
    feature_dict['close_trend'] = (
        (sample_data['close'].iloc[-1] - sample_data['close'].iloc[0]) / 
        sample_data['close'].iloc[0] * 100
    )
    
    # æ¶¨è·Œå¹…ç‰¹å¾
    feature_dict['pct_chg_mean'] = sample_data['pct_chg'].mean()
    feature_dict['pct_chg_std'] = sample_data['pct_chg'].std()
    feature_dict['pct_chg_sum'] = sample_data['pct_chg'].sum()
    feature_dict['positive_days'] = (sample_data['pct_chg'] > 0).sum()
    feature_dict['negative_days'] = (sample_data['pct_chg'] < 0).sum()
    feature_dict['max_gain'] = sample_data['pct_chg'].max()
    feature_dict['max_loss'] = sample_data['pct_chg'].min()
    
    # é‡æ¯”ç‰¹å¾
    if 'volume_ratio' in sample_data.columns:
        feature_dict['volume_ratio_mean'] = sample_data['volume_ratio'].mean()
        feature_dict['volume_ratio_max'] = sample_data['volume_ratio'].max()
        feature_dict['volume_ratio_gt_2'] = (sample_data['volume_ratio'] > 2).sum()
        feature_dict['volume_ratio_gt_4'] = (sample_data['volume_ratio'] > 4).sum()
    
    # MACDç‰¹å¾
    if 'macd' in sample_data.columns:
        macd_data = sample_data['macd'].dropna()
        if len(macd_data) > 0:
            feature_dict['macd_mean'] = macd_data.mean()
            feature_dict['macd_positive_days'] = (macd_data > 0).sum()
            feature_dict['macd_max'] = macd_data.max()
    
    # MAç‰¹å¾
    if 'ma5' in sample_data.columns:
        feature_dict['ma5_mean'] = sample_data['ma5'].mean()
        feature_dict['price_above_ma5'] = (sample_data['close'] > sample_data['ma5']).sum()
    
    if 'ma10' in sample_data.columns:
        feature_dict['ma10_mean'] = sample_data['ma10'].mean()
        feature_dict['price_above_ma10'] = (sample_data['close'] > sample_data['ma10']).sum()
    
    # å¸‚å€¼ç‰¹å¾
    if 'total_mv' in sample_data.columns:
        mv_data = sample_data['total_mv'].dropna()
        if len(mv_data) > 0:
            feature_dict['total_mv_mean'] = mv_data.mean()
    
    if 'circ_mv' in sample_data.columns:
        circ_mv_data = sample_data['circ_mv'].dropna()
        if len(circ_mv_data) > 0:
            feature_dict['circ_mv_mean'] = circ_mv_data.mean()
    
    # RSIç‰¹å¾
    for rsi_period in [6, 12, 24]:
        col = f'rsi_{rsi_period}'
        if col in sample_data.columns:
            rsi_data = sample_data[col].dropna()
            if len(rsi_data) > 0:
                feature_dict[f'rsi_{rsi_period}_mean'] = rsi_data.mean()
                feature_dict[f'rsi_{rsi_period}_std'] = rsi_data.std()
                feature_dict[f'rsi_{rsi_period}_last'] = rsi_data.iloc[-1]
                feature_dict[f'rsi_{rsi_period}_max'] = rsi_data.max()
                feature_dict[f'rsi_{rsi_period}_min'] = rsi_data.min()
                feature_dict[f'rsi_{rsi_period}_gt_70'] = (rsi_data > 70).sum()
                feature_dict[f'rsi_{rsi_period}_lt_30'] = (rsi_data < 30).sum()
    
    # åŠ¨é‡ç‰¹å¾
    days = len(sample_data)
    if days >= 7:
        feature_dict['return_1w'] = (
            (sample_data['close'].iloc[-1] - sample_data['close'].iloc[-7]) /
            sample_data['close'].iloc[-7] * 100
        )
    if days >= 14:
        feature_dict['return_2w'] = (
            (sample_data['close'].iloc[-1] - sample_data['close'].iloc[-14]) /
            sample_data['close'].iloc[-14] * 100
        )
    
    # ===== å¸‚åœºå› å­ç‰¹å¾ =====
    if 'market_pct_chg' in sample_data.columns:
        market_data = sample_data['market_pct_chg'].dropna()
        if len(market_data) > 0:
            feature_dict['market_pct_chg_mean'] = market_data.mean()
    
    if 'market_return_34d' in sample_data.columns:
        market_return_data = sample_data['market_return_34d'].dropna()
        if len(market_return_data) > 0:
            feature_dict['market_return_34d_last'] = market_return_data.iloc[-1]
    
    if 'market_volatility_34d' in sample_data.columns:
        market_vol_data = sample_data['market_volatility_34d'].dropna()
        if len(market_vol_data) > 0:
            feature_dict['market_volatility_34d_last'] = market_vol_data.iloc[-1]
    
    if 'market_trend' in sample_data.columns:
        market_trend_data = sample_data['market_trend'].dropna()
        if len(market_trend_data) > 0:
            feature_dict['market_trend_last'] = market_trend_data.iloc[-1]
    
    if 'excess_return' in sample_data.columns:
        excess_data = sample_data['excess_return'].dropna()
        if len(excess_data) > 0:
            feature_dict['excess_return_mean'] = excess_data.mean()
            feature_dict['excess_return_sum'] = excess_data.sum()
            feature_dict['excess_return_positive_days'] = (excess_data > 0).sum()
    
    if 'excess_return_cumsum' in sample_data.columns:
        excess_cumsum_data = sample_data['excess_return_cumsum'].dropna()
        if len(excess_cumsum_data) > 0:
            feature_dict['excess_return_cumsum_last'] = excess_cumsum_data.iloc[-1]
    
    if 'price_vs_hist_mean' in sample_data.columns:
        hist_mean_data = sample_data['price_vs_hist_mean'].dropna()
        if len(hist_mean_data) > 0:
            feature_dict['price_vs_hist_mean_last'] = hist_mean_data.iloc[-1]
    
    # ===== æ–°æŠ€æœ¯å› å­ç‰¹å¾ï¼ˆfullï¼‰=====
    # æ¢æ‰‹ç‡
    if 'turnover_rate_f' in sample_data.columns:
        turnover_data = sample_data['turnover_rate_f'].dropna()
        if len(turnover_data) > 0:
            feature_dict['turnover_rate_f_mean'] = turnover_data.mean()
            feature_dict['turnover_rate_f_max'] = turnover_data.max()
            feature_dict['turnover_rate_f_std'] = turnover_data.std()
    
    # ä¹–ç¦»ç‡BIAS
    for bias_type in ['short', 'mid', 'long']:
        col = f'bias_{bias_type}'
        if col in sample_data.columns:
            bias_data = sample_data[col].dropna()
            if len(bias_data) > 0:
                feature_dict[f'{col}_last'] = bias_data.iloc[-1]
                if bias_type == 'short':
                    feature_dict[f'{col}_mean'] = bias_data.mean()
    
    # EMA
    if 'ema_5' in sample_data.columns and 'ema_20' in sample_data.columns:
        ema5 = sample_data['ema_5'].dropna()
        ema20 = sample_data['ema_20'].dropna()
        if len(ema5) > 0 and len(ema20) > 0:
            feature_dict['ema_ratio_5_20'] = ema5.iloc[-1] / ema20.iloc[-1] if ema20.iloc[-1] != 0 else 1
            if len(sample_data['close'].dropna()) > 0:
                close_last = sample_data['close'].dropna().iloc[-1]
                feature_dict['price_vs_ema5'] = (close_last - ema5.iloc[-1]) / ema5.iloc[-1] * 100 if ema5.iloc[-1] != 0 else 0
                feature_dict['price_vs_ema20'] = (close_last - ema20.iloc[-1]) / ema20.iloc[-1] * 100 if ema20.iloc[-1] != 0 else 0
    if 'ema_60' in sample_data.columns:
        ema60 = sample_data['ema_60'].dropna()
        if len(ema60) > 0 and len(sample_data['close'].dropna()) > 0:
            close_last = sample_data['close'].dropna().iloc[-1]
            feature_dict['price_vs_ema60'] = (close_last - ema60.iloc[-1]) / ema60.iloc[-1] * 100 if ema60.iloc[-1] != 0 else 0
    
    # KDJ
    for kdj_type in ['k', 'd', 'j']:
        col = f'kdj_{kdj_type}'
        if col in sample_data.columns:
            kdj_data = sample_data[col].dropna()
            if len(kdj_data) > 0:
                feature_dict[f'{col}_last'] = kdj_data.iloc[-1]
                if kdj_type == 'k':
                    feature_dict[f'{col}_mean'] = kdj_data.mean()
                if kdj_type == 'j':
                    feature_dict['kdj_j_overbought'] = (kdj_data > 80).sum()
                    feature_dict['kdj_j_oversold'] = (kdj_data < 20).sum()
    
    # æ¶¨åœç»Ÿè®¡
    if 'is_limit_up' in sample_data.columns:
        is_limit = sample_data['is_limit_up'].dropna()
        if len(is_limit) > 0:
            feature_dict['limit_up_count'] = is_limit.sum()
    
    # OBV
    if 'obv' in sample_data.columns:
        obv = sample_data['obv'].dropna()
        if len(obv) > 0:
            feature_dict['obv_change'] = (obv.iloc[-1] - obv.iloc[0]) / abs(obv.iloc[0]) * 100 if obv.iloc[0] != 0 else 0
            feature_dict['obv_trend'] = 1 if obv.iloc[-1] > obv.mean() else 0
    
    # æˆäº¤é‡ä¸å‡é‡æ¯”
    for vol_period in [5, 20]:
        col = f'vol_ma{vol_period}_ratio'
        if col in sample_data.columns:
            vol_r = sample_data[col].dropna()
            if len(vol_r) > 0:
                feature_dict[f'{col}_mean'] = vol_r.mean()
                feature_dict[f'{col}_max'] = vol_r.max()
    
    # ===== é«˜çº§æŠ€æœ¯å› å­ï¼ˆadvancedï¼‰=====
    # åŠ¨é‡å› å­
    for period in [5, 10, 20]:
        col = f'momentum_{period}d'
        if col in sample_data.columns:
            data = sample_data[col].dropna()
            if len(data) > 0:
                feature_dict[f'{col}_last'] = data.iloc[-1]
                feature_dict[f'{col}_mean'] = data.mean()
    
    if 'momentum_acceleration' in sample_data.columns:
        data = sample_data['momentum_acceleration'].dropna()
        if len(data) > 0:
            feature_dict['momentum_acceleration_last'] = data.iloc[-1]
    
    # é‡ä»·é…åˆåº¦
    if 'volume_price_corr_10d' in sample_data.columns:
        data = sample_data['volume_price_corr_10d'].dropna()
        if len(data) > 0:
            feature_dict['volume_price_corr_last'] = data.iloc[-1]
    if 'volume_price_match_sum_10d' in sample_data.columns:
        data = sample_data['volume_price_match_sum_10d'].dropna()
        if len(data) > 0:
            feature_dict['volume_price_match_sum'] = data.iloc[-1]
    
    # å¤šæ—¶é—´æ¡†æ¶ç‰¹å¾
    for tf in [8, 55]:
        for metric in ['return', 'price_vs_ma', 'volatility', 'price_position', 'trend_slope']:
            col = f'{metric}_{tf}d'
            if col in sample_data.columns:
                data = sample_data[col].dropna()
                if len(data) > 0:
                    feature_dict[f'{col}_last'] = data.iloc[-1]
    
    # çªç ´å½¢æ€
    for period in [10, 20, 55]:
        col = f'breakout_high_{period}d'
        if col in sample_data.columns:
            data = sample_data[col].dropna()
            if len(data) > 0:
                feature_dict[f'{col}_sum'] = data.sum()
    
    for ma in [5, 10, 20, 55]:
        col = f'breakout_ma{ma}'
        if col in sample_data.columns:
            data = sample_data[col].dropna()
            if len(data) > 0:
                feature_dict[f'{col}_sum'] = data.sum()
    
    if 'high_volume_breakout' in sample_data.columns:
        data = sample_data['high_volume_breakout'].dropna()
        if len(data) > 0:
            feature_dict['high_volume_breakout_sum'] = data.sum()
    
    if 'consecutive_new_high' in sample_data.columns:
        data = sample_data['consecutive_new_high'].dropna()
        if len(data) > 0:
            feature_dict['consecutive_new_high_max'] = data.max()
    
    # æ”¯æ’‘é˜»åŠ›
    for period in [10, 20]:
        for metric in ['dist_to_support', 'dist_to_resistance']:
            col = f'{metric}_{period}d'
            if col in sample_data.columns:
                data = sample_data[col].dropna()
                if len(data) > 0:
                    feature_dict[f'{col}_last'] = data.iloc[-1]
        
        for metric in ['support_strength', 'resistance_strength']:
            col = f'{metric}_{period}d'
            if col in sample_data.columns:
                data = sample_data[col].dropna()
                if len(data) > 0:
                    feature_dict[f'{col}_last'] = data.iloc[-1]
    
    if 'channel_width_20d' in sample_data.columns:
        data = sample_data['channel_width_20d'].dropna()
        if len(data) > 0:
            feature_dict['channel_width_last'] = data.iloc[-1]
    
    # é«˜çº§æˆäº¤é‡
    for col in ['volume_trend_slope_10d', 'volume_trend_slope_20d']:
        if col in sample_data.columns:
            data = sample_data[col].dropna()
            if len(data) > 0:
                feature_dict[f'{col}_last'] = data.iloc[-1]
    
    if 'volume_breakout_count_20d' in sample_data.columns:
        data = sample_data['volume_breakout_count_20d'].dropna()
        if len(data) > 0:
            feature_dict['volume_breakout_count'] = data.iloc[-1]
    
    if 'price_up_vol_down_count_10d' in sample_data.columns:
        data = sample_data['price_up_vol_down_count_10d'].dropna()
        if len(data) > 0:
            feature_dict['price_up_vol_down_count'] = data.iloc[-1]
    
    if 'price_down_vol_up_count_10d' in sample_data.columns:
        data = sample_data['price_down_vol_up_count_10d'].dropna()
        if len(data) > 0:
            feature_dict['price_down_vol_up_count'] = data.iloc[-1]
    
    if 'volume_rsv_20d' in sample_data.columns:
        data = sample_data['volume_rsv_20d'].dropna()
        if len(data) > 0:
            feature_dict['volume_rsv_last'] = data.iloc[-1]
    
    if 'obv_trend' in sample_data.columns:
        data = sample_data['obv_trend'].dropna()
        if len(data) > 0:
            feature_dict['obv_trend_sum'] = data.sum()
    
    return feature_dict


def get_valid_stocks(dm: DataManager, target_date: datetime) -> pd.DataFrame:
    """è·å–æœ‰æ•ˆè‚¡ç¥¨åˆ—è¡¨"""
    log.info("="*80)
    log.info("è·å–è‚¡ç¥¨åˆ—è¡¨")
    log.info("="*80)
    
    stock_list = dm.get_stock_list()
    log.info(f"âœ“ è·å–åˆ° {len(stock_list)} åªè‚¡ç¥¨")
    
    excluded = {'st': 0, 'new': 0, 'delisted': 0, 'bj': 0}
    valid_stocks = []
    
    for _, stock in stock_list.iterrows():
        ts_code = stock['ts_code']
        name = stock['name']
        
        # æ’é™¤ST
        if 'ST' in name or 'st' in name.lower() or '*' in name:
            excluded['st'] += 1
            continue
        
        # æ’é™¤é€€å¸‚
        if 'é€€' in name:
            excluded['delisted'] += 1
            continue
        
        # æ’é™¤åŒ—äº¤æ‰€
        if ts_code.endswith('.BJ'):
            excluded['bj'] += 1
            continue
        
        # æ£€æŸ¥ä¸Šå¸‚å¤©æ•°
        list_date = stock.get('list_date', '')
        if list_date:
            try:
                days = (target_date - pd.to_datetime(list_date)).days
                if days < 120:
                    excluded['new'] += 1
                    continue
            except:
                pass
        
        valid_stocks.append(stock)
    
    log.info(f"\nå‰”é™¤ç»Ÿè®¡: ST={excluded['st']}, æ¬¡æ–°={excluded['new']}, "
            f"é€€å¸‚={excluded['delisted']}, åŒ—äº¤æ‰€={excluded['bj']}")
    log.info(f"âœ“ ç¬¦åˆæ¡ä»¶: {len(valid_stocks)} åª")
    
    return pd.DataFrame(valid_stocks)


def score_single_stock(dm: DataManager, ts_code: str, name: str, 
                       target_date: datetime, feature_names: list,
                       df_market: pd.DataFrame = None,
                       lookback_days: int = 34, max_lookback: int = 90) -> dict:
    """
    å¯¹å•åªè‚¡ç¥¨è¿›è¡Œç‰¹å¾æå–å’Œè¯„åˆ†
    
    Args:
        df_market: ç¼“å­˜çš„å¸‚åœºæ•°æ®ï¼ˆé¿å…é‡å¤APIè°ƒç”¨ï¼‰
    """
    try:
        # è·å–æ—¥çº¿æ•°æ®ï¼ˆè·å–æ›´é•¿æ—¶é—´ä»¥è®¡ç®—é«˜çº§å› å­ï¼‰
        end_date = target_date.strftime('%Y%m%d')
        start_date = (target_date - timedelta(days=max_lookback)).strftime('%Y%m%d')
        
        df = dm.get_daily_data(ts_code, start_date, end_date)
        
        if df is None or len(df) < 20:
            return None
        
        df = df.sort_values('trade_date').reset_index(drop=True)
        
        # ç¡®ä¿æ•°å€¼åˆ—
        for col in ['close', 'pct_chg', 'vol', 'open', 'high', 'low']:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors='coerce')
        
        # è®¡ç®—åŸºç¡€æŠ€æœ¯æŒ‡æ ‡
        if 'ma5' not in df.columns:
            df['ma5'] = df['close'].rolling(5).mean()
        if 'ma10' not in df.columns:
            df['ma10'] = df['close'].rolling(10).mean()
        
        # é‡æ¯”
        if 'volume_ratio' not in df.columns:
            vol_ma5 = df['vol'].rolling(5).mean()
            df['volume_ratio'] = df['vol'] / vol_ma5
        
        # MACD
        if 'macd' not in df.columns:
            ema12 = df['close'].ewm(span=12, adjust=False).mean()
            ema26 = df['close'].ewm(span=26, adjust=False).mean()
            df['macd_dif'] = ema12 - ema26
            df['macd_dea'] = df['macd_dif'].ewm(span=9, adjust=False).mean()
            df['macd'] = (df['macd_dif'] - df['macd_dea']) * 2
        
        # RSI
        for period in [6, 12, 24]:
            col = f'rsi_{period}'
            if col not in df.columns:
                delta = df['close'].diff()
                gain = delta.where(delta > 0, 0).rolling(period).mean()
                loss = (-delta.where(delta < 0, 0)).rolling(period).mean()
                rs = gain / (loss + 1e-8)
                df[col] = 100 - (100 / (1 + rs))
        
        # KDJ
        if 'kdj_k' not in df.columns:
            n, m1, m2 = 9, 3, 3
            low_n = df['low'].rolling(n).min()
            high_n = df['high'].rolling(n).max()
            rsv = (df['close'] - low_n) / (high_n - low_n + 1e-8) * 100
            df['kdj_k'] = rsv.ewm(com=m1-1, adjust=False).mean()
            df['kdj_d'] = df['kdj_k'].ewm(com=m2-1, adjust=False).mean()
            df['kdj_j'] = 3 * df['kdj_k'] - 2 * df['kdj_d']
        
        # EMA
        for ema_period in [5, 10, 20, 60]:
            col = f'ema_{ema_period}'
            if col not in df.columns:
                df[col] = df['close'].ewm(span=ema_period, adjust=False).mean()
        
        # BIAS (ä¹–ç¦»ç‡)
        if 'bias_short' not in df.columns:
            df['bias_short'] = (df['close'] - df['close'].rolling(6).mean()) / df['close'].rolling(6).mean() * 100
        if 'bias_mid' not in df.columns:
            df['bias_mid'] = (df['close'] - df['close'].rolling(12).mean()) / df['close'].rolling(12).mean() * 100
        if 'bias_long' not in df.columns:
            df['bias_long'] = (df['close'] - df['close'].rolling(24).mean()) / df['close'].rolling(24).mean() * 100
        
        # OBV
        if 'obv' not in df.columns:
            df['obv'] = (np.sign(df['close'].diff()) * df['vol']).fillna(0).cumsum()
        
        # æˆäº¤é‡/å‡é‡æ¯”
        if 'vol_ma5_ratio' not in df.columns:
            vol_ma5 = df['vol'].rolling(5).mean()
            df['vol_ma5_ratio'] = df['vol'] / (vol_ma5 + 1e-8)
        if 'vol_ma20_ratio' not in df.columns:
            vol_ma20 = df['vol'].rolling(20).mean()
            df['vol_ma20_ratio'] = df['vol'] / (vol_ma20 + 1e-8)
        
        # æ¶¨åœåˆ¤æ–­
        if 'is_limit_up' not in df.columns:
            df['is_limit_up'] = (df['pct_chg'] >= 9.5).astype(int)
        
        # è®¡ç®—å¸‚åœºå› å­ï¼ˆä½¿ç”¨ç¼“å­˜çš„å¸‚åœºæ•°æ®ï¼‰
        df = calculate_market_factors(df, df_market)
        
        # è®¡ç®—é«˜çº§æŠ€æœ¯å› å­
        df = calculate_advanced_factors(df)
        
        # å–æœ€è¿‘34å¤©æ•°æ®
        df_sample = df.tail(lookback_days).copy()
        
        if len(df_sample) < 20:
            return None
        
        # æå–ç‰¹å¾
        features = extract_features_from_sample(df_sample, feature_names)
        
        if features is None:
            return None
        
        # æ·»åŠ å…ƒæ•°æ®
        features['ts_code'] = ts_code
        features['name'] = name
        features['latest_date'] = df_sample['trade_date'].iloc[-1]
        features['latest_close'] = df_sample['close'].iloc[-1]
        
        return features
        
    except Exception as e:
        return None


def score_all_stocks(dm: DataManager, booster: xgb.Booster, feature_names: list,
                    valid_stocks: pd.DataFrame, target_date: datetime,
                    max_stocks: int = None) -> pd.DataFrame:
    """å¯¹æ‰€æœ‰è‚¡ç¥¨è¿›è¡Œè¯„åˆ†"""
    log.info("="*80)
    log.info("å¼€å§‹è¯„åˆ†")
    log.info("="*80)
    
    if max_stocks:
        valid_stocks = valid_stocks.head(max_stocks)
        log.info(f"âš ï¸ æµ‹è¯•æ¨¡å¼ï¼šä»…è¯„åˆ†å‰ {max_stocks} åª")
    
    total = len(valid_stocks)
    features_list = []
    stock_info_list = []
    stats = {'success': 0, 'no_data': 0, 'error': 0}
    
    # ã€ä¼˜åŒ–ã€‘é¢„å…ˆè·å–å¹¶ç¼“å­˜å¸‚åœºæ•°æ®ï¼ˆåªè°ƒç”¨ä¸€æ¬¡APIï¼‰
    target_date_str = target_date.strftime('%Y%m%d')
    df_market = get_cached_market_data(dm, target_date_str, lookback_days=120)
    
    # æå–ç‰¹å¾
    for i, (_, stock) in enumerate(valid_stocks.iterrows()):
        if (i + 1) % 100 == 0 or i == 0 or (i + 1) == total:
            log.info(f"è¿›åº¦: {i+1}/{total} ({(i+1)/total*100:.1f}%)")
        
        ts_code = stock['ts_code']
        name = stock['name']
        
        features = score_single_stock(dm, ts_code, name, target_date, feature_names, df_market=df_market)
        
        if features is None:
            stats['no_data'] += 1
            continue
        
        features_list.append(features)
        stock_info_list.append({
            'ts_code': ts_code,
            'name': name,
            'features': features
        })
        stats['success'] += 1
    
    log.info(f"\nç‰¹å¾æå–: æˆåŠŸ={stats['success']}, æ— æ•°æ®={stats['no_data']}")
    
    if not features_list:
        log.error("æ²¡æœ‰æˆåŠŸæå–ç‰¹å¾çš„è‚¡ç¥¨")
        return pd.DataFrame()
    
    # æ‰¹é‡é¢„æµ‹
    log.info("æ‰¹é‡é¢„æµ‹...")
    feature_vectors = []
    for features in features_list:
        vector = []
        for name in feature_names:
            value = features.get(name, 0)
            if pd.isna(value):
                value = 0
            vector.append(value)
        feature_vectors.append(vector)
    
    dmatrix = xgb.DMatrix(feature_vectors, feature_names=feature_names)
    probabilities = booster.predict(dmatrix)
    
    # æ„å»ºç»“æœ
    results = []
    for i, info in enumerate(stock_info_list):
        features = info['features']
        results.append({
            'è‚¡ç¥¨ä»£ç ': info['ts_code'],
            'è‚¡ç¥¨åç§°': info['name'],
            'ç‰›è‚¡æ¦‚ç‡': float(probabilities[i]),
            'æ•°æ®æ—¥æœŸ': features.get('latest_date', ''),
            'æœ€æ–°ä»·æ ¼': features.get('latest_close', 0),
            '34æ—¥æ¶¨å¹…%': round(features.get('close_trend', 0), 2),
            'ç´¯è®¡æ¶¨è·Œ%': round(features.get('pct_chg_sum', 0), 2),
            '1å‘¨æ¶¨å¹…%': round(features.get('return_1w', 0), 2),
            '2å‘¨æ¶¨å¹…%': round(features.get('return_2w', 0), 2),
        })
    
    df_results = pd.DataFrame(results)
    df_results = df_results.sort_values('ç‰›è‚¡æ¦‚ç‡', ascending=False).reset_index(drop=True)
    
    log.success(f"âœ“ è¯„åˆ†å®Œæˆ: {len(df_results)} åªè‚¡ç¥¨")
    
    return df_results


def apply_risk_filter(df_scores: pd.DataFrame, 
                     max_34d_return: float = 50.0,
                     filter_mode: str = 'é™æƒ') -> pd.DataFrame:
    """
    å¯¹è¯„åˆ†ç»“æœåº”ç”¨é£é™©è¿‡æ»¤
    
    Args:
        df_scores: è¯„åˆ†ç»“æœDataFrame
        max_34d_return: 34æ—¥æ¶¨å¹…é˜ˆå€¼ï¼ˆè¶…è¿‡æ­¤å€¼çš„è‚¡ç¥¨ä¼šè¢«å¤„ç†ï¼‰
        filter_mode: å¤„ç†æ¨¡å¼
            - 'é™æƒ': é™ä½ç‰›è‚¡æ¦‚ç‡ï¼ˆæ¨èï¼Œä¿ç•™æ¨¡å‹è¯†åˆ«èƒ½åŠ›ï¼‰
            - 'æ ‡è®°': ä»…æ·»åŠ é£é™©æ ‡è®°ï¼Œä¸æ”¹å˜æ¦‚ç‡
            - 'è¿‡æ»¤': ç›´æ¥ç§»é™¤é«˜é£é™©è‚¡ç¥¨
    
    Returns:
        å¤„ç†åçš„DataFrame
    """
    df_filtered = df_scores.copy()
    
    # è¯†åˆ«é«˜é£é™©è‚¡ç¥¨
    high_risk_mask = df_filtered['34æ—¥æ¶¨å¹…%'] > max_34d_return
    high_risk_count = high_risk_mask.sum()
    
    if high_risk_count == 0:
        log.info(f"âœ“ æ— é«˜é£é™©è‚¡ç¥¨ï¼ˆ34æ—¥æ¶¨å¹…>{max_34d_return}%ï¼‰")
        return df_filtered
    
    log.warning(f"âš ï¸  å‘ç° {high_risk_count} åªé«˜é£é™©è‚¡ç¥¨ï¼ˆ34æ—¥æ¶¨å¹…>{max_34d_return}%ï¼‰")
    
    if filter_mode == 'é™æƒ':
        # é™æƒç­–ç•¥ï¼šæ ¹æ®æ¶¨å¹…è¶…é˜ˆå€¼ç¨‹åº¦é™ä½æ¦‚ç‡
        # ä¾‹å¦‚ï¼šæ¶¨å¹…60% â†’ é™æƒ20%ï¼Œæ¶¨å¹…80% â†’ é™æƒ40%
        def calculate_penalty(row):
            excess = row['34æ—¥æ¶¨å¹…%'] - max_34d_return
            # æ¯è¶…è¿‡10%é™æƒ5%ï¼Œæœ€å¤§é™æƒ50%
            penalty_rate = min(0.5, excess / 10 * 0.05)
            return row['ç‰›è‚¡æ¦‚ç‡'] * (1 - penalty_rate)
        
        df_filtered.loc[high_risk_mask, 'åŸå§‹æ¦‚ç‡'] = df_filtered.loc[high_risk_mask, 'ç‰›è‚¡æ¦‚ç‡']
        df_filtered.loc[high_risk_mask, 'ç‰›è‚¡æ¦‚ç‡'] = df_filtered.loc[high_risk_mask].apply(calculate_penalty, axis=1)
        df_filtered.loc[high_risk_mask, 'é£é™©æ ‡è®°'] = 'é«˜é£é™©-å·²é™æƒ'
        
        # é‡æ–°æ’åº
        df_filtered = df_filtered.sort_values('ç‰›è‚¡æ¦‚ç‡', ascending=False).reset_index(drop=True)
        
        log.info(f"âœ“ å·²å¯¹ {high_risk_count} åªè‚¡ç¥¨è¿›è¡Œé™æƒå¤„ç†")
        
        # æ˜¾ç¤ºé™æƒè¯¦æƒ…
        high_risk_stocks = df_filtered[df_filtered['é£é™©æ ‡è®°'] == 'é«˜é£é™©-å·²é™æƒ']
        if len(high_risk_stocks) > 0:
            log.info("\né™æƒè‚¡ç¥¨è¯¦æƒ…:")
            log.info(f"{'ä»£ç ':<12} {'åç§°':<10} {'åŸå§‹æ¦‚ç‡':<10} {'é™æƒå':<10} {'34æ—¥%':<8}")
            log.info("-" * 60)
            for _, row in high_risk_stocks.head(10).iterrows():
                original = row.get('åŸå§‹æ¦‚ç‡', row['ç‰›è‚¡æ¦‚ç‡'])
                log.info(f"{row['è‚¡ç¥¨ä»£ç ']:<12} {row['è‚¡ç¥¨åç§°']:<10} "
                        f"{original:<10.4f} {row['ç‰›è‚¡æ¦‚ç‡']:<10.4f} {row['34æ—¥æ¶¨å¹…%']:<8.2f}")
    
    elif filter_mode == 'æ ‡è®°':
        # ä»…æ·»åŠ æ ‡è®°ï¼Œä¸æ”¹å˜æ¦‚ç‡
        df_filtered.loc[high_risk_mask, 'é£é™©æ ‡è®°'] = 'é«˜é£é™©-è¿½é«˜'
        log.info(f"âœ“ å·²æ ‡è®° {high_risk_count} åªé«˜é£é™©è‚¡ç¥¨")
    
    elif filter_mode == 'è¿‡æ»¤':
        # ç›´æ¥ç§»é™¤
        df_filtered = df_filtered[~high_risk_mask].reset_index(drop=True)
        log.info(f"âœ“ å·²è¿‡æ»¤ {high_risk_count} åªé«˜é£é™©è‚¡ç¥¨ï¼Œå‰©ä½™ {len(df_filtered)} åª")
    
    return df_filtered


def save_results(df_scores: pd.DataFrame, df_top: pd.DataFrame, 
                target_date: datetime, model_info: dict, top_n: int = 50):
    """ä¿å­˜ç»“æœ"""
    date_str = target_date.strftime('%Y%m%d')
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    # è¾“å‡ºç›®å½•
    output_dir = PROJECT_ROOT / 'data' / 'prediction' / 'results'
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # å®Œæ•´è¯„åˆ†
    scores_file = output_dir / f"stock_scores_advanced_{date_str}.csv"
    df_scores.to_csv(scores_file, index=False, encoding='utf-8-sig')
    log.success(f"âœ“ å®Œæ•´è¯„åˆ†: {scores_file}")
    
    # Top N
    top_file = output_dir / f"top_{top_n}_advanced_{date_str}.csv"
    df_top.to_csv(top_file, index=False, encoding='utf-8-sig')
    log.success(f"âœ“ Top {top_n}: {top_file}")
    
    # å…ƒæ•°æ®
    metadata = {
        'prediction_date': date_str,
        'model': model_info,
        'total_scored': len(df_scores),
        'top_n': top_n,
        'created_at': datetime.now().isoformat(),
        'top_stocks': [
            {'rank': i+1, 'code': row['è‚¡ç¥¨ä»£ç '], 'name': row['è‚¡ç¥¨åç§°'],
             'probability': float(row['ç‰›è‚¡æ¦‚ç‡'])}
            for i, row in df_top.iterrows()
        ]
    }
    
    metadata_dir = PROJECT_ROOT / 'data' / 'prediction' / 'metadata'
    metadata_dir.mkdir(parents=True, exist_ok=True)
    metadata_file = metadata_dir / f"prediction_metadata_advanced_{date_str}.json"
    with open(metadata_file, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, indent=2, ensure_ascii=False)
    
    return scores_file, top_file


def main():
    parser = argparse.ArgumentParser(description='è‚¡ç¥¨è¯„åˆ†ï¼ˆé«˜çº§ç‰ˆï¼Œæ”¯æŒå…¨éƒ¨ç‰¹å¾ï¼‰')
    parser.add_argument('--date', '-d', default=None, help='ç›®æ ‡æ—¥æœŸï¼ˆYYYYMMDDæ ¼å¼ï¼Œå¦‚20251225ï¼‰')
    parser.add_argument('--max-stocks', type=int, default=None, help='æœ€å¤§è¯„åˆ†æ•°é‡ï¼ˆæµ‹è¯•ç”¨ï¼‰')
    parser.add_argument('--top-n', type=int, default=50, help='Top Næ¨èæ•°é‡')
    parser.add_argument('--risk-threshold', type=float, default=50.0, 
                       help='34æ—¥æ¶¨å¹…é£é™©é˜ˆå€¼ï¼ˆè¶…è¿‡æ­¤å€¼ä¼šè¢«å¤„ç†ï¼Œé»˜è®¤50.0ï¼‰')
    parser.add_argument('--risk-mode', choices=['é™æƒ', 'æ ‡è®°', 'è¿‡æ»¤'], default='é™æƒ',
                       help='é£é™©å¤„ç†æ¨¡å¼ï¼šé™æƒï¼ˆæ¨èï¼‰ã€æ ‡è®°ã€è¿‡æ»¤')
    parser.add_argument('--disable-risk-filter', action='store_true',
                       help='ç¦ç”¨é£é™©è¿‡æ»¤ï¼ˆä¿ç•™æ‰€æœ‰è‚¡ç¥¨ï¼‰')
    
    args = parser.parse_args()
    
    # è§£ææ—¥æœŸ
    if args.date:
        target_date = datetime.strptime(args.date, '%Y%m%d')
        log.info(f"ğŸ“… ç›®æ ‡æ—¥æœŸ: {target_date.strftime('%Yå¹´%mæœˆ%dæ—¥')} æ”¶ç›˜å")
    else:
        target_date = datetime.now()
        log.info(f"ğŸ“… ä½¿ç”¨å½“å‰æ—¥æœŸ: {target_date.strftime('%Yå¹´%mæœˆ%dæ—¥')}")
    
    log.info("="*80)
    log.info("è‚¡ç¥¨è¯„åˆ†ç³»ç»Ÿï¼ˆé«˜çº§ç‰ˆ - æ”¯æŒå¸‚åœºå› å­+é«˜çº§æŠ€æœ¯å› å­ï¼‰")
    log.info("="*80)
    
    try:
        # 1. åŠ è½½æ¨¡å‹
        booster, feature_names, model_info = load_model_and_features()
        
        # 2. åˆå§‹åŒ–æ•°æ®ç®¡ç†å™¨
        log.info("\nåˆå§‹åŒ–æ•°æ®ç®¡ç†å™¨...")
        dm = DataManager()
        log.success("âœ“ æ•°æ®ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
        
        # 3. è·å–è‚¡ç¥¨åˆ—è¡¨
        valid_stocks = get_valid_stocks(dm, target_date)
        
        # 4. è¯„åˆ†
        df_scores = score_all_stocks(
            dm, booster, feature_names, valid_stocks, 
            target_date, args.max_stocks
        )
        
        if df_scores.empty:
            log.error("è¯„åˆ†å¤±è´¥ï¼Œæ²¡æœ‰ç»“æœ")
            return
        
        # 5. åº”ç”¨é£é™©è¿‡æ»¤ï¼ˆå¯é€‰ï¼‰
        if not args.disable_risk_filter:
            log.info("\n" + "="*80)
            log.info("é£é™©è¿‡æ»¤")
            log.info("="*80)
            log.info(f"é£é™©é˜ˆå€¼: 34æ—¥æ¶¨å¹… > {args.risk_threshold}%")
            log.info(f"å¤„ç†æ¨¡å¼: {args.risk_mode}")
            df_scores = apply_risk_filter(
                df_scores, 
                max_34d_return=args.risk_threshold,
                filter_mode=args.risk_mode
            )
        else:
            log.info("\nâš ï¸  é£é™©è¿‡æ»¤å·²ç¦ç”¨")
        
        # 6. Top N
        df_top = df_scores.head(args.top_n)
        
        # 7. æ˜¾ç¤ºç»“æœ
        log.info("\n" + "="*80)
        log.info(f"Top {args.top_n} æ¨è")
        log.info("="*80)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰é£é™©æ ‡è®°åˆ—
        has_risk_marker = 'é£é™©æ ‡è®°' in df_top.columns
        
        if has_risk_marker:
            print(f"\n{'åºå·':<4} {'ä»£ç ':<12} {'åç§°':<10} {'æ¦‚ç‡':<8} {'æœ€æ–°ä»·':<8} {'34æ—¥%':<8} {'é£é™©':<10}")
            print("-" * 70)
            for i, row in df_top.iterrows():
                risk_marker = row.get('é£é™©æ ‡è®°', '')
                print(f"{i+1:<4} {row['è‚¡ç¥¨ä»£ç ']:<12} {row['è‚¡ç¥¨åç§°']:<10} "
                      f"{row['ç‰›è‚¡æ¦‚ç‡']:.4f} {row['æœ€æ–°ä»·æ ¼']:<8.2f} {row['34æ—¥æ¶¨å¹…%']:<8.2f} {risk_marker:<10}")
        else:
            print(f"\n{'åºå·':<4} {'ä»£ç ':<12} {'åç§°':<10} {'æ¦‚ç‡':<8} {'æœ€æ–°ä»·':<8} {'34æ—¥%':<8}")
            print("-" * 60)
            for i, row in df_top.iterrows():
                print(f"{i+1:<4} {row['è‚¡ç¥¨ä»£ç ']:<12} {row['è‚¡ç¥¨åç§°']:<10} "
                      f"{row['ç‰›è‚¡æ¦‚ç‡']:.4f} {row['æœ€æ–°ä»·æ ¼']:<8.2f} {row['34æ—¥æ¶¨å¹…%']:<8.2f}")
        
        # 8. ä¿å­˜ç»“æœ
        save_results(df_scores, df_top, target_date, model_info, args.top_n)
        
        log.success("\nâœ… è¯„åˆ†å®Œæˆï¼")
        
    except Exception as e:
        log.error(f"è¯„åˆ†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == '__main__':
    main()

