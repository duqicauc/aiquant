"""
XGBoostæ¨¡å‹è®­ç»ƒè„šæœ¬ - æ—¶é—´åºåˆ—ç‰ˆæœ¬ï¼ˆé¿å…æœªæ¥å‡½æ•°ï¼‰

å…³é”®æ”¹è¿›ï¼š
1. æŒ‰æ—¶é—´åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼ˆè€Œééšæœºåˆ’åˆ†ï¼‰
2. è®­ç»ƒé›†ï¼šå†å²æ•°æ®ï¼ˆå¦‚2022-2023å¹´ï¼‰
3. æµ‹è¯•é›†ï¼šæœªæ¥æ•°æ®ï¼ˆå¦‚2024å¹´ï¼‰
4. ç¡®ä¿ä¸ä¼šç”¨æœªæ¥ä¿¡æ¯è®­ç»ƒæ¨¡å‹
"""
import sys
import os
import warnings
import pandas as pd
import numpy as np
from datetime import datetime
import json

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

# å¿½ç•¥è­¦å‘Š
warnings.filterwarnings('ignore', category=FutureWarning)
warnings.filterwarnings('ignore', category=UserWarning)

from sklearn.metrics import (
    classification_report, confusion_matrix, 
    roc_auc_score, roc_curve, precision_recall_curve
)
import xgboost as xgb
from src.utils.logger import log
from src.utils.human_intervention import HumanInterventionChecker, require_human_confirmation
from src.visualization.training_visualizer import TrainingVisualizer


def load_and_prepare_data(neg_version='v2'):
    """
    åŠ è½½å¹¶å‡†å¤‡è®­ç»ƒæ•°æ®
    
    Args:
        neg_version: è´Ÿæ ·æœ¬ç‰ˆæœ¬ ('v1' æˆ– 'v2')
        
    Returns:
        df_features: ç‰¹å¾DataFrame
    """
    log.info("="*80)
    log.info("ç¬¬ä¸€æ­¥ï¼šåŠ è½½æ•°æ®")
    log.info("="*80)
    
    # åŠ è½½æ­£æ ·æœ¬ï¼ˆä½¿ç”¨æ–°çš„ç›®å½•ç»“æ„ï¼‰
    df_pos = pd.read_csv('data/training/features/feature_data_34d.csv')
    df_pos['label'] = 1
    log.success(f"âœ“ æ­£æ ·æœ¬åŠ è½½å®Œæˆ: {len(df_pos)} æ¡")
    
    # åŠ è½½è´Ÿæ ·æœ¬
    if neg_version == 'v2':
        neg_file = 'data/training/features/negative_feature_data_v2_34d.csv'
    else:
        neg_file = 'data/training/features/negative_feature_data_34d.csv'
    
    df_neg = pd.read_csv(neg_file)
    log.success(f"âœ“ è´Ÿæ ·æœ¬åŠ è½½å®Œæˆ: {len(df_neg)} æ¡ (ç‰ˆæœ¬: {neg_version})")
    
    # åˆå¹¶
    df = pd.concat([df_pos, df_neg])
    log.info(f"âœ“ æ•°æ®åˆå¹¶å®Œæˆ: {len(df)} æ¡")
    log.info(f"  - æ­£æ ·æœ¬: {len(df_pos)} æ¡")
    log.info(f"  - è´Ÿæ ·æœ¬: {len(df_neg)} æ¡")
    log.info("")
    
    return df


def extract_features_with_time(df):
    """
    ä»34å¤©çš„æ—¶åºæ•°æ®ä¸­æå–ç»Ÿè®¡ç‰¹å¾ï¼ˆä¿ç•™æ—¶é—´ä¿¡æ¯ï¼‰
    
    Args:
        df: åŸå§‹DataFrameï¼ˆæ¯è¡Œæ˜¯ä¸€å¤©çš„æ•°æ®ï¼‰
        
    Returns:
        df_features: ç‰¹å¾DataFrameï¼ˆæ¯è¡Œæ˜¯ä¸€ä¸ªæ ·æœ¬ï¼ŒåŒ…å«T1æ—¥æœŸï¼‰
    """
    log.info("="*80)
    log.info("ç¬¬äºŒæ­¥ï¼šç‰¹å¾å·¥ç¨‹ï¼ˆä¿ç•™æ—¶é—´ä¿¡æ¯ï¼‰")
    log.info("="*80)
    log.info("å°†34å¤©æ—¶åºæ•°æ®è½¬æ¢ä¸ºç»Ÿè®¡ç‰¹å¾...")
    
    # é‡æ–°åˆ†é…å”¯ä¸€çš„sample_id
    df['unique_sample_id'] = df.groupby(['ts_code', 'label']).ngroup()
    
    features = []
    sample_ids = df['unique_sample_id'].unique()
    
    # è·å–æ­£æ ·æœ¬çš„T1æ—¥æœŸæ˜ å°„ï¼ˆä½¿ç”¨æ–°çš„ç›®å½•ç»“æ„ï¼‰
    df_positive_samples = pd.read_csv('data/training/samples/positive_samples.csv')
    t1_date_map = dict(zip(
        df_positive_samples.index,
        pd.to_datetime(df_positive_samples['t1_date'])
    ))
    
    # è·å–è´Ÿæ ·æœ¬çš„T1æ—¥æœŸæ˜ å°„
    if os.path.exists('data/training/samples/negative_samples_v2.csv'):
        df_negative_samples = pd.read_csv('data/training/samples/negative_samples_v2.csv')
    else:
        df_negative_samples = pd.read_csv('data/training/samples/negative_samples.csv')
    
    # è´Ÿæ ·æœ¬çš„sample_idéœ€è¦åç§»ï¼ˆå› ä¸ºæ˜¯ä»0å¼€å§‹çš„ï¼‰
    max_positive_id = df_positive_samples.index.max()
    for idx, row in df_negative_samples.iterrows():
        t1_date_map[max_positive_id + 1 + idx] = pd.to_datetime(row['t1_date'])
    
    for i, sample_id in enumerate(sample_ids):
        if (i + 1) % 500 == 0:
            log.info(f"è¿›åº¦: {i+1}/{len(sample_ids)}")
        
        sample_data = df[df['unique_sample_id'] == sample_id].sort_values('days_to_t1')
        
        if len(sample_data) < 20:  # è‡³å°‘20å¤©æ•°æ®
            continue
        
        # ä»æ•°æ®ä¸­è·å–T1æ—¥æœŸï¼ˆåŸºäºdays_to_t1=0çš„é‚£ä¸€å¤©ï¼‰
        # æ‰¾åˆ° days_to_t1 æœ€æ¥è¿‘0çš„è®°å½•
        t1_row = sample_data.iloc[sample_data['days_to_t1'].abs().argmin()]
        t1_date = pd.to_datetime(t1_row['trade_date'])
        
        feature_dict = {
            'sample_id': sample_id,
            'ts_code': sample_data['ts_code'].iloc[0],
            'name': sample_data['name'].iloc[0],
            'label': int(sample_data['label'].iloc[0]),
            't1_date': t1_date,  # ä¿ç•™T1æ—¥æœŸï¼Œç”¨äºæ—¶é—´åˆ’åˆ†
        }
        
        # ä»·æ ¼ç‰¹å¾
        feature_dict['close_mean'] = sample_data['close'].mean()
        feature_dict['close_std'] = sample_data['close'].std()
        feature_dict['close_max'] = sample_data['close'].max()
        feature_dict['close_min'] = sample_data['close'].min()
        feature_dict['close_trend'] = (
            (sample_data['close'].iloc[-1] - sample_data['close'].iloc[0]) / 
            sample_data['close'].iloc[0] * 100
        )
        
        # æ¶¨è·Œå¹…ç‰¹å¾
        feature_dict['pct_chg_mean'] = sample_data['pct_chg'].mean()
        feature_dict['pct_chg_std'] = sample_data['pct_chg'].std()
        feature_dict['pct_chg_sum'] = sample_data['pct_chg'].sum()
        feature_dict['positive_days'] = (sample_data['pct_chg'] > 0).sum()
        feature_dict['negative_days'] = (sample_data['pct_chg'] < 0).sum()
        feature_dict['max_gain'] = sample_data['pct_chg'].max()
        feature_dict['max_loss'] = sample_data['pct_chg'].min()
        
        # é‡æ¯”ç‰¹å¾
        if 'volume_ratio' in sample_data.columns:
            feature_dict['volume_ratio_mean'] = sample_data['volume_ratio'].mean()
            feature_dict['volume_ratio_max'] = sample_data['volume_ratio'].max()
            feature_dict['volume_ratio_gt_2'] = (sample_data['volume_ratio'] > 2).sum()
            feature_dict['volume_ratio_gt_4'] = (sample_data['volume_ratio'] > 4).sum()
        
        # MACDç‰¹å¾
        if 'macd' in sample_data.columns:
            macd_data = sample_data['macd'].dropna()
            if len(macd_data) > 0:
                feature_dict['macd_mean'] = macd_data.mean()
                feature_dict['macd_positive_days'] = (macd_data > 0).sum()
                feature_dict['macd_max'] = macd_data.max()
        
        # MAç‰¹å¾
        if 'ma5' in sample_data.columns:
            feature_dict['ma5_mean'] = sample_data['ma5'].mean()
            feature_dict['price_above_ma5'] = (
                sample_data['close'] > sample_data['ma5']
            ).sum()
        
        if 'ma10' in sample_data.columns:
            feature_dict['ma10_mean'] = sample_data['ma10'].mean()
            feature_dict['price_above_ma10'] = (
                sample_data['close'] > sample_data['ma10']
            ).sum()
        
        # å¸‚å€¼ç‰¹å¾
        if 'total_mv' in sample_data.columns:
            mv_data = sample_data['total_mv'].dropna()
            if len(mv_data) > 0:
                feature_dict['total_mv_mean'] = mv_data.mean()
        
        if 'circ_mv' in sample_data.columns:
            circ_mv_data = sample_data['circ_mv'].dropna()
            if len(circ_mv_data) > 0:
                feature_dict['circ_mv_mean'] = circ_mv_data.mean()
        
        # åŠ¨é‡ç‰¹å¾ï¼ˆåˆ†æ®µæ”¶ç›Šç‡ï¼‰
        days = len(sample_data)
        if days >= 7:
            feature_dict['return_1w'] = (
                (sample_data['close'].iloc[-1] - sample_data['close'].iloc[-7]) /
                sample_data['close'].iloc[-7] * 100
            )
        if days >= 14:
            feature_dict['return_2w'] = (
                (sample_data['close'].iloc[-1] - sample_data['close'].iloc[-14]) /
                sample_data['close'].iloc[-14] * 100
            )
        
        features.append(feature_dict)
    
    df_features = pd.DataFrame(features)
    
    log.success(f"âœ“ ç‰¹å¾æå–å®Œæˆ: {len(df_features)} ä¸ªæ ·æœ¬")
    log.info(f"âœ“ ç‰¹å¾ç»´åº¦: {len(df_features.columns) - 3} ä¸ªç‰¹å¾ï¼ˆä¸å«sample_id, label, t1_dateï¼‰")
    log.info("")
    
    return df_features


def timeseries_split(df_features, train_end_date=None, test_start_date=None):
    """
    æŒ‰æ—¶é—´åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼ˆé¿å…æœªæ¥å‡½æ•°ï¼‰
    
    Args:
        df_features: ç‰¹å¾DataFrameï¼ˆå¿…é¡»åŒ…å«t1_dateåˆ—ï¼‰
        train_end_date: è®­ç»ƒé›†æˆªæ­¢æ—¥æœŸï¼ˆå¦‚'2023-12-31'ï¼‰
        test_start_date: æµ‹è¯•é›†å¼€å§‹æ—¥æœŸï¼ˆå¦‚'2024-01-01'ï¼‰
        
    Returns:
        X_train, X_test, y_train, y_test, train_dates, test_dates
    """
    log.info("="*80)
    log.info("ç¬¬ä¸‰æ­¥ï¼šæ—¶é—´åºåˆ—åˆ’åˆ†ï¼ˆé¿å…æœªæ¥å‡½æ•°ï¼‰")
    log.info("="*80)
    
    # ç¡®ä¿t1_dateæ˜¯datetimeç±»å‹
    df_features['t1_date'] = pd.to_datetime(df_features['t1_date'])
    
    # æŒ‰æ—¶é—´æ’åº
    df_features = df_features.sort_values('t1_date').reset_index(drop=True)
    
    # æ˜¾ç¤ºæ—¶é—´èŒƒå›´
    min_date = df_features['t1_date'].min()
    max_date = df_features['t1_date'].max()
    log.info(f"æ•°æ®æ—¶é—´èŒƒå›´: {min_date.date()} è‡³ {max_date.date()}")
    
    # å¦‚æœæœªæŒ‡å®šåˆ’åˆ†ç‚¹ï¼Œä½¿ç”¨80%ä½œä¸ºè®­ç»ƒé›†
    if train_end_date is None:
        n_train = int(len(df_features) * 0.8)
        train_end_date = df_features.iloc[n_train]['t1_date']
        test_start_date = df_features.iloc[n_train + 1]['t1_date']
    else:
        train_end_date = pd.to_datetime(train_end_date)
        test_start_date = pd.to_datetime(test_start_date)
    
    # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
    train_mask = df_features['t1_date'] <= train_end_date
    test_mask = df_features['t1_date'] >= test_start_date
    
    df_train = df_features[train_mask]
    df_test = df_features[test_mask]
    
    log.info(f"\næ—¶é—´åˆ’åˆ†:")
    log.info(f"  è®­ç»ƒé›†: {df_train['t1_date'].min().date()} è‡³ {df_train['t1_date'].max().date()}")
    log.info(f"  æµ‹è¯•é›†: {df_test['t1_date'].min().date()} è‡³ {df_test['t1_date'].max().date()}")
    log.info(f"\næ ·æœ¬åˆ’åˆ†:")
    log.info(f"  è®­ç»ƒé›†: {len(df_train)} ä¸ªæ ·æœ¬ (æ­£:{(df_train['label']==1).sum()}, è´Ÿ:{(df_train['label']==0).sum()})")
    log.info(f"  æµ‹è¯•é›†: {len(df_test)} ä¸ªæ ·æœ¬ (æ­£:{(df_test['label']==1).sum()}, è´Ÿ:{(df_test['label']==0).sum()})")
    log.info("")
    
    # ç¡®è®¤æ— æ•°æ®æ³„éœ²
    if df_train['t1_date'].max() >= df_test['t1_date'].min():
        log.warning("âš ï¸  è­¦å‘Šï¼šè®­ç»ƒé›†å’Œæµ‹è¯•é›†æ—¶é—´æœ‰é‡å ï¼Œå¯èƒ½å­˜åœ¨æ•°æ®æ³„éœ²ï¼")
    else:
        log.success("âœ“ è®­ç»ƒé›†å’Œæµ‹è¯•é›†æ—¶é—´æ— é‡å ï¼Œæ— æ•°æ®æ³„éœ²é£é™©")
    
    # å‡†å¤‡ç‰¹å¾å’Œæ ‡ç­¾
    feature_cols = [col for col in df_features.columns 
                   if col not in ['sample_id', 'label', 't1_date']]
    
    X_train = df_train[feature_cols]
    y_train = df_train['label']
    train_dates = df_train['t1_date']
    
    X_test = df_test[feature_cols]
    y_test = df_test['label']
    test_dates = df_test['t1_date']
    
    # å¤„ç†ç¼ºå¤±å€¼
    X_train = X_train.fillna(0)
    X_test = X_test.fillna(0)
    
    # åˆ é™¤éæ•°å€¼åˆ—
    non_numeric_cols = X_train.select_dtypes(include=['object']).columns
    if len(non_numeric_cols) > 0:
        log.info(f"åˆ é™¤éæ•°å€¼åˆ—: {list(non_numeric_cols)}")
        X_train = X_train.drop(columns=non_numeric_cols)
        X_test = X_test.drop(columns=non_numeric_cols)
    
    log.info(f"ç‰¹å¾çŸ©é˜µ:")
    log.info(f"  è®­ç»ƒé›†: {X_train.shape}")
    log.info(f"  æµ‹è¯•é›†: {X_test.shape}")
    log.info("")
    
    return X_train, X_test, y_train, y_test, train_dates, test_dates


def train_model(X_train, y_train, X_test, y_test):
    """
    è®­ç»ƒXGBoostæ¨¡å‹
    
    Args:
        X_train, y_train: è®­ç»ƒé›†
        X_test, y_test: æµ‹è¯•é›†
        
    Returns:
        model, metrics
    """
    log.info("="*80)
    log.info("ç¬¬å››æ­¥ï¼šè®­ç»ƒXGBoostæ¨¡å‹")
    log.info("="*80)
    
    # è®­ç»ƒæ¨¡å‹
    log.info("å¼€å§‹è®­ç»ƒ...")
    model = xgb.XGBClassifier(
        n_estimators=100,
        max_depth=5,
        learning_rate=0.1,
        subsample=0.8,
        colsample_bytree=0.8,
        min_child_weight=3,
        gamma=0.1,
        reg_alpha=0.1,
        reg_lambda=1.0,
        random_state=42,
        eval_metric='logloss'
    )
    
    model.fit(
        X_train, y_train,
        eval_set=[(X_test, y_test)],
        verbose=False
    )
    
    log.success("âœ“ æ¨¡å‹è®­ç»ƒå®Œæˆï¼")
    log.info("")
    
    # é¢„æµ‹
    y_pred = model.predict(X_test)
    y_prob = model.predict_proba(X_test)[:, 1]
    
    # è¯„ä¼°
    log.info("="*80)
    log.info("ç¬¬äº”æ­¥ï¼šæ¨¡å‹è¯„ä¼°ï¼ˆæµ‹è¯•é›† = æœªæ¥æ•°æ®ï¼‰")
    log.info("="*80)
    
    # åˆ†ç±»æŠ¥å‘Š
    log.info("\nåˆ†ç±»æŠ¥å‘Š:")
    report = classification_report(
        y_test, y_pred, 
        target_names=['è´Ÿæ ·æœ¬', 'æ­£æ ·æœ¬'],
        output_dict=True
    )
    print(classification_report(
        y_test, y_pred, 
        target_names=['è´Ÿæ ·æœ¬', 'æ­£æ ·æœ¬']
    ))
    
    # AUC
    auc = roc_auc_score(y_test, y_prob)
    log.info(f"\nAUC-ROC: {auc:.4f}")
    
    # æ··æ·†çŸ©é˜µ
    cm = confusion_matrix(y_test, y_pred)
    log.info("\næ··æ·†çŸ©é˜µ:")
    log.info(f"  çœŸè´Ÿä¾‹(TN): {cm[0,0]:4d}  |  å‡æ­£ä¾‹(FP): {cm[0,1]:4d}")
    log.info(f"  å‡è´Ÿä¾‹(FN): {cm[1,0]:4d}  |  çœŸæ­£ä¾‹(TP): {cm[1,1]:4d}")
    
    # ç‰¹å¾é‡è¦æ€§
    feature_importance = pd.DataFrame({
        'feature': X_train.columns,
        'importance': model.feature_importances_
    }).sort_values('importance', ascending=False)
    
    log.info("\n" + "="*80)
    log.info("ç‰¹å¾é‡è¦æ€§ Top 10:")
    log.info("="*80)
    for idx, row in feature_importance.head(10).iterrows():
        log.info(f"  {row['feature']:25s}: {row['importance']:.4f}")
    
    # æ±‡æ€»æŒ‡æ ‡
    metrics = {
        'accuracy': report['accuracy'],
        'precision': report['æ­£æ ·æœ¬']['precision'],
        'recall': report['æ­£æ ·æœ¬']['recall'],
        'f1_score': report['æ­£æ ·æœ¬']['f1-score'],
        'auc': auc,
        'confusion_matrix': cm.tolist(),
        'feature_importance': feature_importance.to_dict('records')
    }
    
    return model, metrics, y_prob


def generate_training_visualizations(model, X_train, df_features, train_dates, test_dates, neg_version):
    """ç”Ÿæˆè®­ç»ƒè¿‡ç¨‹å¯è§†åŒ–å›¾è¡¨"""
    try:
        log.info("="*80)
        log.info("ç”Ÿæˆè®­ç»ƒå¯è§†åŒ–å›¾è¡¨")
        log.info("="*80)
        
        visualizer = TrainingVisualizer(
            output_dir=f"data/training/charts"
        )
        
        # 1. æ ·æœ¬è´¨é‡å¯è§†åŒ–ï¼ˆæ­£æ ·æœ¬ï¼‰
        try:
            df_positive_samples = pd.read_csv('data/training/samples/positive_samples.csv')
            visualizer.visualize_sample_quality(
                df_positive_samples,
                save_prefix="positive_sample_quality"
            )
        except Exception as e:
            log.warning(f"ç”Ÿæˆæ­£æ ·æœ¬è´¨é‡å¯è§†åŒ–æ—¶å‡ºé”™: {e}")
        
        # è´Ÿæ ·æœ¬
        try:
            if neg_version == 'v2':
                neg_file = 'data/training/samples/negative_samples_v2.csv'
            else:
                neg_file = 'data/training/samples/negative_samples.csv'
            
            if os.path.exists(neg_file):
                df_negative_samples = pd.read_csv(neg_file)
                visualizer.visualize_sample_quality(
                    df_negative_samples,
                    save_prefix="negative_sample_quality"
                )
        except Exception as e:
            log.warning(f"ç”Ÿæˆè´Ÿæ ·æœ¬è´¨é‡å¯è§†åŒ–æ—¶å‡ºé”™: {e}")
        
        # 2. å› å­é‡è¦æ€§å¯è§†åŒ–
        feature_importance = pd.DataFrame({
            'feature': X_train.columns,
            'importance': model.feature_importances_
        })
        
        visualizer.visualize_feature_importance(
            feature_importance,
            model_name=f"xgboost_timeseries_{neg_version}",
            top_n=20
        )
        
        # 3. ç”Ÿæˆç´¢å¼•é¡µé¢
        visualizer.generate_index_page(model_name=f"xgboost_timeseries_{neg_version}")
        
        log.success("âœ“ å¯è§†åŒ–å›¾è¡¨ç”Ÿæˆå®Œæˆ")
        log.info(f"ğŸ“Š æŸ¥çœ‹å›¾è¡¨: open data/training/charts/index.html")
        
    except Exception as e:
        log.warning(f"ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨æ—¶å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()


def save_model(model, metrics, neg_version, train_dates, test_dates):
    """ä¿å­˜æ¨¡å‹å’Œç»“æœ"""
    log.info("\n" + "="*80)
    log.info("ç¬¬å…­æ­¥ï¼šä¿å­˜æ¨¡å‹")
    log.info("="*80)
    
    # åˆ›å»ºç›®å½•ï¼ˆä½¿ç”¨æ–°çš„ç›®å½•ç»“æ„ï¼‰
    os.makedirs('data/training/models', exist_ok=True)
    os.makedirs('data/training/metrics', exist_ok=True)
    
    # ä¿å­˜æ¨¡å‹ï¼ˆä½¿ç”¨boosteræ–¹æ³•é¿å…sklearn mixiné—®é¢˜ï¼‰
    model_file = f'data/training/models/xgboost_timeseries_{neg_version}_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
    model.get_booster().save_model(model_file)
    log.success(f"âœ“ æ¨¡å‹å·²ä¿å­˜: {model_file}")
    
    # ä¿å­˜æŒ‡æ ‡
    metrics_file = f'data/training/metrics/xgboost_timeseries_{neg_version}_metrics.json'
    metrics['model_file'] = model_file
    metrics['timestamp'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    metrics['neg_version'] = neg_version
    metrics['train_date_range'] = f"{train_dates.min().date()} to {train_dates.max().date()}"
    metrics['test_date_range'] = f"{test_dates.min().date()} to {test_dates.max().date()}"
    metrics['note'] = 'ä½¿ç”¨æ—¶é—´åºåˆ—åˆ’åˆ†ï¼Œé¿å…æœªæ¥å‡½æ•°'
    
    with open(metrics_file, 'w', encoding='utf-8') as f:
        json.dump(metrics, f, indent=2, ensure_ascii=False)
    
    log.success(f"âœ“ è¯„ä¼°æŠ¥å‘Šå·²ä¿å­˜: {metrics_file}")
    log.info("")


def main():
    """ä¸»å‡½æ•°"""
    log.info("="*80)
    log.info("XGBoost è‚¡ç¥¨é€‰è‚¡æ¨¡å‹è®­ç»ƒ - æ—¶é—´åºåˆ—ç‰ˆæœ¬")
    log.info("="*80)
    log.info("")
    log.info("âš ï¸  é‡è¦æ”¹è¿›ï¼š")
    log.info("  1. æŒ‰æ—¶é—´åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼ˆè€Œééšæœºåˆ’åˆ†ï¼‰")
    log.info("  2. è®­ç»ƒé›† = å†å²æ•°æ®ï¼Œæµ‹è¯•é›† = æœªæ¥æ•°æ®")
    log.info("  3. é¿å…æœªæ¥å‡½æ•°ï¼Œç¡®ä¿æ— æ•°æ®æ³„éœ²")
    log.info("")
    
    # é€‰æ‹©è´Ÿæ ·æœ¬ç‰ˆæœ¬
    NEG_VERSION = 'v2'  # 'v1' æˆ– 'v2'
    
    log.info(f"é…ç½®:")
    log.info(f"  è´Ÿæ ·æœ¬ç‰ˆæœ¬: {NEG_VERSION}")
    log.info(f"  åˆ’åˆ†æ–¹å¼: æ—¶é—´åºåˆ—åˆ’åˆ†ï¼ˆ80%è®­ç»ƒï¼Œ20%æµ‹è¯•ï¼‰")
    log.info(f"  æ¨¡å‹: XGBoost")
    log.info("")
    
    try:
        # ğŸ‘¤ äººå·¥ä»‹å…¥æ£€æŸ¥ï¼šç‰¹å¾é€‰æ‹©
        checker = HumanInterventionChecker()
        feature_check = checker.check_feature_selection()
        checker.print_intervention_reminder("ç‰¹å¾é€‰æ‹©", feature_check)
        
        # 1. åŠ è½½æ•°æ®
        df = load_and_prepare_data(neg_version=NEG_VERSION)
        
        # 2. ç‰¹å¾å·¥ç¨‹ï¼ˆä¿ç•™æ—¶é—´ä¿¡æ¯ï¼‰
        df_features = extract_features_with_time(df)
        
        # ğŸ‘¤ äººå·¥ä»‹å…¥æé†’ï¼šç‰¹å¾æå–å®Œæˆ
        log.warning("\n" + "="*80)
        log.warning("ğŸ‘¤ äººå·¥ä»‹å…¥æé†’ï¼šç‰¹å¾æå–å®Œæˆ")
        log.warning("="*80)
        log.warning(f"å½“å‰ç‰¹å¾æ•°é‡: {len(df_features.columns) - 3} ä¸ªï¼ˆä¸å«sample_id, label, t1_dateï¼‰")
        log.warning("è¯·ç¡®è®¤ï¼š")
        log.warning("  1. ç‰¹å¾æ˜¯å¦è¶³å¤Ÿï¼Ÿæ˜¯å¦éœ€è¦æ·»åŠ åŸºæœ¬é¢ç‰¹å¾æˆ–å…¶ä»–æŠ€æœ¯æŒ‡æ ‡ï¼Ÿ")
        log.warning("  2. ç‰¹å¾æ˜¯å¦é¿å…äº†æœªæ¥å‡½æ•°ï¼Ÿ")
        log.warning("  3. ç‰¹å¾é‡è¦æ€§å°†åœ¨è®­ç»ƒåæ˜¾ç¤ºï¼Œè¯·å…³æ³¨")
        log.warning("="*80)
        
        # 3. æ—¶é—´åºåˆ—åˆ’åˆ†
        X_train, X_test, y_train, y_test, train_dates, test_dates = timeseries_split(
            df_features
        )
        
        # 4. è®­ç»ƒæ¨¡å‹
        model, metrics, y_prob = train_model(X_train, y_train, X_test, y_test)
        
        # 4.5. ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
        generate_training_visualizations(
            model, X_train, df_features, train_dates, test_dates, NEG_VERSION
        )
        
        # ğŸ‘¤ äººå·¥ä»‹å…¥æ£€æŸ¥ï¼šè®­ç»ƒç»“æœ
        log.warning("\n" + "="*80)
        log.warning("ğŸ‘¤ äººå·¥ä»‹å…¥æ£€æŸ¥ï¼šè®­ç»ƒç»“æœ")
        log.warning("="*80)
        
        # æ£€æŸ¥æŒ‡æ ‡æ˜¯å¦è¾¾æ ‡
        warnings = []
        if metrics['auc'] < 0.7:
            warnings.append(f"âš ï¸  AUC = {metrics['auc']:.3f} < 0.7ï¼Œæ¨¡å‹æ€§èƒ½å¯èƒ½ä¸ä½³")
        if metrics['accuracy'] < 0.75:
            warnings.append(f"âš ï¸  å‡†ç¡®ç‡ = {metrics['accuracy']:.2%} < 75%ï¼Œæ¨¡å‹æ€§èƒ½å¯èƒ½ä¸ä½³")
        if metrics['f1_score'] < 0.7:
            warnings.append(f"âš ï¸  F1åˆ†æ•° = {metrics['f1_score']:.2%} < 70%ï¼Œå¯èƒ½å­˜åœ¨è¿‡æ‹Ÿåˆæˆ–æ¬ æ‹Ÿåˆ")
        
        if warnings:
            for warning in warnings:
                log.warning(warning)
            log.warning("\nå»ºè®®ï¼š")
            log.warning("  - æ£€æŸ¥ç‰¹å¾é€‰æ‹©ï¼Œè€ƒè™‘æ·»åŠ æ›´å¤šæœ‰æ•ˆç‰¹å¾")
            log.warning("  - è°ƒæ•´è¶…å‚æ•°ï¼ˆn_estimators, max_depth, learning_rateç­‰ï¼‰")
            log.warning("  - æ£€æŸ¥æ•°æ®è´¨é‡ï¼Œç¡®ä¿æ­£è´Ÿæ ·æœ¬è´¨é‡")
            log.warning("  - è€ƒè™‘å°è¯•å…¶ä»–ç®—æ³•ï¼ˆLightGBM, CatBoostç­‰ï¼‰")
        else:
            log.success("âœ“ æ¨¡å‹æ€§èƒ½æŒ‡æ ‡æ­£å¸¸")
        log.warning("="*80)
        
        # 5. ä¿å­˜æ¨¡å‹
        save_model(model, metrics, NEG_VERSION, train_dates, test_dates)
        
        # 6. æœ€ç»ˆæ€»ç»“
        log.info("="*80)
        log.success("âœ… æ¨¡å‹è®­ç»ƒå®Œæˆï¼ï¼ˆæ—¶é—´åºåˆ—ç‰ˆæœ¬ï¼‰")
        log.info("="*80)
        log.info("")
        log.info("ğŸ“Š æ¨¡å‹æ€§èƒ½æ€»ç»“:")
        log.info(f"  å‡†ç¡®ç‡ (Accuracy):  {metrics['accuracy']:.2%}")
        log.info(f"  ç²¾ç¡®ç‡ (Precision): {metrics['precision']:.2%}")
        log.info(f"  å¬å›ç‡ (Recall):    {metrics['recall']:.2%}")
        log.info(f"  F1åˆ†æ•° (F1-Score):  {metrics['f1_score']:.2%}")
        log.info(f"  AUC-ROC:            {metrics['auc']:.4f}")
        log.info("")
        log.info("ğŸ¯ å…³é”®æ”¹è¿›:")
        log.info("  âœ“ è®­ç»ƒé›† = å†å²æ•°æ®")
        log.info("  âœ“ æµ‹è¯•é›† = æœªæ¥æ•°æ®ï¼ˆæ¨¡æ‹ŸçœŸå®åœºæ™¯ï¼‰")
        log.info("  âœ“ æ— æœªæ¥å‡½æ•°é£é™©")
        log.info("  âœ“ æ— æ•°æ®æ³„éœ²")
        log.info("")
        log.info("ä¸‹ä¸€æ­¥:")
        log.info("  1. ä½¿ç”¨walk-forwardéªŒè¯è¿›ä¸€æ­¥æµ‹è¯•")
        log.info("  2. åœ¨å¤šä¸ªæ—¶é—´çª—å£ä¸ŠéªŒè¯ç¨³å®šæ€§")
        log.info("  3. å›æµ‹éªŒè¯å®é™…æ”¶ç›Š")
        log.info("")
        
    except FileNotFoundError as e:
        log.error(f"âœ— æ–‡ä»¶æœªæ‰¾åˆ°: {e}")
        log.error("è¯·å…ˆè¿è¡Œä»¥ä¸‹å‘½ä»¤å‡†å¤‡æ•°æ®:")
        log.error("  1. python scripts/prepare_positive_samples.py")
        log.error("  2. python scripts/prepare_negative_samples_v2.py")
    except Exception as e:
        log.error(f"âœ— è®­ç»ƒè¿‡ç¨‹å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()


if __name__ == '__main__':
    main()

