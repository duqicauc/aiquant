"""
XGBoostæ¨¡å‹è®­ç»ƒè„šæœ¬ - æ—¶é—´åºåˆ—ç‰ˆæœ¬ï¼ˆé¿å…æœªæ¥å‡½æ•°ï¼‰

å…³é”®æ”¹è¿›ï¼š
1. æŒ‰æ—¶é—´åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼ˆè€Œééšæœºåˆ’åˆ†ï¼‰
2. è®­ç»ƒé›†ï¼šå†å²æ•°æ®ï¼ˆå¦‚2022-2023å¹´ï¼‰
3. æµ‹è¯•é›†ï¼šæœªæ¥æ•°æ®ï¼ˆå¦‚2024å¹´ï¼‰
4. ç¡®ä¿ä¸ä¼šç”¨æœªæ¥ä¿¡æ¯è®­ç»ƒæ¨¡å‹
"""
import sys
import os
import warnings
import pandas as pd
import numpy as np
from datetime import datetime
import json
import yaml

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

# å¿½ç•¥è­¦å‘Š
warnings.filterwarnings('ignore', category=FutureWarning)
warnings.filterwarnings('ignore', category=UserWarning)

from sklearn.metrics import (
    classification_report, confusion_matrix, 
    roc_auc_score, roc_curve, precision_recall_curve
)
import xgboost as xgb
from src.utils.logger import log
from config.feature_config import get_feature_set, filter_available_features, EFFECTIVE_MARKET_FEATURES, INEFFECTIVE_MARKET_FEATURES


def safe_to_datetime(date_value):
    """
    å®‰å…¨åœ°å°†æ—¥æœŸå€¼è½¬æ¢ä¸ºdatetimeç±»å‹
    
    å¤„ç†ä»¥ä¸‹æƒ…å†µï¼š
    - æ•´æ•°ï¼šå¦‚ 20230101 -> è¢«é”™è¯¯è§£æä¸ºçº³ç§’æ—¶é—´æˆ³
    - å­—ç¬¦ä¸²ï¼šå¦‚ '20230101' -> æ­£å¸¸è§£æ
    - datetimeï¼šç›´æ¥è¿”å›
    """
    if pd.isna(date_value):
        return pd.NaT
    if isinstance(date_value, (int, np.integer, float, np.floating)):
        return pd.to_datetime(str(int(date_value)), format='%Y%m%d', errors='coerce')
    return pd.to_datetime(date_value, errors='coerce')
from src.utils.human_intervention import HumanInterventionChecker, require_human_confirmation
from src.visualization.training_visualizer import TrainingVisualizer


def load_and_prepare_data(neg_version='v2', use_market_factors=True, use_tech_factors=False, use_advanced_factors=False):
    """
    åŠ è½½å¹¶å‡†å¤‡è®­ç»ƒæ•°æ®
    
    Args:
        neg_version: è´Ÿæ ·æœ¬ç‰ˆæœ¬ ('v1' æˆ– 'v2')
        use_market_factors: æ˜¯å¦ä½¿ç”¨å¸¦å¸‚åœºå› å­çš„ç‰¹å¾æ–‡ä»¶
        use_tech_factors: æ˜¯å¦ä½¿ç”¨å¸¦æ–°æŠ€æœ¯å› å­çš„v2ç‰¹å¾æ–‡ä»¶
        use_advanced_factors: æ˜¯å¦ä½¿ç”¨å¸¦é«˜çº§å› å­çš„ç‰¹å¾æ–‡ä»¶
        # TODO: use_ma233_factors: æ˜¯å¦ä½¿ç”¨å¸¦MA233å› å­çš„ç‰¹å¾æ–‡ä»¶ (å¾…å®æ–½ï¼Œè§ docs/plans/ma233_feature_plan.md)
        
    Returns:
        df_features: ç‰¹å¾DataFrame
    """
    log.info("="*80)
    log.info("ç¬¬ä¸€æ­¥ï¼šåŠ è½½æ•°æ®")
    log.info("="*80)
    
    # åŠ è½½æ­£æ ·æœ¬ï¼ˆä½¿ç”¨æ–°çš„ç›®å½•ç»“æ„ï¼‰
    # TODO: MA233å› å­æ”¯æŒ (å¾…å®æ–½)
    # if use_ma233_factors:
    #     pos_file = 'data/training/processed/feature_data_34d_ma233.csv'
    #     log.info("ğŸ“Š ä½¿ç”¨å¸¦MA233å› å­çš„ç‰¹å¾æ–‡ä»¶(ma233)")
    if use_advanced_factors:
        pos_file = 'data/training/processed/feature_data_34d_advanced.csv'
        log.info("ğŸ“Š ä½¿ç”¨å¸¦é«˜çº§æŠ€æœ¯å› å­çš„ç‰¹å¾æ–‡ä»¶(advanced)")
    elif use_tech_factors:
        pos_file = 'data/training/processed/feature_data_34d_full.csv'
        log.info("ğŸ“Š ä½¿ç”¨å¸¦æ–°æŠ€æœ¯å› å­çš„ç‰¹å¾æ–‡ä»¶(full)")
    elif use_market_factors:
        pos_file = 'data/training/processed/feature_data_34d_with_market.csv'
        log.info("ğŸ“Š ä½¿ç”¨å¸¦å¸‚åœºå› å­çš„ç‰¹å¾æ–‡ä»¶")
    else:
        pos_file = 'data/training/processed/feature_data_34d.csv'
        log.info("ğŸ“Š ä½¿ç”¨åŸºç¡€ç‰¹å¾æ–‡ä»¶")
    
    df_pos = pd.read_csv(pos_file)
    df_pos['label'] = 1
    log.success(f"âœ“ æ­£æ ·æœ¬åŠ è½½å®Œæˆ: {len(df_pos)} æ¡")
    
    # åŠ è½½è´Ÿæ ·æœ¬
    if neg_version == 'v2':
        # TODO: MA233å› å­æ”¯æŒ (å¾…å®æ–½)
        # if use_ma233_factors:
        #     neg_file = 'data/training/features/negative_feature_data_v2_34d_ma233.csv'
        if use_advanced_factors:
            neg_file = 'data/training/features/negative_feature_data_v2_34d_advanced.csv'
        elif use_tech_factors:
            neg_file = 'data/training/features/negative_feature_data_v2_34d_full.csv'
        elif use_market_factors:
            neg_file = 'data/training/features/negative_feature_data_v2_34d_with_market.csv'
        else:
            neg_file = 'data/training/features/negative_feature_data_v2_34d.csv'
    else:
        neg_file = 'data/training/features/negative_feature_data_34d.csv'
    
    df_neg = pd.read_csv(neg_file)
    log.success(f"âœ“ è´Ÿæ ·æœ¬åŠ è½½å®Œæˆ: {len(df_neg)} æ¡ (ç‰ˆæœ¬: {neg_version})")
    
    # åˆå¹¶
    df = pd.concat([df_pos, df_neg])
    log.info(f"âœ“ æ•°æ®åˆå¹¶å®Œæˆ: {len(df)} æ¡")
    log.info(f"  - æ­£æ ·æœ¬: {len(df_pos)} æ¡")
    log.info(f"  - è´Ÿæ ·æœ¬: {len(df_neg)} æ¡")
    log.info("")
    
    return df


def extract_features_with_time(df):
    """
    ä»34å¤©çš„æ—¶åºæ•°æ®ä¸­æå–ç»Ÿè®¡ç‰¹å¾ï¼ˆä¿ç•™æ—¶é—´ä¿¡æ¯ï¼‰
    
    Args:
        df: åŸå§‹DataFrameï¼ˆæ¯è¡Œæ˜¯ä¸€å¤©çš„æ•°æ®ï¼‰
        
    Returns:
        df_features: ç‰¹å¾DataFrameï¼ˆæ¯è¡Œæ˜¯ä¸€ä¸ªæ ·æœ¬ï¼ŒåŒ…å«T1æ—¥æœŸï¼‰
    """
    log.info("="*80)
    log.info("ç¬¬äºŒæ­¥ï¼šç‰¹å¾å·¥ç¨‹ï¼ˆä¿ç•™æ—¶é—´ä¿¡æ¯ï¼‰")
    log.info("="*80)
    log.info("å°†34å¤©æ—¶åºæ•°æ®è½¬æ¢ä¸ºç»Ÿè®¡ç‰¹å¾...")
    
    # é‡æ–°åˆ†é…å”¯ä¸€çš„sample_id
    df['unique_sample_id'] = df.groupby(['ts_code', 'label']).ngroup()
    
    features = []
    sample_ids = df['unique_sample_id'].unique()
    
    # è·å–æ­£æ ·æœ¬çš„T1æ—¥æœŸæ˜ å°„ï¼ˆä½¿ç”¨æ–°çš„ç›®å½•ç»“æ„ï¼‰
    df_positive_samples = pd.read_csv('data/training/samples/positive_samples.csv')
    t1_date_map = dict(zip(
        df_positive_samples.index,
        df_positive_samples['t1_date'].apply(safe_to_datetime)
    ))
    
    # è·å–è´Ÿæ ·æœ¬çš„T1æ—¥æœŸæ˜ å°„
    if os.path.exists('data/training/samples/negative_samples_v2.csv'):
        df_negative_samples = pd.read_csv('data/training/samples/negative_samples_v2.csv')
    else:
        df_negative_samples = pd.read_csv('data/training/samples/negative_samples.csv')
    
    # è´Ÿæ ·æœ¬çš„sample_idéœ€è¦åç§»ï¼ˆå› ä¸ºæ˜¯ä»0å¼€å§‹çš„ï¼‰
    max_positive_id = df_positive_samples.index.max()
    for idx, row in df_negative_samples.iterrows():
        t1_date_map[max_positive_id + 1 + idx] = safe_to_datetime(row['t1_date'])
    
    for i, sample_id in enumerate(sample_ids):
        if (i + 1) % 500 == 0:
            log.info(f"è¿›åº¦: {i+1}/{len(sample_ids)}")
        
        sample_data = df[df['unique_sample_id'] == sample_id].sort_values('days_to_t1')
        
        if len(sample_data) < 20:  # è‡³å°‘20å¤©æ•°æ®
            continue
        
        # ä»æ•°æ®ä¸­è·å–T1æ—¥æœŸï¼ˆåŸºäºdays_to_t1=0çš„é‚£ä¸€å¤©ï¼‰
        # æ‰¾åˆ° days_to_t1 æœ€æ¥è¿‘0çš„è®°å½•
        t1_row = sample_data.iloc[sample_data['days_to_t1'].abs().argmin()]
        t1_date = safe_to_datetime(t1_row['trade_date'])
        
        feature_dict = {
            'sample_id': sample_id,
            'ts_code': sample_data['ts_code'].iloc[0],
            'name': sample_data['name'].iloc[0],
            'label': int(sample_data['label'].iloc[0]),
            't1_date': t1_date,  # ä¿ç•™T1æ—¥æœŸï¼Œç”¨äºæ—¶é—´åˆ’åˆ†
        }
        
        # ä»·æ ¼ç‰¹å¾
        feature_dict['close_mean'] = sample_data['close'].mean()
        feature_dict['close_std'] = sample_data['close'].std()
        feature_dict['close_max'] = sample_data['close'].max()
        feature_dict['close_min'] = sample_data['close'].min()
        feature_dict['close_trend'] = (
            (sample_data['close'].iloc[-1] - sample_data['close'].iloc[0]) / 
            sample_data['close'].iloc[0] * 100
        )
        
        # æ¶¨è·Œå¹…ç‰¹å¾
        feature_dict['pct_chg_mean'] = sample_data['pct_chg'].mean()
        feature_dict['pct_chg_std'] = sample_data['pct_chg'].std()
        feature_dict['pct_chg_sum'] = sample_data['pct_chg'].sum()
        feature_dict['positive_days'] = (sample_data['pct_chg'] > 0).sum()
        feature_dict['negative_days'] = (sample_data['pct_chg'] < 0).sum()
        feature_dict['max_gain'] = sample_data['pct_chg'].max()
        feature_dict['max_loss'] = sample_data['pct_chg'].min()
        
        # é‡æ¯”ç‰¹å¾
        if 'volume_ratio' in sample_data.columns:
            feature_dict['volume_ratio_mean'] = sample_data['volume_ratio'].mean()
            feature_dict['volume_ratio_max'] = sample_data['volume_ratio'].max()
            feature_dict['volume_ratio_gt_2'] = (sample_data['volume_ratio'] > 2).sum()
            feature_dict['volume_ratio_gt_4'] = (sample_data['volume_ratio'] > 4).sum()
        
        # MACDç‰¹å¾
        if 'macd' in sample_data.columns:
            macd_data = sample_data['macd'].dropna()
            if len(macd_data) > 0:
                feature_dict['macd_mean'] = macd_data.mean()
                feature_dict['macd_positive_days'] = (macd_data > 0).sum()
                feature_dict['macd_max'] = macd_data.max()
        
        # MAç‰¹å¾
        if 'ma5' in sample_data.columns:
            feature_dict['ma5_mean'] = sample_data['ma5'].mean()
            feature_dict['price_above_ma5'] = (
                sample_data['close'] > sample_data['ma5']
            ).sum()
        
        if 'ma10' in sample_data.columns:
            feature_dict['ma10_mean'] = sample_data['ma10'].mean()
            feature_dict['price_above_ma10'] = (
                sample_data['close'] > sample_data['ma10']
            ).sum()
        
        # å¸‚å€¼ç‰¹å¾
        if 'total_mv' in sample_data.columns:
            mv_data = sample_data['total_mv'].dropna()
            if len(mv_data) > 0:
                feature_dict['total_mv_mean'] = mv_data.mean()
        
        if 'circ_mv' in sample_data.columns:
            circ_mv_data = sample_data['circ_mv'].dropna()
            if len(circ_mv_data) > 0:
                feature_dict['circ_mv_mean'] = circ_mv_data.mean()
        
        # RSIç‰¹å¾
        if 'rsi_6' in sample_data.columns:
            rsi6_data = sample_data['rsi_6'].dropna()
            if len(rsi6_data) > 0:
                feature_dict['rsi_6_mean'] = rsi6_data.mean()
                feature_dict['rsi_6_std'] = rsi6_data.std()
                feature_dict['rsi_6_max'] = rsi6_data.max()
                feature_dict['rsi_6_min'] = rsi6_data.min()
                feature_dict['rsi_6_last'] = rsi6_data.iloc[-1]  # æœ€è¿‘ä¸€å¤©çš„RSI
                feature_dict['rsi_6_gt_70'] = (rsi6_data > 70).sum()  # è¶…ä¹°å¤©æ•°
                feature_dict['rsi_6_lt_30'] = (rsi6_data < 30).sum()  # è¶…å–å¤©æ•°
        
        if 'rsi_12' in sample_data.columns:
            rsi12_data = sample_data['rsi_12'].dropna()
            if len(rsi12_data) > 0:
                feature_dict['rsi_12_mean'] = rsi12_data.mean()
                feature_dict['rsi_12_std'] = rsi12_data.std()
                feature_dict['rsi_12_last'] = rsi12_data.iloc[-1]
                feature_dict['rsi_12_gt_70'] = (rsi12_data > 70).sum()
                feature_dict['rsi_12_lt_30'] = (rsi12_data < 30).sum()
        
        if 'rsi_24' in sample_data.columns:
            rsi24_data = sample_data['rsi_24'].dropna()
            if len(rsi24_data) > 0:
                feature_dict['rsi_24_mean'] = rsi24_data.mean()
                feature_dict['rsi_24_std'] = rsi24_data.std()
                feature_dict['rsi_24_last'] = rsi24_data.iloc[-1]
        
        # åŠ¨é‡ç‰¹å¾ï¼ˆåˆ†æ®µæ”¶ç›Šç‡ï¼‰
        days = len(sample_data)
        if days >= 7:
            feature_dict['return_1w'] = (
                (sample_data['close'].iloc[-1] - sample_data['close'].iloc[-7]) /
                sample_data['close'].iloc[-7] * 100
            )
        if days >= 14:
            feature_dict['return_2w'] = (
                (sample_data['close'].iloc[-1] - sample_data['close'].iloc[-14]) /
                sample_data['close'].iloc[-14] * 100
            )
        
        # ===== å¸‚åœºå› å­ç‰¹å¾ï¼ˆå¦‚æœå­˜åœ¨ï¼‰=====
        if 'market_pct_chg' in sample_data.columns:
            market_data = sample_data['market_pct_chg'].dropna()
            if len(market_data) > 0:
                feature_dict['market_pct_chg_mean'] = market_data.mean()
        
        if 'market_return_34d' in sample_data.columns:
            market_return_data = sample_data['market_return_34d'].dropna()
            if len(market_return_data) > 0:
                feature_dict['market_return_34d_last'] = market_return_data.iloc[-1]
        
        if 'market_volatility_34d' in sample_data.columns:
            market_vol_data = sample_data['market_volatility_34d'].dropna()
            if len(market_vol_data) > 0:
                feature_dict['market_volatility_34d_last'] = market_vol_data.iloc[-1]
        
        if 'market_trend' in sample_data.columns:
            market_trend_data = sample_data['market_trend'].dropna()
            if len(market_trend_data) > 0:
                feature_dict['market_trend_last'] = market_trend_data.iloc[-1]
        
        if 'excess_return' in sample_data.columns:
            excess_data = sample_data['excess_return'].dropna()
            if len(excess_data) > 0:
                feature_dict['excess_return_mean'] = excess_data.mean()
                feature_dict['excess_return_sum'] = excess_data.sum()
                feature_dict['excess_return_positive_days'] = (excess_data > 0).sum()
        
        if 'excess_return_cumsum' in sample_data.columns:
            excess_cumsum_data = sample_data['excess_return_cumsum'].dropna()
            if len(excess_cumsum_data) > 0:
                feature_dict['excess_return_cumsum_last'] = excess_cumsum_data.iloc[-1]
        
        if 'price_vs_hist_mean' in sample_data.columns:
            hist_mean_data = sample_data['price_vs_hist_mean'].dropna()
            if len(hist_mean_data) > 0:
                feature_dict['price_vs_hist_mean_last'] = hist_mean_data.iloc[-1]
        
        # ä»¥ä¸‹ä½æ•ˆç‰¹å¾å·²å‰”é™¤ï¼ˆé‡è¦æ€§ < é˜ˆå€¼ï¼‰:
        # - price_vs_hist_high_last: 0.0088
        # - volatility_vs_hist_last: 0.0064
        
        # ===== æ–°æŠ€æœ¯å› å­ç‰¹å¾ï¼ˆfullï¼‰=====
        # æ¢æ‰‹ç‡ï¼ˆè‡ªç”±æµé€šè‚¡ï¼‰
        if 'turnover_rate_f' in sample_data.columns:
            turnover_data = sample_data['turnover_rate_f'].dropna()
            if len(turnover_data) > 0:
                feature_dict['turnover_rate_f_mean'] = turnover_data.mean()
                feature_dict['turnover_rate_f_max'] = turnover_data.max()
                feature_dict['turnover_rate_f_std'] = turnover_data.std()
        
        # ä¹–ç¦»ç‡BIAS (bias_short/mid/long)
        if 'bias_short' in sample_data.columns:
            bias_short = sample_data['bias_short'].dropna()
            if len(bias_short) > 0:
                feature_dict['bias_short_last'] = bias_short.iloc[-1]
                feature_dict['bias_short_mean'] = bias_short.mean()
        if 'bias_mid' in sample_data.columns:
            bias_mid = sample_data['bias_mid'].dropna()
            if len(bias_mid) > 0:
                feature_dict['bias_mid_last'] = bias_mid.iloc[-1]
        if 'bias_long' in sample_data.columns:
            bias_long = sample_data['bias_long'].dropna()
            if len(bias_long) > 0:
                feature_dict['bias_long_last'] = bias_long.iloc[-1]
        
        # EMA
        if 'ema_5' in sample_data.columns and 'ema_20' in sample_data.columns:
            ema5 = sample_data['ema_5'].dropna()
            ema20 = sample_data['ema_20'].dropna()
            if len(ema5) > 0 and len(ema20) > 0:
                # EMAçŸ­æœŸ/é•¿æœŸæ¯”å€¼
                feature_dict['ema_ratio_5_20'] = ema5.iloc[-1] / ema20.iloc[-1] if ema20.iloc[-1] != 0 else 1
                # ä»·æ ¼ç›¸å¯¹EMAä½ç½®
                if len(sample_data['close'].dropna()) > 0:
                    close_last = sample_data['close'].dropna().iloc[-1]
                    feature_dict['price_vs_ema5'] = (close_last - ema5.iloc[-1]) / ema5.iloc[-1] * 100 if ema5.iloc[-1] != 0 else 0
                    feature_dict['price_vs_ema20'] = (close_last - ema20.iloc[-1]) / ema20.iloc[-1] * 100 if ema20.iloc[-1] != 0 else 0
        if 'ema_60' in sample_data.columns:
            ema60 = sample_data['ema_60'].dropna()
            if len(ema60) > 0 and len(sample_data['close'].dropna()) > 0:
                close_last = sample_data['close'].dropna().iloc[-1]
                feature_dict['price_vs_ema60'] = (close_last - ema60.iloc[-1]) / ema60.iloc[-1] * 100 if ema60.iloc[-1] != 0 else 0
        
        # KDJ
        if 'kdj_k' in sample_data.columns:
            kdj_k = sample_data['kdj_k'].dropna()
            if len(kdj_k) > 0:
                feature_dict['kdj_k_last'] = kdj_k.iloc[-1]
                feature_dict['kdj_k_mean'] = kdj_k.mean()
        if 'kdj_d' in sample_data.columns:
            kdj_d = sample_data['kdj_d'].dropna()
            if len(kdj_d) > 0:
                feature_dict['kdj_d_last'] = kdj_d.iloc[-1]
        if 'kdj_j' in sample_data.columns:
            kdj_j = sample_data['kdj_j'].dropna()
            if len(kdj_j) > 0:
                feature_dict['kdj_j_last'] = kdj_j.iloc[-1]
                # Jå€¼è¶…ä¹°è¶…å–
                feature_dict['kdj_j_overbought'] = (kdj_j > 80).sum()
                feature_dict['kdj_j_oversold'] = (kdj_j < 20).sum()
        
        # æ¶¨åœç»Ÿè®¡ (is_limit_up)
        if 'is_limit_up' in sample_data.columns:
            is_limit = sample_data['is_limit_up'].dropna()
            if len(is_limit) > 0:
                feature_dict['limit_up_count'] = is_limit.sum()
        
        # OBV
        if 'obv' in sample_data.columns:
            obv = sample_data['obv'].dropna()
            if len(obv) > 0:
                # OBVå˜åŒ–ç‡
                feature_dict['obv_change'] = (obv.iloc[-1] - obv.iloc[0]) / abs(obv.iloc[0]) * 100 if obv.iloc[0] != 0 else 0
                feature_dict['obv_trend'] = 1 if obv.iloc[-1] > obv.mean() else 0
        
        # æˆäº¤é‡ä¸å‡é‡æ¯” (vol_ma5_ratio/vol_ma20_ratio)
        if 'vol_ma5_ratio' in sample_data.columns:
            vol_r5 = sample_data['vol_ma5_ratio'].dropna()
            if len(vol_r5) > 0:
                feature_dict['vol_ma5_ratio_mean'] = vol_r5.mean()
                feature_dict['vol_ma5_ratio_max'] = vol_r5.max()
        if 'vol_ma20_ratio' in sample_data.columns:
            vol_r20 = sample_data['vol_ma20_ratio'].dropna()
            if len(vol_r20) > 0:
                feature_dict['vol_ma20_ratio_mean'] = vol_r20.mean()
                feature_dict['vol_ma20_ratio_max'] = vol_r20.max()
        
        # ===== é«˜çº§æŠ€æœ¯å› å­ï¼ˆadvancedï¼‰=====
        # åŠ¨é‡å› å­
        for period in [5, 10, 20]:
            col = f'momentum_{period}d'
            if col in sample_data.columns:
                data = sample_data[col].dropna()
                if len(data) > 0:
                    feature_dict[f'{col}_last'] = data.iloc[-1]
                    feature_dict[f'{col}_mean'] = data.mean()
        
        if 'momentum_acceleration' in sample_data.columns:
            data = sample_data['momentum_acceleration'].dropna()
            if len(data) > 0:
                feature_dict['momentum_acceleration_last'] = data.iloc[-1]
        
        # é‡ä»·é…åˆåº¦
        if 'volume_price_corr_10d' in sample_data.columns:
            data = sample_data['volume_price_corr_10d'].dropna()
            if len(data) > 0:
                feature_dict['volume_price_corr_last'] = data.iloc[-1]
        if 'volume_price_match_sum_10d' in sample_data.columns:
            data = sample_data['volume_price_match_sum_10d'].dropna()
            if len(data) > 0:
                feature_dict['volume_price_match_sum'] = data.iloc[-1]
        
        # å¤šæ—¶é—´æ¡†æ¶ç‰¹å¾ (8d, 55d)
        for tf in [8, 55]:
            for metric in ['return', 'price_vs_ma', 'volatility', 'price_position', 'trend_slope']:
                col = f'{metric}_{tf}d'
                if col in sample_data.columns:
                    data = sample_data[col].dropna()
                    if len(data) > 0:
                        feature_dict[f'{col}_last'] = data.iloc[-1]
        
        # çªç ´å½¢æ€
        for period in [10, 20, 55]:
            col = f'breakout_high_{period}d'
            if col in sample_data.columns:
                data = sample_data[col].dropna()
                if len(data) > 0:
                    feature_dict[f'{col}_sum'] = data.sum()
        
        for ma in [5, 10, 20, 55]:
            col = f'breakout_ma{ma}'
            if col in sample_data.columns:
                data = sample_data[col].dropna()
                if len(data) > 0:
                    feature_dict[f'{col}_sum'] = data.sum()
        
        if 'high_volume_breakout' in sample_data.columns:
            data = sample_data['high_volume_breakout'].dropna()
            if len(data) > 0:
                feature_dict['high_volume_breakout_sum'] = data.sum()
        
        if 'consecutive_new_high' in sample_data.columns:
            data = sample_data['consecutive_new_high'].dropna()
            if len(data) > 0:
                feature_dict['consecutive_new_high_max'] = data.max()
        
        # æ”¯æ’‘é˜»åŠ›
        for period in [10, 20]:
            for metric in ['dist_to_support', 'dist_to_resistance']:
                col = f'{metric}_{period}d'
                if col in sample_data.columns:
                    data = sample_data[col].dropna()
                    if len(data) > 0:
                        feature_dict[f'{col}_last'] = data.iloc[-1]
            
            for metric in ['support_strength', 'resistance_strength']:
                col = f'{metric}_{period}d'
                if col in sample_data.columns:
                    data = sample_data[col].dropna()
                    if len(data) > 0:
                        feature_dict[f'{col}_last'] = data.iloc[-1]
        
        if 'channel_width_20d' in sample_data.columns:
            data = sample_data['channel_width_20d'].dropna()
            if len(data) > 0:
                feature_dict['channel_width_last'] = data.iloc[-1]
        
        # é«˜çº§æˆäº¤é‡
        for col in ['volume_trend_slope_10d', 'volume_trend_slope_20d']:
            if col in sample_data.columns:
                data = sample_data[col].dropna()
                if len(data) > 0:
                    feature_dict[f'{col}_last'] = data.iloc[-1]
        
        if 'volume_breakout_count_20d' in sample_data.columns:
            data = sample_data['volume_breakout_count_20d'].dropna()
            if len(data) > 0:
                feature_dict['volume_breakout_count'] = data.iloc[-1]
        
        if 'price_up_vol_down_count_10d' in sample_data.columns:
            data = sample_data['price_up_vol_down_count_10d'].dropna()
            if len(data) > 0:
                feature_dict['price_up_vol_down_count'] = data.iloc[-1]
        
        if 'price_down_vol_up_count_10d' in sample_data.columns:
            data = sample_data['price_down_vol_up_count_10d'].dropna()
            if len(data) > 0:
                feature_dict['price_down_vol_up_count'] = data.iloc[-1]
        
        if 'volume_rsv_20d' in sample_data.columns:
            data = sample_data['volume_rsv_20d'].dropna()
            if len(data) > 0:
                feature_dict['volume_rsv_last'] = data.iloc[-1]
        
        if 'obv_trend' in sample_data.columns:
            data = sample_data['obv_trend'].dropna()
            if len(data) > 0:
                feature_dict['obv_trend_sum'] = data.sum()
        
        features.append(feature_dict)
    
    df_features = pd.DataFrame(features)
    
    log.success(f"âœ“ ç‰¹å¾æå–å®Œæˆ: {len(df_features)} ä¸ªæ ·æœ¬")
    log.info(f"âœ“ ç‰¹å¾ç»´åº¦: {len(df_features.columns) - 3} ä¸ªç‰¹å¾ï¼ˆä¸å«sample_id, label, t1_dateï¼‰")
    log.info("")
    
    return df_features


def timeseries_split(df_features, train_end_date=None, test_start_date=None, feature_set='optimized'):
    """
    æŒ‰æ—¶é—´åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼ˆé¿å…æœªæ¥å‡½æ•°ï¼‰
    
    Args:
        df_features: ç‰¹å¾DataFrameï¼ˆå¿…é¡»åŒ…å«t1_dateåˆ—ï¼‰
        train_end_date: è®­ç»ƒé›†æˆªæ­¢æ—¥æœŸï¼ˆå¦‚'2023-12-31'ï¼‰
        test_start_date: æµ‹è¯•é›†å¼€å§‹æ—¥æœŸï¼ˆå¦‚'2024-01-01'ï¼‰
        feature_set: ç‰¹å¾é›†åç§°ï¼Œå¯é€‰ 'base', 'all_market', 'optimized', 'core'
        
    Returns:
        X_train, X_test, y_train, y_test, train_dates, test_dates
    """
    log.info("="*80)
    log.info("ç¬¬ä¸‰æ­¥ï¼šæ—¶é—´åºåˆ—åˆ’åˆ†ï¼ˆé¿å…æœªæ¥å‡½æ•°ï¼‰")
    log.info("="*80)
    
    # ç¡®ä¿t1_dateæ˜¯datetimeç±»å‹ï¼ˆé˜²æ­¢æ•´æ•°è¢«è¯¯è§£æï¼‰
    df_features['t1_date'] = df_features['t1_date'].apply(safe_to_datetime)
    
    # æŒ‰æ—¶é—´æ’åº
    df_features = df_features.sort_values('t1_date').reset_index(drop=True)
    
    # æ˜¾ç¤ºæ—¶é—´èŒƒå›´
    min_date = df_features['t1_date'].min()
    max_date = df_features['t1_date'].max()
    log.info(f"æ•°æ®æ—¶é—´èŒƒå›´: {min_date.date()} è‡³ {max_date.date()}")
    
    # å¦‚æœæœªæŒ‡å®šåˆ’åˆ†ç‚¹ï¼Œä½¿ç”¨80%ä½œä¸ºè®­ç»ƒé›†
    if train_end_date is None:
        n_train = int(len(df_features) * 0.8)
        train_end_date = df_features.iloc[n_train]['t1_date']
        test_start_date = df_features.iloc[n_train + 1]['t1_date']
    else:
        train_end_date = pd.to_datetime(train_end_date)
        test_start_date = pd.to_datetime(test_start_date)
    
    # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
    train_mask = df_features['t1_date'] <= train_end_date
    test_mask = df_features['t1_date'] >= test_start_date
    
    df_train = df_features[train_mask]
    df_test = df_features[test_mask]
    
    log.info(f"\næ—¶é—´åˆ’åˆ†:")
    log.info(f"  è®­ç»ƒé›†: {df_train['t1_date'].min().date()} è‡³ {df_train['t1_date'].max().date()}")
    log.info(f"  æµ‹è¯•é›†: {df_test['t1_date'].min().date()} è‡³ {df_test['t1_date'].max().date()}")
    log.info(f"\næ ·æœ¬åˆ’åˆ†:")
    log.info(f"  è®­ç»ƒé›†: {len(df_train)} ä¸ªæ ·æœ¬ (æ­£:{(df_train['label']==1).sum()}, è´Ÿ:{(df_train['label']==0).sum()})")
    log.info(f"  æµ‹è¯•é›†: {len(df_test)} ä¸ªæ ·æœ¬ (æ­£:{(df_test['label']==1).sum()}, è´Ÿ:{(df_test['label']==0).sum()})")
    log.info("")
    
    # ç¡®è®¤æ— æ•°æ®æ³„éœ²
    if df_train['t1_date'].max() >= df_test['t1_date'].min():
        log.warning("âš ï¸  è­¦å‘Šï¼šè®­ç»ƒé›†å’Œæµ‹è¯•é›†æ—¶é—´æœ‰é‡å ï¼Œå¯èƒ½å­˜åœ¨æ•°æ®æ³„éœ²ï¼")
    else:
        log.success("âœ“ è®­ç»ƒé›†å’Œæµ‹è¯•é›†æ—¶é—´æ— é‡å ï¼Œæ— æ•°æ®æ³„éœ²é£é™©")
    
    # å‡†å¤‡ç‰¹å¾å’Œæ ‡ç­¾ï¼ˆæ’é™¤éç‰¹å¾åˆ—å’Œéæ•°å€¼åˆ—ï¼‰
    exclude_cols = ['sample_id', 'label', 't1_date', 'ts_code', 'name']
    all_feature_cols = [col for col in df_features.columns 
                       if col not in exclude_cols]
    
    # ç‰¹å¾ç­›é€‰ï¼šæ ¹æ®feature_setå‚æ•°ç­›é€‰ç‰¹å¾
    log.info(f"\nç‰¹å¾ç­›é€‰ï¼ˆä½¿ç”¨ç‰¹å¾é›†: {feature_set}ï¼‰:")
    if feature_set == 'optimized':
        # æ’é™¤ä½æ•ˆå¸‚åœºå› å­
        ineffective_cols = [col for col in INEFFECTIVE_MARKET_FEATURES if col.replace('_last', '') in col or col in all_feature_cols]
        ineffective_cols_in_data = [col for col in all_feature_cols if col in INEFFECTIVE_MARKET_FEATURES]
        feature_cols = [col for col in all_feature_cols if col not in ineffective_cols_in_data]
        log.info(f"  å‰”é™¤ä½æ•ˆå¸‚åœºå› å­: {ineffective_cols_in_data}")
        log.success(f"  âœ“ ä¿ç•™ {len(feature_cols)} ä¸ªé«˜æ•ˆç‰¹å¾")
    elif feature_set == 'base':
        # ä»…ä½¿ç”¨åŸºç¡€ç‰¹å¾ï¼Œæ’é™¤æ‰€æœ‰å¸‚åœºå› å­
        all_market_cols = EFFECTIVE_MARKET_FEATURES + INEFFECTIVE_MARKET_FEATURES
        market_cols_in_data = [col for col in all_feature_cols if col in all_market_cols]
        feature_cols = [col for col in all_feature_cols if col not in market_cols_in_data]
        log.info(f"  å‰”é™¤æ‰€æœ‰å¸‚åœºå› å­: {market_cols_in_data}")
        log.success(f"  âœ“ ä¿ç•™ {len(feature_cols)} ä¸ªåŸºç¡€ç‰¹å¾")
    else:
        # ä½¿ç”¨å…¨éƒ¨ç‰¹å¾
        feature_cols = all_feature_cols
        log.info(f"  ä½¿ç”¨å…¨éƒ¨ç‰¹å¾: {len(feature_cols)} ä¸ª")
    
    X_train = df_train[feature_cols].copy()
    y_train = df_train['label']
    train_dates = df_train['t1_date']
    
    X_test = df_test[feature_cols].copy()
    y_test = df_test['label']
    test_dates = df_test['t1_date']
    
    # åˆ é™¤éæ•°å€¼åˆ—ï¼ˆå¦‚æœè¿˜æœ‰çš„è¯ï¼‰
    non_numeric_cols = X_train.select_dtypes(include=['object']).columns
    if len(non_numeric_cols) > 0:
        log.info(f"åˆ é™¤éæ•°å€¼åˆ—: {list(non_numeric_cols)}")
        X_train = X_train.drop(columns=non_numeric_cols)
        X_test = X_test.drop(columns=non_numeric_cols)
        feature_cols = [col for col in feature_cols if col not in non_numeric_cols]
    
    # ç¼ºå¤±å€¼å¤„ç†ï¼šä½¿ç”¨è®­ç»ƒé›†çš„ç»Ÿè®¡é‡å¡«å……ï¼ˆé¿å…æœªæ¥å‡½æ•°ï¼‰
    # å…³é”®ï¼šåªç”¨è®­ç»ƒé›†çš„ç»Ÿè®¡é‡ï¼Œä¸èƒ½ç”¨æµ‹è¯•é›†æ•°æ®
    log.info("\nç¼ºå¤±å€¼å¤„ç†ï¼ˆé¿å…æœªæ¥å‡½æ•°ï¼‰:")
    train_missing = X_train.isnull().sum().sum()
    test_missing = X_test.isnull().sum().sum()
    log.info(f"  è®­ç»ƒé›†ç¼ºå¤±å€¼: {train_missing}")
    log.info(f"  æµ‹è¯•é›†ç¼ºå¤±å€¼: {test_missing}")
    
    # è®¡ç®—è®­ç»ƒé›†çš„ä¸­ä½æ•°ï¼ˆæ›´ç¨³å¥ï¼Œä¸å—å¼‚å¸¸å€¼å½±å“ï¼‰
    train_medians = X_train.median()
    X_train = X_train.fillna(train_medians)
    X_test = X_test.fillna(train_medians)  # ç”¨è®­ç»ƒé›†çš„ç»Ÿè®¡é‡å¡«å……æµ‹è¯•é›†
    log.success("  âœ“ ä½¿ç”¨è®­ç»ƒé›†ä¸­ä½æ•°å¡«å……ï¼ˆé¿å…æ•°æ®æ³„éœ²ï¼‰")
    
    log.info(f"\nç‰¹å¾çŸ©é˜µ:")
    log.info(f"  è®­ç»ƒé›†: {X_train.shape}")
    log.info(f"  æµ‹è¯•é›†: {X_test.shape}")
    log.info(f"  ç‰¹å¾æ•°: {len(feature_cols)}")
    log.info("")
    
    return X_train, X_test, y_train, y_test, train_dates, test_dates


def train_model(X_train, y_train, X_test, y_test):
    """
    è®­ç»ƒXGBoostæ¨¡å‹
    
    Args:
        X_train, y_train: è®­ç»ƒé›†
        X_test, y_test: æµ‹è¯•é›†
        
    Returns:
        model, metrics
    """
    log.info("="*80)
    log.info("ç¬¬å››æ­¥ï¼šè®­ç»ƒXGBoostæ¨¡å‹")
    log.info("="*80)
    
    # è®¡ç®—ç±»åˆ«æƒé‡ï¼ˆå¤„ç†æ ·æœ¬ä¸å‡è¡¡ï¼‰
    neg_count = (y_train == 0).sum()
    pos_count = (y_train == 1).sum()
    raw_weight = neg_count / pos_count if pos_count > 0 else 1.0
    # é™åˆ¶æƒé‡èŒƒå›´åœ¨[0.5, 2.0]ä¹‹é—´ï¼Œé¿å…è¿‡åº¦è¡¥å¿
    scale_pos_weight = max(0.5, min(2.0, raw_weight))
    log.info(f"æ ·æœ¬ä¸å‡è¡¡å¤„ç†: æ­£æ ·æœ¬={pos_count}, è´Ÿæ ·æœ¬={neg_count}, scale_pos_weight={scale_pos_weight:.3f} (åŸå§‹:{raw_weight:.3f})")
    
    # è®­ç»ƒæ¨¡å‹
    log.info("å¼€å§‹è®­ç»ƒ...")
    model = xgb.XGBClassifier(
        n_estimators=100,
        max_depth=5,
        learning_rate=0.1,
        subsample=0.8,
        colsample_bytree=0.8,
        min_child_weight=3,
        gamma=0.1,
        reg_alpha=0.1,
        reg_lambda=1.0,
        scale_pos_weight=scale_pos_weight,  # å¤„ç†æ ·æœ¬ä¸å‡è¡¡
        random_state=42,
        eval_metric='logloss'
    )
    
    model.fit(
        X_train, y_train,
        eval_set=[(X_test, y_test)],
        verbose=False
    )
    
    log.success("âœ“ æ¨¡å‹è®­ç»ƒå®Œæˆï¼")
    log.info("")
    
    # é¢„æµ‹
    y_pred = model.predict(X_test)
    y_prob = model.predict_proba(X_test)[:, 1]
    
    # è¯„ä¼°
    log.info("="*80)
    log.info("ç¬¬äº”æ­¥ï¼šæ¨¡å‹è¯„ä¼°ï¼ˆæµ‹è¯•é›† = æœªæ¥æ•°æ®ï¼‰")
    log.info("="*80)
    
    # åˆ†ç±»æŠ¥å‘Š
    log.info("\nåˆ†ç±»æŠ¥å‘Š:")
    report = classification_report(
        y_test, y_pred, 
        target_names=['è´Ÿæ ·æœ¬', 'æ­£æ ·æœ¬'],
        output_dict=True
    )
    print(classification_report(
        y_test, y_pred, 
        target_names=['è´Ÿæ ·æœ¬', 'æ­£æ ·æœ¬']
    ))
    
    # AUC
    auc = roc_auc_score(y_test, y_prob)
    log.info(f"\nAUC-ROC: {auc:.4f}")
    
    # æ··æ·†çŸ©é˜µ
    cm = confusion_matrix(y_test, y_pred)
    log.info("\næ··æ·†çŸ©é˜µ:")
    log.info(f"  çœŸè´Ÿä¾‹(TN): {cm[0,0]:4d}  |  å‡æ­£ä¾‹(FP): {cm[0,1]:4d}")
    log.info(f"  å‡è´Ÿä¾‹(FN): {cm[1,0]:4d}  |  çœŸæ­£ä¾‹(TP): {cm[1,1]:4d}")
    
    # ç‰¹å¾é‡è¦æ€§
    feature_importance = pd.DataFrame({
        'feature': X_train.columns,
        'importance': model.feature_importances_
    }).sort_values('importance', ascending=False)
    
    log.info("\n" + "="*80)
    log.info("ç‰¹å¾é‡è¦æ€§ Top 10:")
    log.info("="*80)
    for idx, row in feature_importance.head(10).iterrows():
        log.info(f"  {row['feature']:25s}: {row['importance']:.4f}")
    
    # æ±‡æ€»æŒ‡æ ‡
    metrics = {
        'accuracy': report['accuracy'],
        'precision': report['æ­£æ ·æœ¬']['precision'],
        'recall': report['æ­£æ ·æœ¬']['recall'],
        'f1_score': report['æ­£æ ·æœ¬']['f1-score'],
        'auc': auc,
        'confusion_matrix': cm.tolist(),
        'feature_importance': feature_importance.to_dict('records')
    }
    
    return model, metrics, y_prob


def generate_training_visualizations(model, X_train, df_features, train_dates, test_dates, neg_version):
    """ç”Ÿæˆè®­ç»ƒè¿‡ç¨‹å¯è§†åŒ–å›¾è¡¨"""
    try:
        log.info("="*80)
        log.info("ç”Ÿæˆè®­ç»ƒå¯è§†åŒ–å›¾è¡¨")
        log.info("="*80)
        
        visualizer = TrainingVisualizer(
            output_dir=f"data/training/charts"
        )
        
        # 1. æ ·æœ¬è´¨é‡å¯è§†åŒ–ï¼ˆæ­£æ ·æœ¬ï¼‰
        try:
            df_positive_samples = pd.read_csv('data/training/samples/positive_samples.csv')
            visualizer.visualize_sample_quality(
                df_positive_samples,
                save_prefix="positive_sample_quality"
            )
        except Exception as e:
            log.warning(f"ç”Ÿæˆæ­£æ ·æœ¬è´¨é‡å¯è§†åŒ–æ—¶å‡ºé”™: {e}")
        
        # è´Ÿæ ·æœ¬
        try:
            if neg_version == 'v2':
                neg_file = 'data/training/samples/negative_samples_v2.csv'
            else:
                neg_file = 'data/training/samples/negative_samples.csv'
            
            if os.path.exists(neg_file):
                df_negative_samples = pd.read_csv(neg_file)
                visualizer.visualize_sample_quality(
                    df_negative_samples,
                    save_prefix="negative_sample_quality"
                )
        except Exception as e:
            log.warning(f"ç”Ÿæˆè´Ÿæ ·æœ¬è´¨é‡å¯è§†åŒ–æ—¶å‡ºé”™: {e}")
        
        # 2. å› å­é‡è¦æ€§å¯è§†åŒ–
        feature_importance = pd.DataFrame({
            'feature': X_train.columns,
            'importance': model.feature_importances_
        })
        
        visualizer.visualize_feature_importance(
            feature_importance,
            model_name=f"xgboost_timeseries_{neg_version}",
            top_n=20
        )
        
        # 3. ç”Ÿæˆç´¢å¼•é¡µé¢
        visualizer.generate_index_page(model_name=f"xgboost_timeseries_{neg_version}")
        
        log.success("âœ“ å¯è§†åŒ–å›¾è¡¨ç”Ÿæˆå®Œæˆ")
        log.info(f"ğŸ“Š æŸ¥çœ‹å›¾è¡¨: open data/training/charts/index.html")
        
    except Exception as e:
        log.warning(f"ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨æ—¶å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()


def save_model(model, metrics, neg_version, train_dates, test_dates, 
               version=None, model_name='breakout_launch_scorer', feature_names=None,
               training_config=None):
    """
    ä¿å­˜æ¨¡å‹å’Œç»“æœ
    
    Args:
        model: è®­ç»ƒå¥½çš„æ¨¡å‹
        metrics: è¯„ä¼°æŒ‡æ ‡
        neg_version: è´Ÿæ ·æœ¬ç‰ˆæœ¬
        train_dates: è®­ç»ƒé›†æ—¥æœŸ
        test_dates: æµ‹è¯•é›†æ—¥æœŸ
        version: ç‰ˆæœ¬å·ï¼ˆå¦‚ v1.5.0ï¼‰ï¼ŒæŒ‡å®šåå°†ä¿å­˜åˆ°ç‰ˆæœ¬ç›®å½•
        model_name: æ¨¡å‹åç§°
        feature_names: ç‰¹å¾åç§°åˆ—è¡¨
        training_config: è®­ç»ƒé…ç½®å­—å…¸
    """
    log.info("\n" + "="*80)
    log.info("ç¬¬å…­æ­¥ï¼šä¿å­˜æ¨¡å‹")
    log.info("="*80)
    
    # åˆ›å»ºç›®å½•ï¼ˆä½¿ç”¨æ–°çš„ç›®å½•ç»“æ„ï¼‰
    os.makedirs('data/training/models', exist_ok=True)
    os.makedirs('data/training/metrics', exist_ok=True)
    
    # ä¿å­˜æ¨¡å‹ï¼ˆä½¿ç”¨boosteræ–¹æ³•é¿å…sklearn mixiné—®é¢˜ï¼‰
    model_file = f'data/training/models/xgboost_timeseries_{neg_version}_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
    model.get_booster().save_model(model_file)
    log.success(f"âœ“ æ¨¡å‹å·²ä¿å­˜: {model_file}")
    
    # ä¿å­˜æŒ‡æ ‡
    metrics_file = f'data/training/metrics/xgboost_timeseries_{neg_version}_metrics.json'
    metrics['model_file'] = model_file
    metrics['timestamp'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    metrics['neg_version'] = neg_version
    metrics['train_date_range'] = f"{train_dates.min().date()} to {train_dates.max().date()}"
    metrics['test_date_range'] = f"{test_dates.min().date()} to {test_dates.max().date()}"
    metrics['note'] = 'ä½¿ç”¨æ—¶é—´åºåˆ—åˆ’åˆ†ï¼Œé¿å…æœªæ¥å‡½æ•°'
    
    with open(metrics_file, 'w', encoding='utf-8') as f:
        json.dump(metrics, f, indent=2, ensure_ascii=False)
    
    log.success(f"âœ“ è¯„ä¼°æŠ¥å‘Šå·²ä¿å­˜: {metrics_file}")
    
    # å¦‚æœæŒ‡å®šäº†ç‰ˆæœ¬å·ï¼Œä¿å­˜åˆ°ç‰ˆæœ¬ç›®å½•
    if version:
        save_to_version_directory(
            model=model,
            metrics=metrics,
            version=version,
            model_name=model_name,
            feature_names=feature_names,
            train_dates=train_dates,
            test_dates=test_dates,
            training_config=training_config
        )
    
    log.info("")


def save_to_version_directory(model, metrics, version, model_name, feature_names,
                              train_dates, test_dates, training_config=None):
    """
    å°†æ¨¡å‹ä¿å­˜åˆ°ç‰ˆæœ¬ç®¡ç†ç›®å½•
    
    Args:
        model: è®­ç»ƒå¥½çš„æ¨¡å‹
        metrics: è¯„ä¼°æŒ‡æ ‡
        version: ç‰ˆæœ¬å·ï¼ˆå¦‚ v1.5.0ï¼‰
        model_name: æ¨¡å‹åç§°
        feature_names: ç‰¹å¾åç§°åˆ—è¡¨
        train_dates: è®­ç»ƒé›†æ—¥æœŸ
        test_dates: æµ‹è¯•é›†æ—¥æœŸ
        training_config: è®­ç»ƒé…ç½®å­—å…¸
    """
    import shutil
    
    log.info("\n" + "-"*60)
    log.info(f"ğŸ“¦ ä¿å­˜åˆ°ç‰ˆæœ¬ç›®å½•: {model_name}/{version}")
    log.info("-"*60)
    
    # ç‰ˆæœ¬ç›®å½•
    version_dir = f'data/models/{model_name}/versions/{version}'
    model_dir = f'{version_dir}/model'
    training_dir = f'{version_dir}/training'
    charts_dir = f'{version_dir}/charts'
    
    # åˆ›å»ºç›®å½•ç»“æ„
    os.makedirs(model_dir, exist_ok=True)
    os.makedirs(training_dir, exist_ok=True)
    os.makedirs(charts_dir, exist_ok=True)
    os.makedirs(f'{version_dir}/evaluation', exist_ok=True)
    os.makedirs(f'{version_dir}/experiments', exist_ok=True)
    
    # 1. ä¿å­˜æ¨¡å‹æ–‡ä»¶
    model_file = f'{model_dir}/model.json'
    model.get_booster().save_model(model_file)
    log.success(f"  âœ“ æ¨¡å‹æ–‡ä»¶: {model_file}")
    
    # 2. ä¿å­˜ç‰¹å¾åç§°
    if feature_names:
        feature_file = f'{model_dir}/feature_names.json'
        with open(feature_file, 'w', encoding='utf-8') as f:
            json.dump(feature_names, f, indent=2, ensure_ascii=False)
        log.success(f"  âœ“ ç‰¹å¾åç§°: {feature_file}")
    
    # 3. ä¿å­˜è®­ç»ƒæŒ‡æ ‡
    metrics_file = f'{training_dir}/metrics.json'
    with open(metrics_file, 'w', encoding='utf-8') as f:
        json.dump(metrics, f, indent=2, ensure_ascii=False)
    log.success(f"  âœ“ è®­ç»ƒæŒ‡æ ‡: {metrics_file}")
    
    # 4. ä¿å­˜å…ƒæ•°æ®
    metadata = {
        'version': version,
        'model_name': model_name,
        'status': 'development',
        'created_at': datetime.now().isoformat(),
        'created_by': 'train_xgboost_timeseries.py',
        'parent_version': None,
        'metrics': {
            'training': {
                'accuracy': metrics.get('accuracy'),
                'precision': metrics.get('precision'),
                'recall': metrics.get('recall'),
                'f1': metrics.get('f1_score'),
                'auc': metrics.get('auc')
            },
            'validation': {
                'accuracy': metrics.get('accuracy'),
                'precision': metrics.get('precision'),
                'recall': metrics.get('recall'),
                'f1': metrics.get('f1_score'),
                'auc': metrics.get('auc')
            },
            'test': {
                'accuracy': metrics.get('accuracy'),
                'precision': metrics.get('precision'),
                'recall': metrics.get('recall'),
                'f1': metrics.get('f1_score'),
                'auc': metrics.get('auc'),
                'confusion_matrix': metrics.get('confusion_matrix', [])
            }
        },
        'training': {
            'train_date_range': f"{train_dates.min().date()} to {train_dates.max().date()}",
            'test_date_range': f"{test_dates.min().date()} to {test_dates.max().date()}",
            'completed_at': datetime.now().isoformat()
        },
        'notes': 'ç”± train_xgboost_timeseries.py è®­ç»ƒ'
    }
    
    metadata_file = f'{version_dir}/metadata.json'
    with open(metadata_file, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, indent=2, ensure_ascii=False)
    log.success(f"  âœ“ å…ƒæ•°æ®: {metadata_file}")
    
    # 5. ä¿å­˜è®­ç»ƒé…ç½®
    if training_config:
        config_file = f'{version_dir}/training_config.yaml'
        with open(config_file, 'w', encoding='utf-8') as f:
            yaml.dump(training_config, f, default_flow_style=False, allow_unicode=True)
        log.success(f"  âœ“ è®­ç»ƒé…ç½®: {config_file}")
    
    # 6. æ›´æ–° current.json
    current_file = f'data/models/{model_name}/current.json'
    if os.path.exists(current_file):
        with open(current_file, 'r', encoding='utf-8') as f:
            current = json.load(f)
    else:
        current = {
            'production': None,
            'staging': None,
            'testing': None,
            'development': None
        }
    
    current['development'] = version
    current['updated_at'] = datetime.now().isoformat()
    
    with open(current_file, 'w', encoding='utf-8') as f:
        json.dump(current, f, indent=2, ensure_ascii=False)
    log.success(f"  âœ“ ç‰ˆæœ¬æŒ‡é’ˆ: {current_file}")
    
    log.info("")
    log.success(f"âœ… ç‰ˆæœ¬ {version} å·²ä¿å­˜åˆ°: {version_dir}")
    log.info("   ä¸‹ä¸€æ­¥å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤æå‡ç‰ˆæœ¬çŠ¶æ€:")
    log.info(f"   python -c \"from src.models.lifecycle import ModelIterator; mi = ModelIterator('{model_name}'); mi.set_current_version('{version}', 'production')\"")


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='XGBoostæ—¶é—´åºåˆ—æ¨¡å‹è®­ç»ƒ')
    parser.add_argument('--use-market-factors', action='store_true', 
                       help='ä½¿ç”¨å¸¦å¸‚åœºå› å­çš„ç‰¹å¾æ–‡ä»¶')
    parser.add_argument('--use-tech-factors', action='store_true',
                       help='ä½¿ç”¨å¸¦æ–°æŠ€æœ¯å› å­çš„v2ç‰¹å¾æ–‡ä»¶')
    parser.add_argument('--use-advanced-factors', action='store_true',
                       help='ä½¿ç”¨å¸¦é«˜çº§æŠ€æœ¯å› å­çš„ç‰¹å¾æ–‡ä»¶')
    # TODO: MA233å› å­æ”¯æŒ (å¾…å®æ–½ï¼Œè§ docs/plans/ma233_feature_plan.md)
    # parser.add_argument('--use-ma233-factors', action='store_true',
    #                    help='ä½¿ç”¨å¸¦MA233å› å­çš„ç‰¹å¾æ–‡ä»¶ï¼ˆåŒ…å«5æ—¥/233æ—¥å‡çº¿çªç ´ç‰¹å¾ï¼‰')
    parser.add_argument('--neg-version', default='v2', choices=['v1', 'v2'],
                       help='è´Ÿæ ·æœ¬ç‰ˆæœ¬')
    # ç‰ˆæœ¬ç®¡ç†å‚æ•°
    parser.add_argument('--version', type=str, default=None,
                       help='æ¨¡å‹ç‰ˆæœ¬å·ï¼ˆå¦‚ v1.5.0ï¼‰ï¼ŒæŒ‡å®šåå°†ä¿å­˜åˆ°ç‰ˆæœ¬ç›®å½•')
    parser.add_argument('--model-name', type=str, default='breakout_launch_scorer',
                       help='æ¨¡å‹åç§°ï¼ˆé»˜è®¤: breakout_launch_scorerï¼‰')
    args = parser.parse_args()
    
    log.info("="*80)
    log.info("XGBoost è‚¡ç¥¨é€‰è‚¡æ¨¡å‹è®­ç»ƒ - æ—¶é—´åºåˆ—ç‰ˆæœ¬")
    log.info("="*80)
    log.info("")
    log.info("âš ï¸  é‡è¦æ”¹è¿›ï¼š")
    log.info("  1. æŒ‰æ—¶é—´åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼ˆè€Œééšæœºåˆ’åˆ†ï¼‰")
    log.info("  2. è®­ç»ƒé›† = å†å²æ•°æ®ï¼Œæµ‹è¯•é›† = æœªæ¥æ•°æ®")
    log.info("  3. é¿å…æœªæ¥å‡½æ•°ï¼Œç¡®ä¿æ— æ•°æ®æ³„éœ²")
    log.info("")
    
    # é…ç½®
    NEG_VERSION = args.neg_version
    USE_ADVANCED_FACTORS = args.use_advanced_factors
    USE_TECH_FACTORS = args.use_tech_factors and not USE_ADVANCED_FACTORS
    USE_MARKET_FACTORS = args.use_market_factors or (not args.use_tech_factors and not USE_ADVANCED_FACTORS)
    # TODO: MA233å› å­æ”¯æŒ (å¾…å®æ–½ï¼Œè§ docs/plans/ma233_feature_plan.md)
    # USE_MA233_FACTORS = args.use_ma233_factors
    
    log.info(f"é…ç½®:")
    log.info(f"  è´Ÿæ ·æœ¬ç‰ˆæœ¬: {NEG_VERSION}")
    log.info(f"  ä½¿ç”¨å¸‚åœºå› å­: {USE_MARKET_FACTORS}")
    log.info(f"  ä½¿ç”¨æ–°æŠ€æœ¯å› å­: {USE_TECH_FACTORS}")
    log.info(f"  ä½¿ç”¨é«˜çº§å› å­: {USE_ADVANCED_FACTORS}")
    log.info(f"  åˆ’åˆ†æ–¹å¼: æ—¶é—´åºåˆ—åˆ’åˆ†ï¼ˆ80%è®­ç»ƒï¼Œ20%æµ‹è¯•ï¼‰")
    log.info(f"  æ¨¡å‹: XGBoost")
    log.info("")
    
    try:
        # ğŸ‘¤ äººå·¥ä»‹å…¥æ£€æŸ¥ï¼šç‰¹å¾é€‰æ‹©
        checker = HumanInterventionChecker()
        feature_check = checker.check_feature_selection()
        checker.print_intervention_reminder("ç‰¹å¾é€‰æ‹©", feature_check)
        
        # 1. åŠ è½½æ•°æ®
        df = load_and_prepare_data(
            neg_version=NEG_VERSION, 
            use_market_factors=USE_MARKET_FACTORS,
            use_tech_factors=USE_TECH_FACTORS,
            use_advanced_factors=USE_ADVANCED_FACTORS
        )
        
        # 2. ç‰¹å¾å·¥ç¨‹ï¼ˆä¿ç•™æ—¶é—´ä¿¡æ¯ï¼‰
        df_features = extract_features_with_time(df)
        
        # ğŸ‘¤ äººå·¥ä»‹å…¥æé†’ï¼šç‰¹å¾æå–å®Œæˆ
        log.warning("\n" + "="*80)
        log.warning("ğŸ‘¤ äººå·¥ä»‹å…¥æé†’ï¼šç‰¹å¾æå–å®Œæˆ")
        log.warning("="*80)
        log.warning(f"å½“å‰ç‰¹å¾æ•°é‡: {len(df_features.columns) - 3} ä¸ªï¼ˆä¸å«sample_id, label, t1_dateï¼‰")
        log.warning("è¯·ç¡®è®¤ï¼š")
        log.warning("  1. ç‰¹å¾æ˜¯å¦è¶³å¤Ÿï¼Ÿæ˜¯å¦éœ€è¦æ·»åŠ åŸºæœ¬é¢ç‰¹å¾æˆ–å…¶ä»–æŠ€æœ¯æŒ‡æ ‡ï¼Ÿ")
        log.warning("  2. ç‰¹å¾æ˜¯å¦é¿å…äº†æœªæ¥å‡½æ•°ï¼Ÿ")
        log.warning("  3. ç‰¹å¾é‡è¦æ€§å°†åœ¨è®­ç»ƒåæ˜¾ç¤ºï¼Œè¯·å…³æ³¨")
        log.warning("="*80)
        
        # 3. æ—¶é—´åºåˆ—åˆ’åˆ†
        X_train, X_test, y_train, y_test, train_dates, test_dates = timeseries_split(
            df_features
        )
        
        # 4. è®­ç»ƒæ¨¡å‹
        model, metrics, y_prob = train_model(X_train, y_train, X_test, y_test)
        
        # 4.5. ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
        generate_training_visualizations(
            model, X_train, df_features, train_dates, test_dates, NEG_VERSION
        )
        
        # ğŸ‘¤ äººå·¥ä»‹å…¥æ£€æŸ¥ï¼šè®­ç»ƒç»“æœ
        log.warning("\n" + "="*80)
        log.warning("ğŸ‘¤ äººå·¥ä»‹å…¥æ£€æŸ¥ï¼šè®­ç»ƒç»“æœ")
        log.warning("="*80)
        
        # æ£€æŸ¥æŒ‡æ ‡æ˜¯å¦è¾¾æ ‡
        warnings = []
        if metrics['auc'] < 0.7:
            warnings.append(f"âš ï¸  AUC = {metrics['auc']:.3f} < 0.7ï¼Œæ¨¡å‹æ€§èƒ½å¯èƒ½ä¸ä½³")
        if metrics['accuracy'] < 0.75:
            warnings.append(f"âš ï¸  å‡†ç¡®ç‡ = {metrics['accuracy']:.2%} < 75%ï¼Œæ¨¡å‹æ€§èƒ½å¯èƒ½ä¸ä½³")
        if metrics['f1_score'] < 0.7:
            warnings.append(f"âš ï¸  F1åˆ†æ•° = {metrics['f1_score']:.2%} < 70%ï¼Œå¯èƒ½å­˜åœ¨è¿‡æ‹Ÿåˆæˆ–æ¬ æ‹Ÿåˆ")
        
        if warnings:
            for warning in warnings:
                log.warning(warning)
            log.warning("\nå»ºè®®ï¼š")
            log.warning("  - æ£€æŸ¥ç‰¹å¾é€‰æ‹©ï¼Œè€ƒè™‘æ·»åŠ æ›´å¤šæœ‰æ•ˆç‰¹å¾")
            log.warning("  - è°ƒæ•´è¶…å‚æ•°ï¼ˆn_estimators, max_depth, learning_rateç­‰ï¼‰")
            log.warning("  - æ£€æŸ¥æ•°æ®è´¨é‡ï¼Œç¡®ä¿æ­£è´Ÿæ ·æœ¬è´¨é‡")
            log.warning("  - è€ƒè™‘å°è¯•å…¶ä»–ç®—æ³•ï¼ˆLightGBM, CatBoostç­‰ï¼‰")
        else:
            log.success("âœ“ æ¨¡å‹æ€§èƒ½æŒ‡æ ‡æ­£å¸¸")
        log.warning("="*80)
        
        # 5. ä¿å­˜æ¨¡å‹
        # æ„å»ºè®­ç»ƒé…ç½®ï¼ˆç”¨äºç‰ˆæœ¬ç®¡ç†ï¼‰
        training_config = {
            'version': args.version,
            'created_at': datetime.now().strftime('%Y-%m-%d'),
            'training_script': 'scripts/train_xgboost_timeseries.py',
            'data': {
                'neg_version': NEG_VERSION,
                'use_market_factors': USE_MARKET_FACTORS,
                'use_tech_factors': USE_TECH_FACTORS,
                'use_advanced_factors': USE_ADVANCED_FACTORS,
                'feature_type': 'advanced' if USE_ADVANCED_FACTORS else ('full' if USE_TECH_FACTORS else ('with_market' if USE_MARKET_FACTORS else 'base'))
            },
            'split': {
                'method': 'time_series',
                'train_ratio': 0.8
            },
            'model_params': {
                'algorithm': 'XGBoost',
                'n_estimators': 100,
                'max_depth': 5,
                'learning_rate': 0.1,
                'subsample': 0.8,
                'colsample_bytree': 0.8
            }
        }
        
        save_model(
            model, metrics, NEG_VERSION, train_dates, test_dates,
            version=args.version,
            model_name=args.model_name,
            feature_names=list(X_train.columns),
            training_config=training_config if args.version else None
        )
        
        # 6. æœ€ç»ˆæ€»ç»“
        log.info("="*80)
        log.success("âœ… æ¨¡å‹è®­ç»ƒå®Œæˆï¼ï¼ˆæ—¶é—´åºåˆ—ç‰ˆæœ¬ï¼‰")
        log.info("="*80)
        log.info("")
        log.info("ğŸ“Š æ¨¡å‹æ€§èƒ½æ€»ç»“:")
        log.info(f"  å‡†ç¡®ç‡ (Accuracy):  {metrics['accuracy']:.2%}")
        log.info(f"  ç²¾ç¡®ç‡ (Precision): {metrics['precision']:.2%}")
        log.info(f"  å¬å›ç‡ (Recall):    {metrics['recall']:.2%}")
        log.info(f"  F1åˆ†æ•° (F1-Score):  {metrics['f1_score']:.2%}")
        log.info(f"  AUC-ROC:            {metrics['auc']:.4f}")
        log.info("")
        log.info("ğŸ¯ å…³é”®æ”¹è¿›:")
        log.info("  âœ“ è®­ç»ƒé›† = å†å²æ•°æ®")
        log.info("  âœ“ æµ‹è¯•é›† = æœªæ¥æ•°æ®ï¼ˆæ¨¡æ‹ŸçœŸå®åœºæ™¯ï¼‰")
        log.info("  âœ“ æ— æœªæ¥å‡½æ•°é£é™©")
        log.info("  âœ“ æ— æ•°æ®æ³„éœ²")
        log.info("")
        log.info("ä¸‹ä¸€æ­¥:")
        log.info("  1. ä½¿ç”¨walk-forwardéªŒè¯è¿›ä¸€æ­¥æµ‹è¯•")
        log.info("  2. åœ¨å¤šä¸ªæ—¶é—´çª—å£ä¸ŠéªŒè¯ç¨³å®šæ€§")
        log.info("  3. å›æµ‹éªŒè¯å®é™…æ”¶ç›Š")
        log.info("")
        
    except FileNotFoundError as e:
        log.error(f"âœ— æ–‡ä»¶æœªæ‰¾åˆ°: {e}")
        log.error("è¯·å…ˆè¿è¡Œä»¥ä¸‹å‘½ä»¤å‡†å¤‡æ•°æ®:")
        log.error("  1. python scripts/prepare_positive_samples.py")
        log.error("  2. python scripts/prepare_negative_samples_v2.py")
    except Exception as e:
        log.error(f"âœ— è®­ç»ƒè¿‡ç¨‹å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()


if __name__ == '__main__':
    main()

