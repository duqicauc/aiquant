"""
æ­£æ ·æœ¬æ•°æ®è´¨é‡æ ¸æŸ¥å·¥å…·

åŠŸèƒ½ï¼š
1. æ•°æ®å®Œæ•´æ€§æ£€æŸ¥
2. æ•°æ®ä¸€è‡´æ€§éªŒè¯
3. æ¶¨å¹…è®¡ç®—éªŒè¯
4. å¯è§†åŒ–åˆ†æ
5. å¼‚å¸¸æ ·æœ¬æ£€æµ‹
"""
import sys
from pathlib import Path

PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

import pandas as pd
import numpy as np
from datetime import datetime
from src.utils.logger import log
from src.visualization.training_visualizer import TrainingVisualizer


class SampleQualityChecker:
    """æ•°æ®è´¨é‡æ ¸æŸ¥å™¨"""
    
    def __init__(self, samples_file: str, features_file: str = None):
        """
        åˆå§‹åŒ–
        
        Args:
            samples_file: æ­£æ ·æœ¬æ–‡ä»¶è·¯å¾„
            features_file: ç‰¹å¾æ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
        """
        self.samples_file = Path(samples_file)
        self.features_file = Path(features_file) if features_file else None
        
        self.df_samples = None
        self.df_features = None
        
        self._load_data()
    
    def _load_data(self):
        """åŠ è½½æ•°æ®"""
        log.info("="*80)
        log.info("åŠ è½½æ•°æ®")
        log.info("="*80)
        
        # åŠ è½½æ­£æ ·æœ¬
        if not self.samples_file.exists():
            log.error(f"æ­£æ ·æœ¬æ–‡ä»¶ä¸å­˜åœ¨: {self.samples_file}")
            return
        
        self.df_samples = pd.read_csv(self.samples_file)
        log.success(f"âœ“ æ­£æ ·æœ¬åŠ è½½æˆåŠŸ: {len(self.df_samples)} æ¡")
        
        # åŠ è½½ç‰¹å¾æ•°æ®
        if self.features_file and self.features_file.exists():
            self.df_features = pd.read_csv(self.features_file)
            log.success(f"âœ“ ç‰¹å¾æ•°æ®åŠ è½½æˆåŠŸ: {len(self.df_features)} æ¡")
        elif self.features_file:
            log.warning(f"ç‰¹å¾æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {self.features_file}")
    
    def check_all(self):
        """æ‰§è¡Œæ‰€æœ‰æ£€æŸ¥"""
        if self.df_samples is None:
            log.error("æ•°æ®æœªåŠ è½½ï¼Œæ— æ³•æ£€æŸ¥")
            return
        
        log.info("\n" + "="*80)
        log.info("å¼€å§‹æ•°æ®è´¨é‡æ ¸æŸ¥")
        log.info("="*80)
        
        # 1. åŸºç¡€ç»Ÿè®¡
        self.check_basic_stats()
        
        # 2. æ•°æ®å®Œæ•´æ€§
        self.check_completeness()
        
        # 3. æ•°æ®ä¸€è‡´æ€§
        self.check_consistency()
        
        # 4. æ¶¨å¹…éªŒè¯
        self.check_returns()
        
        # 5. å¼‚å¸¸æ£€æµ‹
        self.check_anomalies()
        
        # 6. å»é‡æ£€æŸ¥
        self.check_duplicates()
        
        # 7. æ—¥æœŸæ£€æŸ¥
        self.check_dates()
        
        # 8. ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
        self.generate_visualizations()
        
        # ç”Ÿæˆæ€»ç»“
        self.generate_summary()
    
    def check_basic_stats(self):
        """åŸºç¡€ç»Ÿè®¡"""
        log.info("\n" + "="*80)
        log.info("ã€1ã€‘åŸºç¡€ç»Ÿè®¡")
        log.info("="*80)
        
        df = self.df_samples
        
        log.info(f"æ ·æœ¬æ€»æ•°: {len(df)}")
        log.info(f"è‚¡ç¥¨æ•°é‡: {df['ts_code'].nunique()}")
        log.info(f"è‚¡ç¥¨åç§°æ•°: {df['name'].nunique()}")
        
        # æ¶¨å¹…ç»Ÿè®¡
        log.info(f"\næ¶¨å¹…ç»Ÿè®¡:")
        log.info(f"  æ€»æ¶¨å¹… - å¹³å‡: {df['total_return'].mean():.2f}%")
        log.info(f"  æ€»æ¶¨å¹… - ä¸­ä½æ•°: {df['total_return'].median():.2f}%")
        log.info(f"  æ€»æ¶¨å¹… - æœ€å°: {df['total_return'].min():.2f}%")
        log.info(f"  æ€»æ¶¨å¹… - æœ€å¤§: {df['total_return'].max():.2f}%")
        
        log.info(f"\n  æœ€é«˜æ¶¨å¹… - å¹³å‡: {df['max_return'].mean():.2f}%")
        log.info(f"  æœ€é«˜æ¶¨å¹… - ä¸­ä½æ•°: {df['max_return'].median():.2f}%")
        log.info(f"  æœ€é«˜æ¶¨å¹… - æœ€å°: {df['max_return'].min():.2f}%")
        log.info(f"  æœ€é«˜æ¶¨å¹… - æœ€å¤§: {df['max_return'].max():.2f}%")
        
        # æ—¶é—´ç»Ÿè®¡
        if 't1_date' in df.columns:
            df['t1_date'] = pd.to_datetime(df['t1_date'])
            log.info(f"\nT1æ—¥æœŸèŒƒå›´:")
            log.info(f"  æœ€æ—©: {df['t1_date'].min()}")
            log.info(f"  æœ€æ™š: {df['t1_date'].max()}")
    
    def check_completeness(self):
        """æ•°æ®å®Œæ•´æ€§æ£€æŸ¥"""
        log.info("\n" + "="*80)
        log.info("ã€2ã€‘æ•°æ®å®Œæ•´æ€§æ£€æŸ¥")
        log.info("="*80)
        
        df = self.df_samples
        
        # æ£€æŸ¥å¿…éœ€å­—æ®µ
        required_fields = ['ts_code', 'name', 't1_date', 'total_return', 'max_return']
        missing_fields = [f for f in required_fields if f not in df.columns]
        
        if missing_fields:
            log.error(f"âœ— ç¼ºå°‘å¿…éœ€å­—æ®µ: {missing_fields}")
        else:
            log.success("âœ“ æ‰€æœ‰å¿…éœ€å­—æ®µéƒ½å­˜åœ¨")
        
        # æ£€æŸ¥ç©ºå€¼
        null_counts = df.isnull().sum()
        null_fields = null_counts[null_counts > 0]
        
        if len(null_fields) > 0:
            log.warning(f"å‘ç°ç©ºå€¼:")
            for field, count in null_fields.items():
                log.warning(f"  {field}: {count} ä¸ªç©ºå€¼ ({count/len(df)*100:.2f}%)")
        else:
            log.success("âœ“ æ²¡æœ‰ç©ºå€¼")
    
    def check_consistency(self):
        """æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥"""
        log.info("\n" + "="*80)
        log.info("ã€3ã€‘æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥")
        log.info("="*80)
        
        df = self.df_samples
        
        # æ£€æŸ¥æ€»æ¶¨å¹…å’Œæœ€é«˜æ¶¨å¹…çš„å…³ç³»
        invalid = df[df['total_return'] > df['max_return']]
        if len(invalid) > 0:
            log.error(f"âœ— å‘ç° {len(invalid)} ä¸ªæ ·æœ¬çš„æ€»æ¶¨å¹… > æœ€é«˜æ¶¨å¹…ï¼ˆä¸åˆç†ï¼‰")
            print(invalid[['ts_code', 'name', 'total_return', 'max_return']])
        else:
            log.success("âœ“ æ€»æ¶¨å¹…å’Œæœ€é«˜æ¶¨å¹…å…³ç³»æ­£ç¡®")
        
        # æ£€æŸ¥æ¶¨å¹…èŒƒå›´
        min_total = df['total_return'].min()
        min_max = df['max_return'].min()
        
        if min_total < 50:
            log.warning(f"âš ï¸  å‘ç°æ€»æ¶¨å¹… < 50%çš„æ ·æœ¬ (æœ€å°: {min_total:.2f}%)")
        else:
            log.success("âœ“ æ‰€æœ‰æ ·æœ¬æ€»æ¶¨å¹… >= 50%")
        
        if min_max < 70:
            log.warning(f"âš ï¸  å‘ç°æœ€é«˜æ¶¨å¹… < 70%çš„æ ·æœ¬ (æœ€å°: {min_max:.2f}%)")
        else:
            log.success("âœ“ æ‰€æœ‰æ ·æœ¬æœ€é«˜æ¶¨å¹… >= 70%")
    
    def check_returns(self):
        """æ¶¨å¹…è®¡ç®—éªŒè¯"""
        log.info("\n" + "="*80)
        log.info("ã€4ã€‘æ¶¨å¹…è®¡ç®—éªŒè¯")
        log.info("="*80)
        
        df = self.df_samples
        
        # éªŒè¯æ¶¨å¹…è®¡ç®—ï¼ˆå¦‚æœæœ‰å¼€ç›˜ä»·å’Œæ”¶ç›˜ä»·ï¼‰
        if 'week1_open' in df.columns and 'week3_close' in df.columns:
            # é‡æ–°è®¡ç®—æ€»æ¶¨å¹…
            df['calculated_return'] = (df['week3_close'] - df['week1_open']) / df['week1_open'] * 100
            
            # æ¯”è¾ƒ
            diff = abs(df['total_return'] - df['calculated_return'])
            max_diff = diff.max()
            
            if max_diff > 0.1:  # å…è®¸0.1%çš„è¯¯å·®
                problematic = df[diff > 0.1]
                log.warning(f"âš ï¸  å‘ç° {len(problematic)} ä¸ªæ ·æœ¬æ¶¨å¹…è®¡ç®—å¯èƒ½æœ‰è¯¯å·®")
                print(problematic[['ts_code', 'name', 'total_return', 'calculated_return']].head())
            else:
                log.success("âœ“ æ¶¨å¹…è®¡ç®—æ­£ç¡®")
        else:
            log.info("æ²¡æœ‰è¶³å¤Ÿçš„ä»·æ ¼æ•°æ®è¿›è¡Œæ¶¨å¹…éªŒè¯")
    
    def check_anomalies(self):
        """å¼‚å¸¸æ£€æµ‹"""
        log.info("\n" + "="*80)
        log.info("ã€5ã€‘å¼‚å¸¸å€¼æ£€æµ‹")
        log.info("="*80)
        
        df = self.df_samples
        
        # ä½¿ç”¨IQRæ–¹æ³•æ£€æµ‹å¼‚å¸¸æ¶¨å¹…
        Q1 = df['total_return'].quantile(0.25)
        Q3 = df['total_return'].quantile(0.75)
        IQR = Q3 - Q1
        
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        
        outliers = df[(df['total_return'] < lower_bound) | (df['total_return'] > upper_bound)]
        
        if len(outliers) > 0:
            log.info(f"å‘ç° {len(outliers)} ä¸ªæ€»æ¶¨å¹…å¼‚å¸¸å€¼ï¼ˆIQRæ–¹æ³•ï¼‰:")
            for _, row in outliers.iterrows():
                log.info(f"  {row['ts_code']} {row['name']}: {row['total_return']:.2f}%")
        else:
            log.success("âœ“ æ²¡æœ‰æ£€æµ‹åˆ°æ˜æ˜¾çš„æ¶¨å¹…å¼‚å¸¸å€¼")
        
        # æ£€æµ‹æç«¯å€¼ï¼ˆè¶…è¿‡200%ï¼‰
        extreme = df[df['total_return'] > 200]
        if len(extreme) > 0:
            log.warning(f"âš ï¸  å‘ç° {len(extreme)} ä¸ªæç«¯æ¶¨å¹…æ ·æœ¬ï¼ˆ>200%ï¼‰:")
            print(extreme[['ts_code', 'name', 'total_return', 'max_return']])
        else:
            log.info("æ²¡æœ‰æç«¯æ¶¨å¹…æ ·æœ¬")
    
    def check_duplicates(self):
        """å»é‡æ£€æŸ¥"""
        log.info("\n" + "="*80)
        log.info("ã€6ã€‘é‡å¤æ•°æ®æ£€æŸ¥")
        log.info("="*80)
        
        df = self.df_samples
        
        # æ£€æŸ¥å®Œå…¨é‡å¤çš„è®°å½•
        duplicates = df[df.duplicated(keep=False)]
        if len(duplicates) > 0:
            log.warning(f"âš ï¸  å‘ç° {len(duplicates)} æ¡å®Œå…¨é‡å¤çš„è®°å½•")
            print(duplicates)
        else:
            log.success("âœ“ æ²¡æœ‰å®Œå…¨é‡å¤çš„è®°å½•")
        
        # æ£€æŸ¥åŒä¸€è‚¡ç¥¨å¤šä¸ªT1æ—¥æœŸ
        stock_counts = df['ts_code'].value_counts()
        multi_samples = stock_counts[stock_counts > 1]
        
        if len(multi_samples) > 0:
            log.info(f"å‘ç° {len(multi_samples)} åªè‚¡ç¥¨æœ‰å¤šä¸ªæ ·æœ¬:")
            for ts_code, count in multi_samples.items():
                name = df[df['ts_code'] == ts_code]['name'].iloc[0]
                log.info(f"  {ts_code} {name}: {count} ä¸ªæ ·æœ¬")
            
            log.info("\nâš ï¸  æ³¨æ„: æŒ‰è§„åˆ™åº”è¯¥æ¯åªè‚¡ç¥¨åªä¿ç•™æœ€æ—©çš„æ ·æœ¬")
        else:
            log.success("âœ“ æ¯åªè‚¡ç¥¨åªæœ‰ä¸€ä¸ªæ ·æœ¬ï¼ˆç¬¦åˆå»é‡è§„åˆ™ï¼‰")
    
    def check_dates(self):
        """æ—¥æœŸæ£€æŸ¥"""
        log.info("\n" + "="*80)
        log.info("ã€7ã€‘æ—¥æœŸåˆç†æ€§æ£€æŸ¥")
        log.info("="*80)
        
        df = self.df_samples
        
        if 't1_date' not in df.columns:
            log.warning("æ²¡æœ‰T1æ—¥æœŸæ•°æ®")
            return
        
        df['t1_date'] = pd.to_datetime(df['t1_date'])
        
        # æ£€æŸ¥æœªæ¥æ—¥æœŸ
        today = pd.Timestamp.now()
        future_dates = df[df['t1_date'] > today]
        
        if len(future_dates) > 0:
            log.error(f"âœ— å‘ç° {len(future_dates)} ä¸ªæœªæ¥æ—¥æœŸï¼ˆé”™è¯¯ï¼‰")
            print(future_dates[['ts_code', 'name', 't1_date']])
        else:
            log.success("âœ“ æ²¡æœ‰æœªæ¥æ—¥æœŸ")
        
        # æ£€æŸ¥è¿‡äºä¹…è¿œçš„æ—¥æœŸ
        very_old = df[df['t1_date'] < pd.Timestamp('2000-01-01')]
        if len(very_old) > 0:
            log.warning(f"âš ï¸  å‘ç° {len(very_old)} ä¸ª2000å¹´ä¹‹å‰çš„æ ·æœ¬")
        
        # æ£€æŸ¥æ—¥æœŸåˆ†å¸ƒ
        df['year'] = df['t1_date'].dt.year
        year_counts = df['year'].value_counts().sort_index()
        
        log.info(f"\nT1æ—¥æœŸå¹´ä»½åˆ†å¸ƒ:")
        for year, count in year_counts.items():
            log.info(f"  {year}å¹´: {count} ä¸ªæ ·æœ¬")
    
    def generate_summary(self):
        """ç”Ÿæˆæ£€æŸ¥æ€»ç»“"""
        log.info("\n" + "="*80)
        log.info("æ•°æ®è´¨é‡æ ¸æŸ¥æ€»ç»“")
        log.info("="*80)
        
        df = self.df_samples
        
        # è®¡ç®—è´¨é‡å¾—åˆ†
        issues = []
        
        # 1. å®Œæ•´æ€§æ£€æŸ¥
        if df.isnull().sum().sum() > 0:
            issues.append("å­˜åœ¨ç©ºå€¼")
        
        # 2. ä¸€è‡´æ€§æ£€æŸ¥
        if len(df[df['total_return'] > df['max_return']]) > 0:
            issues.append("æ¶¨å¹…é€»è¾‘é”™è¯¯")
        
        if df['total_return'].min() < 50 or df['max_return'].min() < 70:
            issues.append("ä¸æ»¡è¶³æ¶¨å¹…æ¡ä»¶")
        
        # 3. é‡å¤æ£€æŸ¥
        if len(df[df.duplicated()]) > 0:
            issues.append("å­˜åœ¨é‡å¤è®°å½•")
        
        # 4. æ—¥æœŸæ£€æŸ¥
        if 't1_date' in df.columns:
            df['t1_date'] = pd.to_datetime(df['t1_date'])
            if len(df[df['t1_date'] > pd.Timestamp.now()]) > 0:
                issues.append("å­˜åœ¨æœªæ¥æ—¥æœŸ")
        
        # æ€»ç»“
        if len(issues) == 0:
            log.success("\nâœ… æ•°æ®è´¨é‡è‰¯å¥½ï¼æœªå‘ç°é‡å¤§é—®é¢˜")
            quality_score = 100
        else:
            log.warning(f"\nâš ï¸  å‘ç° {len(issues)} ä¸ªé—®é¢˜:")
            for issue in issues:
                log.warning(f"  - {issue}")
            quality_score = max(0, 100 - len(issues) * 15)
        
        log.info(f"\næ•°æ®è´¨é‡è¯„åˆ†: {quality_score}/100")
        
        if quality_score >= 85:
            log.success("è¯„çº§: ä¼˜ç§€ â­â­â­â­â­")
        elif quality_score >= 70:
            log.info("è¯„çº§: è‰¯å¥½ â­â­â­â­")
        elif quality_score >= 60:
            log.warning("è¯„çº§: ä¸­ç­‰ â­â­â­")
        else:
            log.error("è¯„çº§: éœ€è¦æ”¹è¿› â­â­")
    
    def generate_visualizations(self):
        """ç”Ÿæˆæ ·æœ¬è´¨é‡å¯è§†åŒ–å›¾è¡¨"""
        if self.df_samples is None:
            return
        
        try:
            log.info("\n" + "="*80)
            log.info("ç”Ÿæˆæ ·æœ¬è´¨é‡å¯è§†åŒ–å›¾è¡¨")
            log.info("="*80)
            
            # ç¡®å®šè¾“å‡ºç›®å½•
            output_dir = PROJECT_ROOT / 'data' / 'training' / 'charts'
            visualizer = TrainingVisualizer(output_dir=str(output_dir))
            
            # ç”Ÿæˆå¯è§†åŒ–
            visualizer.visualize_sample_quality(
                self.df_samples,
                save_prefix="sample_quality_check"
            )
            
            # ç”Ÿæˆç´¢å¼•é¡µé¢
            visualizer.generate_index_page(model_name="sample_quality_check")
            
            log.success(f"âœ“ å¯è§†åŒ–å›¾è¡¨å·²ç”Ÿæˆåˆ°: {output_dir}")
            log.info(f"ğŸ“Š æŸ¥çœ‹å›¾è¡¨: open {output_dir}/index.html")
            
        except Exception as e:
            log.warning(f"ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨æ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()


def main():
    """ä¸»å‡½æ•°"""
    log.info("="*80)
    log.info("æ­£æ ·æœ¬æ•°æ®è´¨é‡æ ¸æŸ¥å·¥å…·")
    log.info("="*80)
    
    # æ–‡ä»¶è·¯å¾„ï¼ˆä½¿ç”¨æ–°çš„ç›®å½•ç»“æ„ï¼‰
    samples_file = PROJECT_ROOT / 'data' / 'training' / 'samples' / 'positive_samples.csv'
    features_file = PROJECT_ROOT / 'data' / 'training' / 'features' / 'feature_data_34d.csv'
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not samples_file.exists():
        log.error(f"æ­£æ ·æœ¬æ–‡ä»¶ä¸å­˜åœ¨: {samples_file}")
        log.info("è¯·å…ˆè¿è¡Œ scripts/prepare_positive_samples.py ç”Ÿæˆæ•°æ®")
        return
    
    # åˆ›å»ºæ£€æŸ¥å™¨
    checker = SampleQualityChecker(samples_file, features_file)
    
    # æ‰§è¡Œæ£€æŸ¥
    checker.check_all()
    
    log.info("\n" + "="*80)
    log.info("æ ¸æŸ¥å®Œæˆï¼")
    log.info("="*80)


if __name__ == '__main__':
    main()
