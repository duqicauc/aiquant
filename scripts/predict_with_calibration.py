#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
æ ¡å‡†æ¨¡å‹é¢„æµ‹è„šæœ¬

ä½¿ç”¨æ¦‚ç‡æ ¡å‡†åçš„æ¨¡å‹è¿›è¡Œé¢„æµ‹ï¼Œå¹¶åº”ç”¨é£é™©è¿‡æ»¤

ä½¿ç”¨æ–¹æ³•ï¼š
    python scripts/predict_with_calibration.py --predict-date 20251212 --eval-date 20251231
"""

import sys
import json
import pickle
import argparse
import warnings
from pathlib import Path
from datetime import datetime, timedelta

import pandas as pd
import numpy as np
import xgboost as xgb

PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

warnings.filterwarnings('ignore')

from src.utils.logger import log
from src.data.data_manager import DataManager


# é£é™©è¿‡æ»¤é˜ˆå€¼
RISK_THRESHOLD = 0.7


def load_calibrated_model(version='v2.2.0'):
    """åŠ è½½æ ¡å‡†åçš„æ¨¡å‹"""
    model_dir = PROJECT_ROOT / 'data' / 'models' / 'breakout_launch_scorer' / 'versions' / version / 'model'
    
    # åŠ è½½æ ¡å‡†æ¨¡å‹
    calibrated_model_file = model_dir / 'calibrated_model.pkl'
    if calibrated_model_file.exists():
        with open(calibrated_model_file, 'rb') as f:
            calibrated_model = pickle.load(f)
        log.success(f"âœ“ æ ¡å‡†æ¨¡å‹åŠ è½½æˆåŠŸ: {version}")
    else:
        # å¦‚æœæ²¡æœ‰æ ¡å‡†æ¨¡å‹ï¼Œå›é€€åˆ°åŸºç¡€æ¨¡å‹
        log.warning(f"æœªæ‰¾åˆ°æ ¡å‡†æ¨¡å‹ï¼Œä½¿ç”¨åŸºç¡€æ¨¡å‹")
        booster = xgb.Booster()
        booster.load_model(str(model_dir / 'model.json'))
        calibrated_model = None
    
    # åŠ è½½ç‰¹å¾åç§°
    with open(model_dir / 'feature_names.json', 'r') as f:
        feature_names = json.load(f)
    
    return calibrated_model, feature_names


def load_base_model(version='v2.1.0'):
    """åŠ è½½åŸºç¡€æ¨¡å‹ï¼ˆç”¨äºå¯¹æ¯”ï¼‰"""
    model_dir = PROJECT_ROOT / 'data' / 'models' / 'breakout_launch_scorer' / 'versions' / version / 'model'
    
    booster = xgb.Booster()
    booster.load_model(str(model_dir / 'model.json'))
    
    with open(model_dir / 'feature_names.json', 'r') as f:
        feature_names = json.load(f)
    
    return booster, feature_names


def get_valid_stock_list(dm):
    """è·å–æœ‰æ•ˆè‚¡ç¥¨åˆ—è¡¨"""
    stock_list = dm.get_stock_list()
    
    valid = stock_list[
        ~stock_list['name'].str.contains('ST|é€€', na=False) &
        ~stock_list['ts_code'].str.startswith('688') &
        ~stock_list['ts_code'].str.startswith('8')
    ]
    
    return valid


def get_stock_features(dm, ts_code, predict_date, feature_names):
    """è·å–å•åªè‚¡ç¥¨çš„ç‰¹å¾"""
    try:
        end_date = predict_date
        start_date = (datetime.strptime(predict_date, '%Y%m%d') - timedelta(days=200)).strftime('%Y%m%d')
        
        df = dm.get_daily_data(ts_code, start_date, end_date)
        if df is None or len(df) < 60:
            return None, None
        
        df = df.sort_values('trade_date').reset_index(drop=True)
        
        # è®¡ç®—å„ç§æŠ€æœ¯æŒ‡æ ‡ï¼ˆä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´ï¼‰
        # MA
        df['ma5'] = df['close'].rolling(5).mean()
        df['ma10'] = df['close'].rolling(10).mean()
        df['ma_20d'] = df['close'].rolling(20).mean()
        
        # MACD
        df['ema12'] = df['close'].ewm(span=12, adjust=False).mean()
        df['ema26'] = df['close'].ewm(span=26, adjust=False).mean()
        df['macd_dif'] = df['ema12'] - df['ema26']
        df['macd_dea'] = df['macd_dif'].ewm(span=9, adjust=False).mean()
        df['macd'] = 2 * (df['macd_dif'] - df['macd_dea'])
        
        # RSI
        delta = df['close'].diff()
        gain = delta.where(delta > 0, 0).rolling(6).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(6).mean()
        df['rsi_6'] = 100 - (100 / (1 + gain / (loss + 1e-10)))
        
        gain14 = delta.where(delta > 0, 0).rolling(14).mean()
        loss14 = (-delta.where(delta < 0, 0)).rolling(14).mean()
        df['rsi_14'] = 100 - (100 / (1 + gain14 / (loss14 + 1e-10)))
        
        # å¤šå‘¨æœŸç‰¹å¾
        for period in [8, 34, 55]:
            df[f'return_{period}d'] = df['close'].pct_change(period) * 100
            df[f'ma_{period}d'] = df['close'].rolling(period).mean()
            df[f'price_vs_ma_{period}d'] = (df['close'] - df[f'ma_{period}d']) / df[f'ma_{period}d'] * 100
            df[f'volatility_{period}d'] = df['pct_chg'].rolling(period).std()
            df[f'high_{period}d'] = df['high'].rolling(period).max()
            df[f'low_{period}d'] = df['low'].rolling(period).min()
        
        # é‡ä»·ç‰¹å¾
        df['volume_ratio'] = df['vol'] / (df['vol'].rolling(5).mean() + 1e-8)
        df['volume_price_corr_10d'] = df['close'].rolling(10).corr(df['vol'])
        
        # OBV
        df['obv'] = (np.sign(df['close'].diff()) * df['vol']).fillna(0).cumsum()
        
        # çªç ´ç‰¹å¾
        for period in [10, 20, 55]:
            df[f'breakout_high_{period}d'] = (df['close'] > df['high'].rolling(period).max().shift(1)).astype(int)
        
        # å–æœ€åä¸€è¡Œä½œä¸ºç‰¹å¾
        last_row = df.iloc[-1]
        
        features = {}
        for fn in feature_names:
            if fn in last_row:
                val = last_row[fn]
                features[fn] = 0 if pd.isna(val) else val
            else:
                features[fn] = 0
        
        # è®¡ç®—é£é™©æŒ‡æ ‡
        risk_metrics = calculate_risk_metrics(df)
        
        return features, risk_metrics
        
    except Exception as e:
        return None, None


def calculate_risk_metrics(df):
    """è®¡ç®—é£é™©æŒ‡æ ‡"""
    if df is None or len(df) < 34:
        return None
    
    # 34æ—¥æ¶¨å¹…
    return_34d = (df['close'].iloc[-1] / df['close'].iloc[-34] - 1) * 100 if len(df) >= 34 else 0
    
    # RSI
    delta = df['close'].diff()
    gain = delta.where(delta > 0, 0).rolling(14).mean()
    loss = (-delta.where(delta < 0, 0)).rolling(14).mean()
    rsi = 100 - (100 / (1 + gain / (loss + 1e-10)))
    rsi_14 = rsi.iloc[-1] if not pd.isna(rsi.iloc[-1]) else 50
    
    # æ³¢åŠ¨ç‡
    volatility = df['pct_chg'].std()
    vol_mean = df['pct_chg'].rolling(20).std().mean()
    
    # è¿‘5æ—¥ä¸‹è·Œ
    consecutive_down = (df['pct_chg'].tail(5) < 0).sum()
    
    # è¿‘æœŸæ¶¨åœæ¬¡æ•°
    limit_up_count = (df['pct_chg'].tail(10) >= 9.8).sum()
    
    return {
        'return_34d': return_34d,
        'rsi_14': rsi_14,
        'volatility': volatility,
        'volatility_mean': vol_mean if not pd.isna(vol_mean) else volatility,
        'consecutive_down': consecutive_down,
        'limit_up_count': limit_up_count
    }


def calculate_risk_score(risk_metrics):
    """è®¡ç®—é£é™©ç³»æ•° (0-1)ï¼Œç³»æ•°è¶Šé«˜é£é™©è¶Šä½"""
    if risk_metrics is None:
        return 0.5, []
    
    risk_score = 1.0
    risk_reasons = []
    
    # è§„åˆ™1: 34æ—¥æ¶¨å¹…
    return_34d = risk_metrics.get('return_34d', 0)
    if return_34d > 80:
        risk_score *= 0.3
        risk_reasons.append(f'34æ—¥æ¶¨å¹…è¿‡å¤§({return_34d:.1f}%)')
    elif return_34d > 60:
        risk_score *= 0.5
        risk_reasons.append(f'34æ—¥æ¶¨å¹…è¾ƒå¤§({return_34d:.1f}%)')
    elif return_34d > 40:
        risk_score *= 0.7
        risk_reasons.append(f'34æ—¥æ¶¨å¹…åé«˜({return_34d:.1f}%)')
    
    # è§„åˆ™2: æ³¢åŠ¨ç‡
    volatility = risk_metrics.get('volatility', 0)
    vol_mean = risk_metrics.get('volatility_mean', volatility)
    if vol_mean > 0 and volatility > vol_mean * 2.5:
        risk_score *= 0.5
        risk_reasons.append('æ³¢åŠ¨ç‡è¿‡é«˜')
    elif vol_mean > 0 and volatility > vol_mean * 2:
        risk_score *= 0.7
        risk_reasons.append('æ³¢åŠ¨ç‡åé«˜')
    
    # è§„åˆ™3: è¿‘5æ—¥è¿ç»­ä¸‹è·Œ
    consecutive_down = risk_metrics.get('consecutive_down', 0)
    if consecutive_down >= 5:
        risk_score *= 0.4
        risk_reasons.append('è¿ç»­5æ—¥ä¸‹è·Œ')
    elif consecutive_down >= 4:
        risk_score *= 0.6
        risk_reasons.append('è¿‘5æ—¥å¤šæ•°ä¸‹è·Œ')
    
    # è§„åˆ™4: RSIè¶…ä¹°
    rsi = risk_metrics.get('rsi_14', 50)
    if rsi > 85:
        risk_score *= 0.5
        risk_reasons.append(f'RSIè¶…ä¹°({rsi:.1f})')
    elif rsi > 75:
        risk_score *= 0.7
        risk_reasons.append(f'RSIåé«˜({rsi:.1f})')
    
    # è§„åˆ™5: è¿‘æœŸæ¶¨åœ
    limit_up_count = risk_metrics.get('limit_up_count', 0)
    if limit_up_count >= 3:
        risk_score *= 0.5
        risk_reasons.append(f'è¿‘æœŸå¤šæ¬¡æ¶¨åœ({limit_up_count}æ¬¡)')
    elif limit_up_count >= 2:
        risk_score *= 0.7
        risk_reasons.append(f'è¿‘æœŸæ¶¨åœ({limit_up_count}æ¬¡)')
    
    return risk_score, risk_reasons


def predict_with_model(dm, stock_list, model, feature_names, predict_date, use_calibrated=True):
    """ä½¿ç”¨æ¨¡å‹é¢„æµ‹"""
    log.info(f"\nå¼€å§‹é¢„æµ‹...")
    
    results = []
    total = len(stock_list)
    
    for idx, (_, row) in enumerate(stock_list.iterrows()):
        ts_code = row['ts_code']
        name = row['name']
        
        if (idx + 1) % 500 == 0:
            log.info(f"è¿›åº¦: {idx+1}/{total} | å·²è¯„åˆ†: {len(results)}")
        
        try:
            # è·å–ç‰¹å¾å’Œé£é™©æŒ‡æ ‡
            features, risk_metrics = get_stock_features(dm, ts_code, predict_date, feature_names)
            if features is None:
                continue
            
            # è®¡ç®—é£é™©ç³»æ•°
            risk_score, risk_reasons = calculate_risk_score(risk_metrics)
            
            # é¢„æµ‹
            feature_vector = [features.get(fn, 0) for fn in feature_names]
            feature_df = pd.DataFrame([feature_vector], columns=feature_names)
            
            if use_calibrated and hasattr(model, 'predict_proba'):
                # æ ¡å‡†æ¨¡å‹
                prob = model.predict_proba(feature_df)[0, 1]
            else:
                # åŸºç¡€æ¨¡å‹
                dmatrix = xgb.DMatrix(feature_df, feature_names=feature_names)
                prob = model.predict(dmatrix)[0]
            
            results.append({
                'ts_code': ts_code,
                'name': name,
                'probability': prob,
                'risk_score': risk_score,
                'adjusted_prob': prob * risk_score,  # é£é™©è°ƒæ•´åæ¦‚ç‡
                'return_34d': risk_metrics.get('return_34d', 0) if risk_metrics else 0,
                'rsi_14': risk_metrics.get('rsi_14', 50) if risk_metrics else 50,
                'risk_reasons': '; '.join(risk_reasons) if risk_reasons else ''
            })
            
        except Exception as e:
            continue
    
    df_results = pd.DataFrame(results)
    df_results = df_results.sort_values('adjusted_prob', ascending=False)
    
    log.success(f"âœ“ é¢„æµ‹å®Œæˆ: {len(df_results)} åªè‚¡ç¥¨")
    
    return df_results


def evaluate_predictions(dm, df_predictions, eval_date):
    """è¯„ä¼°é¢„æµ‹ç»“æœ"""
    log.info(f"\nè¯„ä¼°é¢„æµ‹ç»“æœ (è¯„ä¼°æ—¥æœŸ: {eval_date})...")
    
    results = []
    
    for idx, row in df_predictions.iterrows():
        ts_code = row['ts_code']
        
        # è·å–è¯„ä¼°æ—¥çš„ä»·æ ¼
        eval_start = (datetime.strptime(eval_date, '%Y%m%d') - timedelta(days=10)).strftime('%Y%m%d')
        eval_end = (datetime.strptime(eval_date, '%Y%m%d') + timedelta(days=5)).strftime('%Y%m%d')
        
        df_eval = dm.get_daily_data(ts_code, eval_start, eval_end)
        if df_eval is None or len(df_eval) == 0:
            continue
        
        # è·å–é¢„æµ‹æ—¥ä»·æ ¼
        predict_start = (datetime.strptime(df_predictions['predict_date'].iloc[0] if 'predict_date' in df_predictions.columns else '20251212', '%Y%m%d') - timedelta(days=5)).strftime('%Y%m%d')
        predict_end = df_predictions['predict_date'].iloc[0] if 'predict_date' in df_predictions.columns else '20251212'
        
        df_pred = dm.get_daily_data(ts_code, predict_start, predict_end)
        if df_pred is None or len(df_pred) == 0:
            continue
        
        predict_price = df_pred.iloc[-1]['close']
        eval_price = df_eval.iloc[-1]['close']
        
        return_pct = (eval_price / predict_price - 1) * 100
        
        result = row.to_dict()
        result['predict_price'] = predict_price
        result['eval_price'] = eval_price
        result['return_pct'] = return_pct
        
        results.append(result)
    
    return pd.DataFrame(results)


def print_summary(df_eval, title):
    """æ‰“å°è¯„ä¼°æ‘˜è¦"""
    log.info("="*80)
    log.info(title)
    log.info("="*80)
    
    if len(df_eval) == 0:
        log.warning("æ— æœ‰æ•ˆè¯„ä¼°æ•°æ®")
        return {}
    
    avg_return = df_eval['return_pct'].mean()
    median_return = df_eval['return_pct'].median()
    win_rate = (df_eval['return_pct'] > 0).mean() * 100
    max_return = df_eval['return_pct'].max()
    min_return = df_eval['return_pct'].min()
    
    log.info(f"\nğŸ“Š æ•´ä½“ç»Ÿè®¡ï¼ˆ{len(df_eval)}åªï¼‰:")
    log.info(f"  å¹³å‡æ”¶ç›Šç‡: {avg_return:.2f}%")
    log.info(f"  ä¸­ä½æ•°æ”¶ç›Š: {median_return:.2f}%")
    log.info(f"  èƒœç‡: {win_rate:.1f}%")
    log.info(f"  æœ€é«˜æ”¶ç›Š: {max_return:.2f}%")
    log.info(f"  æœ€ä½æ”¶ç›Š: {min_return:.2f}%")
    
    return {
        'avg_return': avg_return,
        'median_return': median_return,
        'win_rate': win_rate,
        'max_return': max_return,
        'min_return': min_return,
        'count': len(df_eval)
    }


def main():
    parser = argparse.ArgumentParser(description='æ ¡å‡†æ¨¡å‹é¢„æµ‹è¯„ä¼°')
    parser.add_argument('--predict-date', type=str, default='20251212', help='é¢„æµ‹æ—¥æœŸ')
    parser.add_argument('--eval-date', type=str, default='20251231', help='è¯„ä¼°æ—¥æœŸ')
    parser.add_argument('--top-n', type=int, default=50, help='Top Nè‚¡ç¥¨æ•°é‡')
    args = parser.parse_args()
    
    log.info("="*80)
    log.info("æ ¡å‡†æ¨¡å‹é¢„æµ‹è¯„ä¼°")
    log.info("="*80)
    log.info(f"é¢„æµ‹æ—¥æœŸ: {args.predict_date}")
    log.info(f"è¯„ä¼°æ—¥æœŸ: {args.eval_date}")
    log.info(f"Top N: {args.top_n}")
    log.info(f"é£é™©è¿‡æ»¤é˜ˆå€¼: {RISK_THRESHOLD}")
    log.info("")
    
    # åˆå§‹åŒ–
    dm = DataManager()
    stock_list = get_valid_stock_list(dm)
    log.info(f"æœ‰æ•ˆè‚¡ç¥¨æ•°: {len(stock_list)}")
    
    # æ£€æŸ¥æ ¡å‡†æ¨¡å‹æ˜¯å¦å­˜åœ¨
    calibrated_model_path = PROJECT_ROOT / 'data' / 'models' / 'breakout_launch_scorer' / 'versions' / 'v2.2.0' / 'model' / 'calibrated_model.pkl'
    
    if calibrated_model_path.exists():
        log.info("\nä½¿ç”¨v2.2.0æ ¡å‡†æ¨¡å‹...")
        model, feature_names = load_calibrated_model('v2.2.0')
        use_calibrated = True
    else:
        log.warning("\næ ¡å‡†æ¨¡å‹ä¸å­˜åœ¨ï¼Œä½¿ç”¨v2.1.0åŸºç¡€æ¨¡å‹...")
        log.warning("è¯·å…ˆè¿è¡Œ python scripts/train_calibrated_model.py è®­ç»ƒæ ¡å‡†æ¨¡å‹")
        model, feature_names = load_base_model('v2.1.0')
        use_calibrated = False
    
    log.info(f"ç‰¹å¾æ•°: {len(feature_names)}")
    
    # é¢„æµ‹
    df_predictions = predict_with_model(dm, stock_list, model, feature_names, 
                                        args.predict_date, use_calibrated)
    
    # æ·»åŠ é¢„æµ‹æ—¥æœŸ
    df_predictions['predict_date'] = args.predict_date
    
    # ========== æ— é£é™©è¿‡æ»¤ ==========
    log.info("\n" + "="*80)
    log.info("æ— é£é™©è¿‡æ»¤ï¼ˆæŒ‰åŸå§‹æ¦‚ç‡æ’åºï¼‰")
    log.info("="*80)
    
    df_top_raw = df_predictions.nlargest(args.top_n, 'probability')
    df_eval_raw = evaluate_predictions(dm, df_top_raw, args.eval_date)
    stats_raw = print_summary(df_eval_raw, f"æ— é£é™©è¿‡æ»¤ Top{args.top_n}")
    
    # ========== å¸¦é£é™©è¿‡æ»¤ ==========
    log.info("\n" + "="*80)
    log.info(f"å¸¦é£é™©è¿‡æ»¤ï¼ˆrisk_score >= {RISK_THRESHOLD}ï¼‰")
    log.info("="*80)
    
    # å…ˆè¿‡æ»¤é£é™©ï¼Œå†æŒ‰æ¦‚ç‡æ’åº
    df_filtered = df_predictions[df_predictions['risk_score'] >= RISK_THRESHOLD]
    log.info(f"é£é™©è¿‡æ»¤åå‰©ä½™: {len(df_filtered)} åª")
    
    df_top_filtered = df_filtered.nlargest(args.top_n, 'probability')
    df_eval_filtered = evaluate_predictions(dm, df_top_filtered, args.eval_date)
    stats_filtered = print_summary(df_eval_filtered, f"é£é™©è¿‡æ»¤å Top{min(args.top_n, len(df_top_filtered))}")
    
    # ========== å¯¹æ¯” ==========
    log.info("\n" + "="*80)
    log.info("å¯¹æ¯”åˆ†æ")
    log.info("="*80)
    
    if stats_raw and stats_filtered:
        log.info("\n| æŒ‡æ ‡ | æ— è¿‡æ»¤ | å¸¦é£é™©è¿‡æ»¤ | å˜åŒ– |")
        log.info("|------|--------|------------|------|")
        log.info(f"| å¹³å‡æ”¶ç›Šç‡ | {stats_raw['avg_return']:.2f}% | {stats_filtered['avg_return']:.2f}% | {stats_filtered['avg_return'] - stats_raw['avg_return']:+.2f}% |")
        log.info(f"| èƒœç‡ | {stats_raw['win_rate']:.1f}% | {stats_filtered['win_rate']:.1f}% | {stats_filtered['win_rate'] - stats_raw['win_rate']:+.1f}% |")
        log.info(f"| æœ€å¤§äºæŸ | {stats_raw['min_return']:.2f}% | {stats_filtered['min_return']:.2f}% | - |")
    
    # ä¿å­˜ç»“æœ
    output_dir = PROJECT_ROOT / 'data' / 'prediction' / 'evaluation'
    output_dir.mkdir(parents=True, exist_ok=True)
    
    model_version = 'v2.2.0_calibrated' if use_calibrated else 'v2.1.0_base'
    df_eval_raw.to_csv(output_dir / f'{model_version}_raw_top{args.top_n}_{args.predict_date}.csv', 
                       index=False, encoding='utf-8-sig')
    df_eval_filtered.to_csv(output_dir / f'{model_version}_filtered_top{args.top_n}_{args.predict_date}.csv',
                            index=False, encoding='utf-8-sig')
    
    log.success(f"\nâœ“ ç»“æœå·²ä¿å­˜åˆ° {output_dir}")


if __name__ == '__main__':
    main()

