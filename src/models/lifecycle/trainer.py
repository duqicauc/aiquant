"""
æ¨¡å‹è®­ç»ƒå™¨
"""
import sys
import os
import warnings
import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime
import json
import yaml
import xgboost as xgb
from sklearn.metrics import (
    classification_report, confusion_matrix, 
    roc_auc_score
)

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '../../..')))

from src.utils.logger import log
from src.models.lifecycle.iterator import ModelIterator
from src.visualization.training_visualizer import TrainingVisualizer


class ModelTrainer:
    """æ¨¡å‹è®­ç»ƒå™¨"""
    
    def __init__(self, model_name: str, config_path: str = None):
        self.model_name = model_name
        self.iterator = ModelIterator(model_name)
        
        # åŠ è½½é…ç½®
        if config_path is None:
            config_path = f"config/models/{model_name}.yaml"
        
        with open(config_path, 'r', encoding='utf-8') as f:
            self.config = yaml.safe_load(f)
        
        # è®¾ç½®è·¯å¾„
        self.base_path = Path(f"data/models/{model_name}")
        self.base_path.mkdir(parents=True, exist_ok=True)
    
    def train_version(
        self,
        version: str = None,
        neg_version: str = 'v2'
    ):
        """è®­ç»ƒæŒ‡å®šç‰ˆæœ¬"""
        if version is None:
            # åˆ›å»ºæ–°ç‰ˆæœ¬
            existing_versions = self.iterator.list_versions()
            if existing_versions:
                latest = existing_versions[-1]
                # é€’å¢ç‰ˆæœ¬å·
                version = self._increment_version(latest)
            else:
                version = 'v1.0.0'
            
            self.iterator.create_version(version)
        else:
            # å¦‚æœæŒ‡å®šäº†ç‰ˆæœ¬ï¼Œæ£€æŸ¥æ˜¯å¦å­˜åœ¨ï¼Œä¸å­˜åœ¨åˆ™åˆ›å»º
            try:
                self.iterator.get_version_info(version)
                log.info(f"ç‰ˆæœ¬ {version} å·²å­˜åœ¨ï¼Œå°†é‡æ–°è®­ç»ƒå¹¶è¦†ç›–")
            except ValueError:
                # ç‰ˆæœ¬ä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–°ç‰ˆæœ¬
                self.iterator.create_version(version)
                log.info(f"åˆ›å»ºæ–°ç‰ˆæœ¬: {version}")
        
        log.info("="*80)
        log.info(f"è®­ç»ƒæ¨¡å‹: {self.model_name} ç‰ˆæœ¬: {version}")
        log.info("="*80)
        
        # 1. åŠ è½½æ•°æ®
        df = self._load_and_prepare_data(neg_version)
        
        # 2. ç‰¹å¾å·¥ç¨‹
        df_features = self._extract_features(df)
        
        # 3. æ—¶é—´åºåˆ—åˆ’åˆ†
        X_train, X_test, y_train, y_test, train_dates, test_dates = self._timeseries_split(df_features)
        
        # 4. è®­ç»ƒæ¨¡å‹
        model, metrics = self._train_model(X_train, y_train, X_test, y_test)
        
        # 4.5. ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
        self._generate_visualizations(
            model, X_train, X_test, y_train, y_test,
            train_dates, test_dates, version
        )
        
        # 5. ä¿å­˜æ¨¡å‹ï¼ˆåŒ…å«ç‰¹å¾åç§°ï¼‰
        feature_cols = list(X_train.columns)
        training_start_time = datetime.now()
        self._save_model(model, metrics, version, train_dates, test_dates, feature_cols)
        training_end_time = datetime.now()
        
        # 6. æ›´æ–°ç‰ˆæœ¬å…ƒæ•°æ®ï¼ˆåŒ…å«å®Œæ•´ä¿¡æ¯ï¼‰
        # é‡æ–°ç»„ç»‡ metrics ç»“æ„
        metrics_structured = {
            'training': {
                'accuracy': metrics['accuracy'],
                'precision': metrics['precision'],
                'recall': metrics['recall'],
                'f1': metrics['f1_score'],
                'auc': metrics['auc']
            },
            'validation': {
                'accuracy': metrics['accuracy'],
                'precision': metrics['precision'],
                'recall': metrics['recall'],
                'f1': metrics['f1_score'],
                'auc': metrics['auc']
            },
            'test': {
                'accuracy': metrics['accuracy'],
                'precision': metrics['precision'],
                'recall': metrics['recall'],
                'f1': metrics['f1_score'],
                'auc': metrics['auc'],
                'confusion_matrix': metrics.get('confusion_matrix', [])
            }
        }
        
        # è·å–æ¨¡å‹é…ç½®ä¿¡æ¯
        model_config = self.config.get('model', {})
        display_name = model_config.get('display_name', self.model_name)
        description = model_config.get('description', '')
        
        # æ›´æ–°ç‰ˆæœ¬å…ƒæ•°æ®
        self.iterator.update_version_metadata(
            version,
            display_name=f"{display_name} {version}",
            description=description,
            config=self.config,
            metrics=metrics_structured,
            training={
                'started_at': training_start_time.isoformat(),
                'completed_at': training_end_time.isoformat(),
                'duration_seconds': int((training_end_time - training_start_time).total_seconds()),
                'samples': {
                    'train': len(X_train),
                    'test': len(X_test)
                },
                'hyperparameters': self.config.get('model_params', {}),
                'train_date_range': f"{train_dates.min().date()} to {train_dates.max().date()}",
                'test_date_range': f"{test_dates.min().date()} to {test_dates.max().date()}"
            }
        )
        
        log.success(f"âœ… æ¨¡å‹è®­ç»ƒå®Œæˆï¼ç‰ˆæœ¬: {version}")
        return model, metrics
    
    def _load_and_prepare_data(self, neg_version='v2'):
        """åŠ è½½å¹¶å‡†å¤‡è®­ç»ƒæ•°æ®"""
        log.info("åŠ è½½æ•°æ®...")
        
        # åŠ è½½æ­£æ ·æœ¬
        df_pos = pd.read_csv('data/training/features/feature_data_34d.csv')
        df_pos['label'] = 1
        log.success(f"âœ“ æ­£æ ·æœ¬: {len(df_pos)} æ¡")
        
        # åŠ è½½è´Ÿæ ·æœ¬ï¼ˆæ£€æŸ¥å¤šä¸ªå¯èƒ½çš„ä½ç½®ï¼‰
        if neg_version == 'v2':
            # å…ˆæ£€æŸ¥featuresç›®å½•
            if os.path.exists('data/training/features/negative_feature_data_v2_34d.csv'):
                neg_file = 'data/training/features/negative_feature_data_v2_34d.csv'
            elif os.path.exists('data/training/samples/negative_feature_data_v2_34d.csv'):
                neg_file = 'data/training/samples/negative_feature_data_v2_34d.csv'
            else:
                raise FileNotFoundError("æœªæ‰¾åˆ°è´Ÿæ ·æœ¬ç‰¹å¾æ–‡ä»¶ negative_feature_data_v2_34d.csv")
        else:
            if os.path.exists('data/training/features/negative_feature_data_34d.csv'):
                neg_file = 'data/training/features/negative_feature_data_34d.csv'
            elif os.path.exists('data/training/samples/negative_feature_data_34d.csv'):
                neg_file = 'data/training/samples/negative_feature_data_34d.csv'
            else:
                raise FileNotFoundError("æœªæ‰¾åˆ°è´Ÿæ ·æœ¬ç‰¹å¾æ–‡ä»¶ negative_feature_data_34d.csv")
        
        df_neg = pd.read_csv(neg_file)
        log.success(f"âœ“ è´Ÿæ ·æœ¬: {len(df_neg)} æ¡")
        
        # åˆå¹¶
        df = pd.concat([df_pos, df_neg])
        return df
    
    def _extract_features(self, df):
        """æå–ç‰¹å¾ï¼ˆå¤ç”¨ç°æœ‰é€»è¾‘ï¼‰"""
        log.info("æå–ç‰¹å¾...")
        
        # é‡æ–°åˆ†é…å”¯ä¸€çš„sample_id
        df['unique_sample_id'] = df.groupby(['ts_code', 'label']).ngroup()
        
        features = []
        sample_ids = df['unique_sample_id'].unique()
        
        # è·å–T1æ—¥æœŸæ˜ å°„
        df_positive_samples = pd.read_csv('data/training/samples/positive_samples.csv')
        t1_date_map = dict(zip(
            df_positive_samples.index,
            pd.to_datetime(df_positive_samples['t1_date'])
        ))
        
        if os.path.exists('data/training/samples/negative_samples_v2.csv'):
            df_negative_samples = pd.read_csv('data/training/samples/negative_samples_v2.csv')
        else:
            df_negative_samples = pd.read_csv('data/training/samples/negative_samples.csv')
        
        max_positive_id = df_positive_samples.index.max()
        for idx, row in df_negative_samples.iterrows():
            t1_date_map[max_positive_id + 1 + idx] = pd.to_datetime(row['t1_date'])
        
        for i, sample_id in enumerate(sample_ids):
            if (i + 1) % 500 == 0:
                log.info(f"è¿›åº¦: {i+1}/{len(sample_ids)}")
            
            sample_data = df[df['unique_sample_id'] == sample_id].sort_values('days_to_t1')
            
            if len(sample_data) < 20:
                continue
            
            t1_row = sample_data.iloc[sample_data['days_to_t1'].abs().argmin()]
            t1_date = pd.to_datetime(t1_row['trade_date'])
            
            feature_dict = {
                'sample_id': sample_id,
                'ts_code': sample_data['ts_code'].iloc[0],
                'name': sample_data['name'].iloc[0],
                'label': int(sample_data['label'].iloc[0]),
                't1_date': t1_date,
            }
            
            # ä»·æ ¼ç‰¹å¾
            feature_dict['close_mean'] = sample_data['close'].mean()
            feature_dict['close_std'] = sample_data['close'].std()
            feature_dict['close_max'] = sample_data['close'].max()
            feature_dict['close_min'] = sample_data['close'].min()
            feature_dict['close_trend'] = (
                (sample_data['close'].iloc[-1] - sample_data['close'].iloc[0]) / 
                sample_data['close'].iloc[0] * 100
            )
            
            # æ¶¨è·Œå¹…ç‰¹å¾
            feature_dict['pct_chg_mean'] = sample_data['pct_chg'].mean()
            feature_dict['pct_chg_std'] = sample_data['pct_chg'].std()
            feature_dict['pct_chg_sum'] = sample_data['pct_chg'].sum()
            feature_dict['positive_days'] = (sample_data['pct_chg'] > 0).sum()
            feature_dict['negative_days'] = (sample_data['pct_chg'] < 0).sum()
            feature_dict['max_gain'] = sample_data['pct_chg'].max()
            feature_dict['max_loss'] = sample_data['pct_chg'].min()
            
            # é‡æ¯”ç‰¹å¾
            if 'volume_ratio' in sample_data.columns:
                feature_dict['volume_ratio_mean'] = sample_data['volume_ratio'].mean()
                feature_dict['volume_ratio_max'] = sample_data['volume_ratio'].max()
                feature_dict['volume_ratio_gt_2'] = (sample_data['volume_ratio'] > 2).sum()
                feature_dict['volume_ratio_gt_4'] = (sample_data['volume_ratio'] > 4).sum()
            else:
                feature_dict['volume_ratio_mean'] = 0
                feature_dict['volume_ratio_max'] = 0
                feature_dict['volume_ratio_gt_2'] = 0
                feature_dict['volume_ratio_gt_4'] = 0
            
            # MACDç‰¹å¾
            if 'macd' in sample_data.columns:
                macd_data = sample_data['macd'].dropna()
                if len(macd_data) > 0:
                    feature_dict['macd_mean'] = macd_data.mean()
                    feature_dict['macd_positive_days'] = (macd_data > 0).sum()
                    feature_dict['macd_max'] = macd_data.max()
                else:
                    feature_dict['macd_mean'] = 0
                    feature_dict['macd_positive_days'] = 0
                    feature_dict['macd_max'] = 0
            else:
                feature_dict['macd_mean'] = 0
                feature_dict['macd_positive_days'] = 0
                feature_dict['macd_max'] = 0
            
            # MAç‰¹å¾
            if 'ma5' in sample_data.columns:
                feature_dict['ma5_mean'] = sample_data['ma5'].mean()
                feature_dict['price_above_ma5'] = (sample_data['close'] > sample_data['ma5']).sum()
            else:
                feature_dict['ma5_mean'] = 0
                feature_dict['price_above_ma5'] = 0
            
            if 'ma10' in sample_data.columns:
                feature_dict['ma10_mean'] = sample_data['ma10'].mean()
                feature_dict['price_above_ma10'] = (sample_data['close'] > sample_data['ma10']).sum()
            else:
                feature_dict['ma10_mean'] = 0
                feature_dict['price_above_ma10'] = 0
            
            # å¸‚å€¼ç‰¹å¾
            if 'total_mv' in sample_data.columns:
                mv_data = sample_data['total_mv'].dropna()
                if len(mv_data) > 0:
                    feature_dict['total_mv_mean'] = mv_data.mean()
                else:
                    feature_dict['total_mv_mean'] = 0
            else:
                feature_dict['total_mv_mean'] = 0
            
            if 'circ_mv' in sample_data.columns:
                circ_mv_data = sample_data['circ_mv'].dropna()
                if len(circ_mv_data) > 0:
                    feature_dict['circ_mv_mean'] = circ_mv_data.mean()
                else:
                    feature_dict['circ_mv_mean'] = 0
            else:
                feature_dict['circ_mv_mean'] = 0
            
            # åŠ¨é‡ç‰¹å¾ï¼ˆåˆ†æ®µæ”¶ç›Šç‡ï¼‰
            days = len(sample_data)
            if days >= 7:
                feature_dict['return_1w'] = (
                    (sample_data['close'].iloc[-1] - sample_data['close'].iloc[-7]) /
                    sample_data['close'].iloc[-7] * 100
                )
            else:
                feature_dict['return_1w'] = 0
            
            if days >= 14:
                feature_dict['return_2w'] = (
                    (sample_data['close'].iloc[-1] - sample_data['close'].iloc[-14]) /
                    sample_data['close'].iloc[-14] * 100
                )
            else:
                feature_dict['return_2w'] = 0
            
            features.append(feature_dict)
        
        df_features = pd.DataFrame(features)
        log.success(f"âœ“ ç‰¹å¾æå–å®Œæˆ: {len(df_features)} ä¸ªæ ·æœ¬")
        return df_features
    
    def _timeseries_split(self, df_features):
        """æ—¶é—´åºåˆ—åˆ’åˆ†"""
        df_features['t1_date'] = pd.to_datetime(df_features['t1_date'])
        df_features = df_features.sort_values('t1_date').reset_index(drop=True)
        
        # ä½¿ç”¨é…ç½®ä¸­çš„åˆ’åˆ†æ–¹å¼
        train_end_date = self.config.get('training', {}).get('train_end_date')
        test_start_date = self.config.get('training', {}).get('test_start_date')
        
        if train_end_date is None:
            n_train = int(len(df_features) * 0.8)
            train_end_date = df_features.iloc[n_train]['t1_date']
            test_start_date = df_features.iloc[n_train + 1]['t1_date']
        else:
            train_end_date = pd.to_datetime(train_end_date)
            test_start_date = pd.to_datetime(test_start_date)
        
        train_mask = df_features['t1_date'] <= train_end_date
        test_mask = df_features['t1_date'] >= test_start_date
        
        df_train = df_features[train_mask]
        df_test = df_features[test_mask]
        
        log.info(f"è®­ç»ƒé›†: {len(df_train)} ä¸ªæ ·æœ¬")
        log.info(f"æµ‹è¯•é›†: {len(df_test)} ä¸ªæ ·æœ¬")
        
        feature_cols = [col for col in df_features.columns 
                       if col not in ['sample_id', 'label', 't1_date', 'ts_code', 'name']]
        
        X_train = df_train[feature_cols].fillna(0)
        y_train = df_train['label']
        train_dates = df_train['t1_date']
        
        X_test = df_test[feature_cols].fillna(0)
        y_test = df_test['label']
        test_dates = df_test['t1_date']
        
        # åˆ é™¤éæ•°å€¼åˆ—
        non_numeric_cols = X_train.select_dtypes(include=['object']).columns
        if len(non_numeric_cols) > 0:
            X_train = X_train.drop(columns=non_numeric_cols)
            X_test = X_test.drop(columns=non_numeric_cols)
        
        return X_train, X_test, y_train, y_test, train_dates, test_dates
    
    def _train_model(self, X_train, y_train, X_test, y_test):
        """è®­ç»ƒæ¨¡å‹"""
        log.info("è®­ç»ƒæ¨¡å‹...")
        
        model_params = self.config.get('model_params', {})
        model = xgb.XGBClassifier(**model_params)
        
        model.fit(
            X_train, y_train,
            eval_set=[(X_test, y_test)],
            verbose=False
        )
        
        # è¯„ä¼°
        y_pred = model.predict(X_test)
        y_prob = model.predict_proba(X_test)[:, 1]
        
        report = classification_report(y_test, y_pred, target_names=['è´Ÿæ ·æœ¬', 'æ­£æ ·æœ¬'], output_dict=True)
        auc = roc_auc_score(y_test, y_prob)
        cm = confusion_matrix(y_test, y_pred)
        
        metrics = {
            'accuracy': report['accuracy'],
            'precision': report['æ­£æ ·æœ¬']['precision'],
            'recall': report['æ­£æ ·æœ¬']['recall'],
            'f1_score': report['æ­£æ ·æœ¬']['f1-score'],
            'auc': auc,
            'confusion_matrix': cm.tolist()
        }
        
        log.success("âœ“ æ¨¡å‹è®­ç»ƒå®Œæˆ")
        log.info(f"å‡†ç¡®ç‡: {metrics['accuracy']:.2%}")
        log.info(f"AUC: {metrics['auc']:.4f}")
        
        return model, metrics
    
    def _save_model(self, model, metrics, version, train_dates, test_dates, feature_cols=None):
        """ä¿å­˜æ¨¡å‹"""
        version_path = self.iterator.versions_path / version
        model_path = version_path / "model" / "model.json"
        model_path.parent.mkdir(parents=True, exist_ok=True)
        
        model.get_booster().save_model(str(model_path))
        log.success(f"âœ“ æ¨¡å‹å·²ä¿å­˜: {model_path}")
        
        # ä¿å­˜ç‰¹å¾åç§°
        if feature_cols is not None:
            feature_names_file = version_path / "model" / "feature_names.json"
            with open(feature_names_file, 'w', encoding='utf-8') as f:
                json.dump(feature_cols, f, indent=2, ensure_ascii=False)
            log.success(f"âœ“ ç‰¹å¾åç§°å·²ä¿å­˜: {feature_names_file}")
        
        # ä¿å­˜æŒ‡æ ‡
        metrics_file = version_path / "training" / "metrics.json"
        metrics['model_file'] = str(model_path)
        metrics['timestamp'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        metrics['train_date_range'] = f"{train_dates.min().date()} to {train_dates.max().date()}"
        metrics['test_date_range'] = f"{test_dates.min().date()} to {test_dates.max().date()}"
        
        with open(metrics_file, 'w', encoding='utf-8') as f:
            json.dump(metrics, f, indent=2, ensure_ascii=False)
        
        log.success(f"âœ“ æŒ‡æ ‡å·²ä¿å­˜: {metrics_file}")
    
    def _generate_visualizations(
        self, model, X_train, X_test, y_train, y_test,
        train_dates, test_dates, version
    ):
        """ç”Ÿæˆè®­ç»ƒè¿‡ç¨‹å¯è§†åŒ–å›¾è¡¨"""
        try:
            log.info("="*80)
            log.info("ç”Ÿæˆè®­ç»ƒå¯è§†åŒ–å›¾è¡¨")
            log.info("="*80)
            
            visualizer = TrainingVisualizer(
                output_dir=f"data/models/{self.model_name}/versions/{version}/charts"
            )
            
            # 1. æ ·æœ¬è´¨é‡å¯è§†åŒ–ï¼ˆæ­£æ ·æœ¬å’Œè´Ÿæ ·æœ¬ï¼‰
            df_positive_samples = pd.read_csv('data/training/samples/positive_samples.csv')
            visualizer.visualize_sample_quality(
                df_positive_samples, 
                save_prefix="positive_sample_quality"
            )
            
            # è´Ÿæ ·æœ¬
            if os.path.exists('data/training/samples/negative_samples_v2.csv'):
                df_negative_samples = pd.read_csv('data/training/samples/negative_samples_v2.csv')
                visualizer.visualize_sample_quality(
                    df_negative_samples,
                    save_prefix="negative_sample_quality"
                )
            
            # 2. ç‰¹å¾è´¨é‡è¯„ä¼°å¯è§†åŒ–
            visualizer.visualize_feature_quality(
                X_train, y_train, X_test, y_test,
                model_name=f"{self.model_name}_{version}"
            )
            
            # 3. å› å­é‡è¦æ€§å¯è§†åŒ–
            feature_importance = pd.DataFrame({
                'feature': X_train.columns,
                'importance': model.feature_importances_
            })
            
            visualizer.visualize_feature_importance(
                feature_importance,
                model_name=f"{self.model_name}_{version}",
                top_n=20
            )
            
            # 4. æ¨¡å‹è®­ç»ƒè¿‡ç¨‹å¯è§†åŒ–
            visualizer.visualize_training_process(
                model, X_train, y_train, X_test, y_test,
                model_name=f"{self.model_name}_{version}"
            )
            
            # 5. æ¨¡å‹ç»“æœè¯„æµ‹å¯è§†åŒ–
            y_pred = model.predict(X_test)
            y_prob = model.predict_proba(X_test)[:, 1]
            visualizer.visualize_model_results(
                y_test, y_pred, y_prob,
                model_name=f"{self.model_name}_{version}"
            )
            
            # 6. ç”Ÿæˆç´¢å¼•é¡µé¢
            visualizer.generate_index_page(model_name=f"{self.model_name}_{version}")
            
            log.success("âœ“ å¯è§†åŒ–å›¾è¡¨ç”Ÿæˆå®Œæˆ")
            log.info(f"ğŸ“Š æŸ¥çœ‹å›¾è¡¨: open data/models/{self.model_name}/versions/{version}/charts/index.html")
            
        except Exception as e:
            log.warning(f"ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨æ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
    
    def _increment_version(self, version: str) -> str:
        """é€’å¢ç‰ˆæœ¬å·"""
        version = version.lstrip('v')
        parts = version.split('.')
        if len(parts) == 3:
            parts[2] = str(int(parts[2]) + 1)
        return 'v' + '.'.join(parts)

