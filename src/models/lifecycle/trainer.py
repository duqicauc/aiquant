"""
æ¨¡å‹è®­ç»ƒå™¨
"""
import sys
import os
import warnings
import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime
import json
import yaml
import xgboost as xgb
from sklearn.metrics import (
    classification_report, confusion_matrix, 
    roc_auc_score
)

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '../../..')))

from src.utils.logger import log
from src.models.lifecycle.iterator import ModelIterator
from src.visualization.training_visualizer import TrainingVisualizer


def safe_to_datetime(date_value):
    """
    å®‰å…¨åœ°å°†æ—¥æœŸå€¼è½¬æ¢ä¸ºdatetimeç±»å‹
    
    å¤„ç†ä»¥ä¸‹æƒ…å†µï¼š
    - æ•´æ•°ï¼šå¦‚ 20230101 -> è¢«é”™è¯¯è§£æä¸ºçº³ç§’æ—¶é—´æˆ³
    - å­—ç¬¦ä¸²ï¼šå¦‚ '20230101' -> æ­£å¸¸è§£æ
    - datetimeï¼šç›´æ¥è¿”å›
    """
    if pd.isna(date_value):
        return pd.NaT
    if isinstance(date_value, (int, np.integer, float, np.floating)):
        return pd.to_datetime(str(int(date_value)), format='%Y%m%d', errors='coerce')
    return pd.to_datetime(date_value, errors='coerce')


class ModelTrainer:
    """æ¨¡å‹è®­ç»ƒå™¨"""
    
    def __init__(self, model_name: str, config_path: str = None):
        self.model_name = model_name
        self.iterator = ModelIterator(model_name)
        
        # åŠ è½½é…ç½®
        if config_path is None:
            config_path = f"config/models/{model_name}.yaml"
        
        with open(config_path, 'r', encoding='utf-8') as f:
            self.config = yaml.safe_load(f)
        
        # è®¾ç½®è·¯å¾„
        self.base_path = Path(f"data/models/{model_name}")
        self.base_path.mkdir(parents=True, exist_ok=True)
    
    def train_version(
        self,
        version: str = None,
        neg_version: str = 'v2'
    ):
        """è®­ç»ƒæŒ‡å®šç‰ˆæœ¬"""
        if version is None:
            # åˆ›å»ºæ–°ç‰ˆæœ¬
            existing_versions = self.iterator.list_versions()
            if existing_versions:
                latest = existing_versions[-1]
                # é€’å¢ç‰ˆæœ¬å·
                version = self._increment_version(latest)
            else:
                version = 'v1.0.0'
            
            self.iterator.create_version(version)
        else:
            # å¦‚æœæŒ‡å®šäº†ç‰ˆæœ¬ï¼Œæ£€æŸ¥æ˜¯å¦å­˜åœ¨ï¼Œä¸å­˜åœ¨åˆ™åˆ›å»º
            try:
                self.iterator.get_version_info(version)
                log.info(f"ç‰ˆæœ¬ {version} å·²å­˜åœ¨ï¼Œå°†é‡æ–°è®­ç»ƒå¹¶è¦†ç›–")
            except ValueError:
                # ç‰ˆæœ¬ä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–°ç‰ˆæœ¬
                self.iterator.create_version(version)
                log.info(f"åˆ›å»ºæ–°ç‰ˆæœ¬: {version}")
        
        log.info("="*80)
        log.info(f"è®­ç»ƒæ¨¡å‹: {self.model_name} ç‰ˆæœ¬: {version}")
        log.info("="*80)
        
        # 1. åŠ è½½æ•°æ®
        df = self._load_and_prepare_data(neg_version)
        
        # 2. ç‰¹å¾å·¥ç¨‹
        df_features = self._extract_features(df)
        
        # 3. æ—¶é—´åºåˆ—åˆ’åˆ†
        X_train, X_test, y_train, y_test, train_dates, test_dates = self._timeseries_split(df_features)
        
        # 4. è®­ç»ƒæ¨¡å‹ï¼ˆè®°å½•è®­ç»ƒæ—¶é—´ï¼‰
        training_start_time = datetime.now()
        model, metrics = self._train_model(X_train, y_train, X_test, y_test)
        training_end_time = datetime.now()
        
        # 4.5. ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
        self._generate_visualizations(
            model, X_train, X_test, y_train, y_test,
            train_dates, test_dates, version
        )
        
        # 5. ä¿å­˜æ¨¡å‹ï¼ˆåŒ…å«ç‰¹å¾åç§°ï¼‰
        feature_cols = list(X_train.columns)
        self._save_model(model, metrics, version, train_dates, test_dates, feature_cols)
        
        # 6. æ›´æ–°ç‰ˆæœ¬å…ƒæ•°æ®ï¼ˆåŒ…å«å®Œæ•´ä¿¡æ¯ï¼‰
        # é‡æ–°ç»„ç»‡ metrics ç»“æ„
        metrics_structured = {
            'training': {
                'accuracy': metrics['accuracy'],
                'precision': metrics['precision'],
                'recall': metrics['recall'],
                'f1': metrics['f1_score'],
                'auc': metrics['auc']
            },
            'validation': {
                'accuracy': metrics['accuracy'],
                'precision': metrics['precision'],
                'recall': metrics['recall'],
                'f1': metrics['f1_score'],
                'auc': metrics['auc']
            },
            'test': {
                'accuracy': metrics['accuracy'],
                'precision': metrics['precision'],
                'recall': metrics['recall'],
                'f1': metrics['f1_score'],
                'auc': metrics['auc'],
                'confusion_matrix': metrics.get('confusion_matrix', [])
            }
        }
        
        # è·å–æ¨¡å‹é…ç½®ä¿¡æ¯
        model_config = self.config.get('model', {})
        display_name = model_config.get('display_name', self.model_name)
        description = model_config.get('description', '')
        
        # æ›´æ–°ç‰ˆæœ¬å…ƒæ•°æ®
        self.iterator.update_version_metadata(
            version,
            display_name=f"{display_name} {version}",
            description=description,
            config=self.config,
            metrics=metrics_structured,
            training={
                'started_at': training_start_time.isoformat(),
                'completed_at': training_end_time.isoformat(),
                'duration_seconds': int((training_end_time - training_start_time).total_seconds()),
                'samples': {
                    'train': len(X_train),
                    'test': len(X_test)
                },
                'hyperparameters': self.config.get('model_params', {}),
                'train_date_range': f"{train_dates.min().date()} to {train_dates.max().date()}",
                'test_date_range': f"{test_dates.min().date()} to {test_dates.max().date()}"
            }
        )
        
        log.success(f"âœ… æ¨¡å‹è®­ç»ƒå®Œæˆï¼ç‰ˆæœ¬: {version}")
        return model, metrics
    
    def _load_and_prepare_data(self, neg_version='v2'):
        """
        åŠ è½½å¹¶å‡†å¤‡è®­ç»ƒæ•°æ®ï¼ˆä¸æ—§æ¨¡å‹ train_xgboost_timeseries.py å®Œå…¨ä¸€è‡´ï¼‰
        """
        log.info("="*80)
        log.info("ç¬¬ä¸€æ­¥ï¼šåŠ è½½æ•°æ®")
        log.info("="*80)
        
        # åŠ è½½æ­£æ ·æœ¬ï¼ˆä½¿ç”¨ä¸æ—§æ¨¡å‹å®Œå…¨ç›¸åŒçš„è·¯å¾„ï¼‰
        df_pos = pd.read_csv('data/training/features/feature_data_34d.csv')
        df_pos['label'] = 1
        log.success(f"âœ“ æ­£æ ·æœ¬åŠ è½½å®Œæˆ: {len(df_pos)} æ¡")
        
        # åŠ è½½è´Ÿæ ·æœ¬ï¼ˆä½¿ç”¨ä¸æ—§æ¨¡å‹å®Œå…¨ç›¸åŒçš„è·¯å¾„å’Œé€»è¾‘ï¼‰
        # æ”¯æŒä»æ—§ä½ç½®è‡ªåŠ¨è¿ç§»åˆ°æ–°æ¡†æ¶ç›®å½•ç»“æ„
        if neg_version == 'v2':
            neg_file = 'data/training/features/negative_feature_data_v2_34d.csv'
            old_neg_file = 'data/training/samples/negative_feature_data_v2_34d.csv'
        else:
            neg_file = 'data/training/features/negative_feature_data_34d.csv'
            old_neg_file = 'data/training/samples/negative_feature_data_34d.csv'
        
        # å¦‚æœæ–°ä½ç½®ä¸å­˜åœ¨ï¼Œå°è¯•ä»æ—§ä½ç½®ç§»åŠ¨
        if not os.path.exists(neg_file) and os.path.exists(old_neg_file):
            log.info(f"å‘ç°æ—§ä½ç½®çš„æ•°æ®æ–‡ä»¶: {old_neg_file}")
            log.info(f"æ­£åœ¨ç§»åŠ¨åˆ°æ–°æ¡†æ¶ç›®å½•: {neg_file}")
            import shutil
            os.makedirs(os.path.dirname(neg_file), exist_ok=True)
            shutil.copy2(old_neg_file, neg_file)
            log.success(f"âœ“ æ•°æ®æ–‡ä»¶å·²ç§»åŠ¨åˆ°æ–°æ¡†æ¶ç›®å½•")
        
        df_neg = pd.read_csv(neg_file)
        log.success(f"âœ“ è´Ÿæ ·æœ¬åŠ è½½å®Œæˆ: {len(df_neg)} æ¡ (ç‰ˆæœ¬: {neg_version})")
        
        # åˆå¹¶
        df = pd.concat([df_pos, df_neg])
        log.info(f"âœ“ æ•°æ®åˆå¹¶å®Œæˆ: {len(df)} æ¡")
        log.info(f"  - æ­£æ ·æœ¬: {len(df_pos)} æ¡")
        log.info(f"  - è´Ÿæ ·æœ¬: {len(df_neg)} æ¡")
        log.info("")
        
        return df
    
    def _extract_features(self, df):
        """
        æå–ç‰¹å¾ï¼ˆä¸æ—§æ¨¡å‹ train_xgboost_timeseries.py çš„ extract_features_with_time å®Œå…¨ä¸€è‡´ï¼‰
        """
        log.info("="*80)
        log.info("ç¬¬äºŒæ­¥ï¼šç‰¹å¾å·¥ç¨‹ï¼ˆä¿ç•™æ—¶é—´ä¿¡æ¯ï¼‰")
        log.info("="*80)
        log.info("å°†34å¤©æ—¶åºæ•°æ®è½¬æ¢ä¸ºç»Ÿè®¡ç‰¹å¾...")
        
        # é‡æ–°åˆ†é…å”¯ä¸€çš„sample_id
        df['unique_sample_id'] = df.groupby(['ts_code', 'label']).ngroup()
        
        features = []
        sample_ids = df['unique_sample_id'].unique()
        
        # è·å–T1æ—¥æœŸæ˜ å°„
        df_positive_samples = pd.read_csv('data/training/samples/positive_samples.csv')
        t1_date_map = dict(zip(
            df_positive_samples.index,
            df_positive_samples['t1_date'].apply(safe_to_datetime)
        ))
        
        if os.path.exists('data/training/samples/negative_samples_v2.csv'):
            df_negative_samples = pd.read_csv('data/training/samples/negative_samples_v2.csv')
        else:
            df_negative_samples = pd.read_csv('data/training/samples/negative_samples.csv')
        
        max_positive_id = df_positive_samples.index.max()
        for idx, row in df_negative_samples.iterrows():
            t1_date_map[max_positive_id + 1 + idx] = safe_to_datetime(row['t1_date'])
        
        for i, sample_id in enumerate(sample_ids):
            if (i + 1) % 500 == 0:
                log.info(f"è¿›åº¦: {i+1}/{len(sample_ids)}")
            
            sample_data = df[df['unique_sample_id'] == sample_id].sort_values('days_to_t1')
            
            if len(sample_data) < 20:
                continue
            
            t1_row = sample_data.iloc[sample_data['days_to_t1'].abs().argmin()]
            t1_date = safe_to_datetime(t1_row['trade_date'])
            
            feature_dict = {
                'sample_id': sample_id,
                'ts_code': sample_data['ts_code'].iloc[0],
                'name': sample_data['name'].iloc[0],
                'label': int(sample_data['label'].iloc[0]),
                't1_date': t1_date,
            }
            
            # ä»·æ ¼ç‰¹å¾
            feature_dict['close_mean'] = sample_data['close'].mean()
            feature_dict['close_std'] = sample_data['close'].std()
            feature_dict['close_max'] = sample_data['close'].max()
            feature_dict['close_min'] = sample_data['close'].min()
            feature_dict['close_trend'] = (
                (sample_data['close'].iloc[-1] - sample_data['close'].iloc[0]) / 
                sample_data['close'].iloc[0] * 100
            )
            
            # æ¶¨è·Œå¹…ç‰¹å¾
            feature_dict['pct_chg_mean'] = sample_data['pct_chg'].mean()
            feature_dict['pct_chg_std'] = sample_data['pct_chg'].std()
            feature_dict['pct_chg_sum'] = sample_data['pct_chg'].sum()
            feature_dict['positive_days'] = (sample_data['pct_chg'] > 0).sum()
            feature_dict['negative_days'] = (sample_data['pct_chg'] < 0).sum()
            feature_dict['max_gain'] = sample_data['pct_chg'].max()
            feature_dict['max_loss'] = sample_data['pct_chg'].min()
            
            # é‡æ¯”ç‰¹å¾ï¼ˆä¸æ—§æ¨¡å‹å®Œå…¨ä¸€è‡´ï¼‰
            if 'volume_ratio' in sample_data.columns:
                feature_dict['volume_ratio_mean'] = sample_data['volume_ratio'].mean()
                feature_dict['volume_ratio_max'] = sample_data['volume_ratio'].max()
                feature_dict['volume_ratio_gt_2'] = (sample_data['volume_ratio'] > 2).sum()
                feature_dict['volume_ratio_gt_4'] = (sample_data['volume_ratio'] > 4).sum()
            
            # MACDç‰¹å¾ï¼ˆä¸æ—§æ¨¡å‹å®Œå…¨ä¸€è‡´ï¼‰
            if 'macd' in sample_data.columns:
                macd_data = sample_data['macd'].dropna()
                if len(macd_data) > 0:
                    feature_dict['macd_mean'] = macd_data.mean()
                    feature_dict['macd_positive_days'] = (macd_data > 0).sum()
                    feature_dict['macd_max'] = macd_data.max()
            
            # MAç‰¹å¾ï¼ˆä¸æ—§æ¨¡å‹å®Œå…¨ä¸€è‡´ï¼‰
            if 'ma5' in sample_data.columns:
                feature_dict['ma5_mean'] = sample_data['ma5'].mean()
                feature_dict['price_above_ma5'] = (
                    sample_data['close'] > sample_data['ma5']
                ).sum()
            
            if 'ma10' in sample_data.columns:
                feature_dict['ma10_mean'] = sample_data['ma10'].mean()
                feature_dict['price_above_ma10'] = (
                    sample_data['close'] > sample_data['ma10']
                ).sum()
            
            # å¸‚å€¼ç‰¹å¾ï¼ˆä¸æ—§æ¨¡å‹å®Œå…¨ä¸€è‡´ï¼‰
            if 'total_mv' in sample_data.columns:
                mv_data = sample_data['total_mv'].dropna()
                if len(mv_data) > 0:
                    feature_dict['total_mv_mean'] = mv_data.mean()
            
            if 'circ_mv' in sample_data.columns:
                circ_mv_data = sample_data['circ_mv'].dropna()
                if len(circ_mv_data) > 0:
                    feature_dict['circ_mv_mean'] = circ_mv_data.mean()
            
            # åŠ¨é‡ç‰¹å¾ï¼ˆåˆ†æ®µæ”¶ç›Šç‡ï¼Œä¸æ—§æ¨¡å‹å®Œå…¨ä¸€è‡´ï¼‰
            days = len(sample_data)
            if days >= 7:
                feature_dict['return_1w'] = (
                    (sample_data['close'].iloc[-1] - sample_data['close'].iloc[-7]) /
                    sample_data['close'].iloc[-7] * 100
                )
            if days >= 14:
                feature_dict['return_2w'] = (
                    (sample_data['close'].iloc[-1] - sample_data['close'].iloc[-14]) /
                    sample_data['close'].iloc[-14] * 100
                )
            
            features.append(feature_dict)
        
        df_features = pd.DataFrame(features)
        
        log.success(f"âœ“ ç‰¹å¾æå–å®Œæˆ: {len(df_features)} ä¸ªæ ·æœ¬")
        log.info(f"âœ“ ç‰¹å¾ç»´åº¦: {len(df_features.columns) - 3} ä¸ªç‰¹å¾ï¼ˆä¸å«sample_id, label, t1_dateï¼‰")
        log.info("")
        
        return df_features
    
    def _timeseries_split(self, df_features):
        """
        æ—¶é—´åºåˆ—åˆ’åˆ†ï¼ˆä¸æ—§æ¨¡å‹ train_xgboost_timeseries.py çš„ timeseries_split å®Œå…¨ä¸€è‡´ï¼‰
        """
        log.info("="*80)
        log.info("ç¬¬ä¸‰æ­¥ï¼šæ—¶é—´åºåˆ—åˆ’åˆ†ï¼ˆé¿å…æœªæ¥å‡½æ•°ï¼‰")
        log.info("="*80)
        
        # ç¡®ä¿t1_dateæ˜¯datetimeç±»å‹ï¼ˆé˜²æ­¢æ•´æ•°è¢«è¯¯è§£æï¼‰
        df_features['t1_date'] = df_features['t1_date'].apply(safe_to_datetime)
        
        # æŒ‰æ—¶é—´æ’åº
        df_features = df_features.sort_values('t1_date').reset_index(drop=True)
        
        # æ˜¾ç¤ºæ—¶é—´èŒƒå›´
        min_date = df_features['t1_date'].min()
        max_date = df_features['t1_date'].max()
        log.info(f"æ•°æ®æ—¶é—´èŒƒå›´: {min_date.date()} è‡³ {max_date.date()}")
        
        # ä½¿ç”¨é…ç½®ä¸­çš„åˆ’åˆ†æ–¹å¼ï¼ˆå¦‚æœæœªæŒ‡å®šï¼Œä½¿ç”¨80%ä½œä¸ºè®­ç»ƒé›†ï¼Œä¸æ—§æ¨¡å‹ä¸€è‡´ï¼‰
        train_end_date = self.config.get('training', {}).get('train_end_date')
        test_start_date = self.config.get('training', {}).get('test_start_date')
        
        if train_end_date is None:
            n_train = int(len(df_features) * 0.8)
            train_end_date = df_features.iloc[n_train]['t1_date']
            test_start_date = df_features.iloc[n_train + 1]['t1_date']
        else:
            train_end_date = pd.to_datetime(train_end_date)
            test_start_date = pd.to_datetime(test_start_date)
        
        # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
        train_mask = df_features['t1_date'] <= train_end_date
        test_mask = df_features['t1_date'] >= test_start_date
        
        df_train = df_features[train_mask]
        df_test = df_features[test_mask]
        
        log.info(f"\næ—¶é—´åˆ’åˆ†:")
        log.info(f"  è®­ç»ƒé›†: {df_train['t1_date'].min().date()} è‡³ {df_train['t1_date'].max().date()}")
        log.info(f"  æµ‹è¯•é›†: {df_test['t1_date'].min().date()} è‡³ {df_test['t1_date'].max().date()}")
        log.info(f"\næ ·æœ¬åˆ’åˆ†:")
        log.info(f"  è®­ç»ƒé›†: {len(df_train)} ä¸ªæ ·æœ¬ (æ­£:{(df_train['label']==1).sum()}, è´Ÿ:{(df_train['label']==0).sum()})")
        log.info(f"  æµ‹è¯•é›†: {len(df_test)} ä¸ªæ ·æœ¬ (æ­£:{(df_test['label']==1).sum()}, è´Ÿ:{(df_test['label']==0).sum()})")
        log.info("")
        
        # ç¡®è®¤æ— æ•°æ®æ³„éœ²
        if df_train['t1_date'].max() >= df_test['t1_date'].min():
            log.warning("âš ï¸  è­¦å‘Šï¼šè®­ç»ƒé›†å’Œæµ‹è¯•é›†æ—¶é—´æœ‰é‡å ï¼Œå¯èƒ½å­˜åœ¨æ•°æ®æ³„éœ²ï¼")
        else:
            log.success("âœ“ è®­ç»ƒé›†å’Œæµ‹è¯•é›†æ—¶é—´æ— é‡å ï¼Œæ— æ•°æ®æ³„éœ²é£é™©")
        
        # å‡†å¤‡ç‰¹å¾å’Œæ ‡ç­¾ï¼ˆä¸æ—§æ¨¡å‹å®Œå…¨ä¸€è‡´ï¼‰
        feature_cols = [col for col in df_features.columns 
                       if col not in ['sample_id', 'label', 't1_date']]
        
        X_train = df_train[feature_cols]
        y_train = df_train['label']
        train_dates = df_train['t1_date']
        
        X_test = df_test[feature_cols]
        y_test = df_test['label']
        test_dates = df_test['t1_date']
        
        # å¤„ç†ç¼ºå¤±å€¼ï¼ˆä¸æ—§æ¨¡å‹å®Œå…¨ä¸€è‡´ï¼‰
        X_train = X_train.fillna(0)
        X_test = X_test.fillna(0)
        
        # åˆ é™¤éæ•°å€¼åˆ—ï¼ˆä¸æ—§æ¨¡å‹å®Œå…¨ä¸€è‡´ï¼‰
        non_numeric_cols = X_train.select_dtypes(include=['object']).columns
        if len(non_numeric_cols) > 0:
            log.info(f"åˆ é™¤éæ•°å€¼åˆ—: {list(non_numeric_cols)}")
            X_train = X_train.drop(columns=non_numeric_cols)
            X_test = X_test.drop(columns=non_numeric_cols)
        
        log.info(f"ç‰¹å¾çŸ©é˜µ:")
        log.info(f"  è®­ç»ƒé›†: {X_train.shape}")
        log.info(f"  æµ‹è¯•é›†: {X_test.shape}")
        log.info("")
        
        return X_train, X_test, y_train, y_test, train_dates, test_dates
    
    def _train_model(self, X_train, y_train, X_test, y_test):
        """
        è®­ç»ƒæ¨¡å‹ï¼ˆä¸æ—§æ¨¡å‹ train_xgboost_timeseries.py çš„ train_model å®Œå…¨ä¸€è‡´ï¼‰
        """
        log.info("="*80)
        log.info("ç¬¬å››æ­¥ï¼šè®­ç»ƒXGBoostæ¨¡å‹")
        log.info("="*80)
        
        # è®­ç»ƒæ¨¡å‹ï¼ˆä½¿ç”¨ä¸æ—§æ¨¡å‹å®Œå…¨ç›¸åŒçš„å‚æ•°ï¼‰
        log.info("å¼€å§‹è®­ç»ƒ...")
        
        # ä»é…ç½®è¯»å–å‚æ•°ï¼Œä½†ç¡®ä¿ä¸æ—§æ¨¡å‹å®Œå…¨ä¸€è‡´
        model_params = self.config.get('model_params', {})
        
        # å¦‚æœé…ç½®ä¸­æ²¡æœ‰å‚æ•°ï¼Œä½¿ç”¨æ—§æ¨¡å‹çš„é»˜è®¤å‚æ•°
        if not model_params:
            model_params = {
                'n_estimators': 100,
                'max_depth': 5,
                'learning_rate': 0.1,
                'subsample': 0.8,
                'colsample_bytree': 0.8,
                'min_child_weight': 3,
                'gamma': 0.1,
                'reg_alpha': 0.1,
                'reg_lambda': 1.0,
                'random_state': 42,
                'eval_metric': 'logloss'
            }
        
        model = xgb.XGBClassifier(**model_params)
        
        model.fit(
            X_train, y_train,
            eval_set=[(X_test, y_test)],
            verbose=False
        )
        
        log.success("âœ“ æ¨¡å‹è®­ç»ƒå®Œæˆï¼")
        log.info("")
        
        # é¢„æµ‹
        y_pred = model.predict(X_test)
        y_prob = model.predict_proba(X_test)[:, 1]
        
        # è¯„ä¼°ï¼ˆä¸æ—§æ¨¡å‹å®Œå…¨ä¸€è‡´ï¼‰
        log.info("="*80)
        log.info("ç¬¬äº”æ­¥ï¼šæ¨¡å‹è¯„ä¼°ï¼ˆæµ‹è¯•é›† = æœªæ¥æ•°æ®ï¼‰")
        log.info("="*80)
        
        # åˆ†ç±»æŠ¥å‘Š
        log.info("\nåˆ†ç±»æŠ¥å‘Š:")
        report = classification_report(
            y_test, y_pred, 
            target_names=['è´Ÿæ ·æœ¬', 'æ­£æ ·æœ¬'],
            output_dict=True
        )
        from sklearn.metrics import classification_report as cr_print
        print(cr_print(
            y_test, y_pred, 
            target_names=['è´Ÿæ ·æœ¬', 'æ­£æ ·æœ¬']
        ))
        
        # AUC
        auc = roc_auc_score(y_test, y_prob)
        log.info(f"\nAUC-ROC: {auc:.4f}")
        
        # æ··æ·†çŸ©é˜µ
        cm = confusion_matrix(y_test, y_pred)
        log.info("\næ··æ·†çŸ©é˜µ:")
        log.info(f"  çœŸè´Ÿä¾‹(TN): {cm[0,0]:4d}  |  å‡æ­£ä¾‹(FP): {cm[0,1]:4d}")
        log.info(f"  å‡è´Ÿä¾‹(FN): {cm[1,0]:4d}  |  çœŸæ­£ä¾‹(TP): {cm[1,1]:4d}")
        
        # ç‰¹å¾é‡è¦æ€§
        feature_importance = pd.DataFrame({
            'feature': X_train.columns,
            'importance': model.feature_importances_
        }).sort_values('importance', ascending=False)
        
        log.info("\n" + "="*80)
        log.info("ç‰¹å¾é‡è¦æ€§ Top 10:")
        log.info("="*80)
        for idx, row in feature_importance.head(10).iterrows():
            log.info(f"  {row['feature']:25s}: {row['importance']:.4f}")
        
        metrics = {
            'accuracy': report['accuracy'],
            'precision': report['æ­£æ ·æœ¬']['precision'],
            'recall': report['æ­£æ ·æœ¬']['recall'],
            'f1_score': report['æ­£æ ·æœ¬']['f1-score'],
            'auc': auc,
            'confusion_matrix': cm.tolist()
        }
        
        return model, metrics
    
    def _save_model(self, model, metrics, version, train_dates, test_dates, feature_cols=None):
        """ä¿å­˜æ¨¡å‹"""
        version_path = self.iterator.versions_path / version
        model_path = version_path / "model" / "model.json"
        model_path.parent.mkdir(parents=True, exist_ok=True)
        
        model.get_booster().save_model(str(model_path))
        log.success(f"âœ“ æ¨¡å‹å·²ä¿å­˜: {model_path}")
        
        # ä¿å­˜ç‰¹å¾åç§°
        if feature_cols is not None:
            feature_names_file = version_path / "model" / "feature_names.json"
            with open(feature_names_file, 'w', encoding='utf-8') as f:
                json.dump(feature_cols, f, indent=2, ensure_ascii=False)
            log.success(f"âœ“ ç‰¹å¾åç§°å·²ä¿å­˜: {feature_names_file}")
        
        # ä¿å­˜æŒ‡æ ‡
        metrics_file = version_path / "training" / "metrics.json"
        metrics['model_file'] = str(model_path)
        metrics['timestamp'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        metrics['train_date_range'] = f"{train_dates.min().date()} to {train_dates.max().date()}"
        metrics['test_date_range'] = f"{test_dates.min().date()} to {test_dates.max().date()}"
        
        with open(metrics_file, 'w', encoding='utf-8') as f:
            json.dump(metrics, f, indent=2, ensure_ascii=False)
        
        log.success(f"âœ“ æŒ‡æ ‡å·²ä¿å­˜: {metrics_file}")
    
    def _generate_visualizations(
        self, model, X_train, X_test, y_train, y_test,
        train_dates, test_dates, version
    ):
        """ç”Ÿæˆè®­ç»ƒè¿‡ç¨‹å¯è§†åŒ–å›¾è¡¨"""
        try:
            log.info("="*80)
            log.info("ç”Ÿæˆè®­ç»ƒå¯è§†åŒ–å›¾è¡¨")
            log.info("="*80)
            
            visualizer = TrainingVisualizer(
                output_dir=f"data/models/{self.model_name}/versions/{version}/charts"
            )
            
            # 1. æ ·æœ¬è´¨é‡å¯è§†åŒ–ï¼ˆæ­£æ ·æœ¬å’Œè´Ÿæ ·æœ¬ï¼‰
            df_positive_samples = pd.read_csv('data/training/samples/positive_samples.csv')
            visualizer.visualize_sample_quality(
                df_positive_samples, 
                save_prefix="positive_sample_quality"
            )
            
            # è´Ÿæ ·æœ¬
            if os.path.exists('data/training/samples/negative_samples_v2.csv'):
                df_negative_samples = pd.read_csv('data/training/samples/negative_samples_v2.csv')
                visualizer.visualize_sample_quality(
                    df_negative_samples,
                    save_prefix="negative_sample_quality"
                )
            
            # 2. ç‰¹å¾è´¨é‡è¯„ä¼°å¯è§†åŒ–
            visualizer.visualize_feature_quality(
                X_train, y_train, X_test, y_test,
                model_name=f"{self.model_name}_{version}"
            )
            
            # 3. å› å­é‡è¦æ€§å¯è§†åŒ–
            feature_importance = pd.DataFrame({
                'feature': X_train.columns,
                'importance': model.feature_importances_
            })
            
            visualizer.visualize_feature_importance(
                feature_importance,
                model_name=f"{self.model_name}_{version}",
                top_n=20
            )
            
            # 4. æ¨¡å‹è®­ç»ƒè¿‡ç¨‹å¯è§†åŒ–
            visualizer.visualize_training_process(
                model, X_train, y_train, X_test, y_test,
                model_name=f"{self.model_name}_{version}"
            )
            
            # 5. æ¨¡å‹ç»“æœè¯„æµ‹å¯è§†åŒ–
            y_pred = model.predict(X_test)
            y_prob = model.predict_proba(X_test)[:, 1]
            visualizer.visualize_model_results(
                y_test, y_pred, y_prob,
                model_name=f"{self.model_name}_{version}"
            )
            
            # 6. ç”Ÿæˆç´¢å¼•é¡µé¢
            visualizer.generate_index_page(model_name=f"{self.model_name}_{version}")
            
            log.success("âœ“ å¯è§†åŒ–å›¾è¡¨ç”Ÿæˆå®Œæˆ")
            log.info(f"ğŸ“Š æŸ¥çœ‹å›¾è¡¨: open data/models/{self.model_name}/versions/{version}/charts/index.html")
            
        except Exception as e:
            log.warning(f"ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨æ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
    
    def _increment_version(self, version: str) -> str:
        """é€’å¢ç‰ˆæœ¬å·"""
        version = version.lstrip('v')
        parts = version.split('.')
        if len(parts) == 3:
            parts[2] = str(int(parts[2]) + 1)
        return 'v' + '.'.join(parts)

