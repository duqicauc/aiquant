"""
å·¦ä¾§æ½œåŠ›ç‰›è‚¡æ¨¡å‹æ ¸å¿ƒç±»

æ•´åˆæ ·æœ¬ç­›é€‰ã€ç‰¹å¾å·¥ç¨‹ã€æ¨¡å‹è®­ç»ƒå’Œé¢„æµ‹çš„å®Œæ•´æµç¨‹
"""
import pandas as pd
import numpy as np
import os
import joblib
from datetime import datetime
from typing import Dict, List, Tuple, Optional
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
import xgboost as xgb

from src.utils.logger import log
from .left_positive_screener import LeftPositiveSampleScreener
from .left_negative_screener import LeftNegativeSampleScreener
from .left_feature_engineering import LeftBreakoutFeatureEngineering


class LeftBreakoutModel:
    """å·¦ä¾§æ½œåŠ›ç‰›è‚¡æ¨¡å‹"""

    def __init__(self, data_manager, config: Dict = None):
        """
        åˆå§‹åŒ–å·¦ä¾§æ¨¡å‹

        Args:
            data_manager: æ•°æ®ç®¡ç†å™¨å®ä¾‹
            config: æ¨¡å‹é…ç½®å­—å…¸
        """
        self.dm = data_manager
        self.config = config or self._get_default_config()

        # åˆå§‹åŒ–ç»„ä»¶ï¼ˆä¼ é€’é…ç½®ï¼‰
        self.positive_screener = LeftPositiveSampleScreener(data_manager, self.config)
        self.negative_screener = LeftNegativeSampleScreener(data_manager, self.config)
        self.feature_engineer = LeftBreakoutFeatureEngineering()

        # æ¨¡å‹å’Œæ•°æ®
        self.model = None
        self.feature_columns = []
        self.model_metrics = {}

        # è·¯å¾„é…ç½®
        self.model_dir = self.config.get('save', {}).get('directory', 'data/models/left_breakout')
        os.makedirs(self.model_dir, exist_ok=True)

    def _get_default_config(self) -> Dict:
        """è·å–é»˜è®¤é…ç½®"""
        return {
            'model': {
                'type': 'xgboost',
                'version': 'v1',
                'parameters': {
                    'objective': 'binary:logistic',
                    'eval_metric': 'logloss',
                    'n_estimators': 100,
                    'learning_rate': 0.1,
                    'max_depth': 6,
                    'subsample': 0.8,
                    'colsample_bytree': 0.8,
                    'gamma': 0.1,
                    'random_state': 42,
                    'n_jobs': -1
                },
                'training': {
                    'test_size': 0.2,
                    'time_series_split': True,
                    'n_splits': 5
                },
                'save': {
                    'directory': 'data/models/left_breakout',
                    'auto_backup': True
                }
            },
            'sample_preparation': {
                'start_date': '20000101',
                'end_date': None,
                'lookback_days': 34,
                'look_forward_days': 45,
                'look_forward_days': 45
            }
        }

    def prepare_samples(self, force_refresh: bool = False) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """
        å‡†å¤‡æ­£è´Ÿæ ·æœ¬æ•°æ®

        Args:
            force_refresh: æ˜¯å¦å¼ºåˆ¶é‡æ–°ç”Ÿæˆæ ·æœ¬

        Returns:
            (æ­£æ ·æœ¬DataFrame, è´Ÿæ ·æœ¬DataFrame)
        """
        log.info("å¼€å§‹å‡†å¤‡å·¦ä¾§æ½œåŠ›ç‰›è‚¡æ ·æœ¬...")

        # æ£€æŸ¥æ˜¯å¦å·²æœ‰ç¼“å­˜çš„æ ·æœ¬æ•°æ®
        positive_file = 'data/training/samples/left_positive_samples.csv'
        negative_file = 'data/training/samples/left_negative_samples.csv'

        if not force_refresh and os.path.exists(positive_file) and os.path.exists(negative_file):
            log.info("å‘ç°ç¼“å­˜çš„æ ·æœ¬æ•°æ®ï¼Œæ­£åœ¨åŠ è½½...")
            try:
                positive_samples = pd.read_csv(positive_file)
                negative_samples = pd.read_csv(negative_file)
                log.info(f"åŠ è½½å®Œæˆï¼š{len(positive_samples)} ä¸ªæ­£æ ·æœ¬ï¼Œ{len(negative_samples)} ä¸ªè´Ÿæ ·æœ¬")
                return positive_samples, negative_samples
            except Exception as e:
                log.warning(f"åŠ è½½ç¼“å­˜æ ·æœ¬å¤±è´¥: {e}")

        # é‡æ–°ç”Ÿæˆæ ·æœ¬
        log.info("é‡æ–°ç”Ÿæˆæ ·æœ¬æ•°æ®...")

        # 1. ç”Ÿæˆæ­£æ ·æœ¬
        positive_samples = self.positive_screener.screen_all_stocks(
            start_date=self.config['sample_preparation']['start_date'],
            end_date=self.config['sample_preparation']['end_date'],
            look_forward_days=self.config['sample_preparation']['look_forward_days']
        )

        if positive_samples.empty:
            log.error("æœªæ‰¾åˆ°ä»»ä½•æ­£æ ·æœ¬")
            return pd.DataFrame(), pd.DataFrame()

        log.info(f"ç”Ÿæˆæ­£æ ·æœ¬: {len(positive_samples)} ä¸ª")

        # 2. ç”Ÿæˆè´Ÿæ ·æœ¬
        negative_samples = self.negative_screener.screen_negative_samples(
            positive_samples=positive_samples,
            start_date=self.config['sample_preparation']['start_date'],
            end_date=self.config['sample_preparation']['end_date'],
            look_forward_days=self.config['sample_preparation']['look_forward_days']
        )

        log.info(f"ç”Ÿæˆè´Ÿæ ·æœ¬: {len(negative_samples)} ä¸ª")

        # 3. ä¿å­˜æ ·æœ¬æ•°æ®
        os.makedirs('data/training/samples', exist_ok=True)
        positive_samples.to_csv(positive_file, index=False)
        negative_samples.to_csv(negative_file, index=False)

        log.info("æ ·æœ¬æ•°æ®ä¿å­˜å®Œæˆ")

        return positive_samples, negative_samples

    def extract_features(self, positive_samples: pd.DataFrame, negative_samples: pd.DataFrame) -> pd.DataFrame:
        """
        ä»æ ·æœ¬æ•°æ®ä¸­æå–ç‰¹å¾

        Args:
            positive_samples: æ­£æ ·æœ¬DataFrame
            negative_samples: è´Ÿæ ·æœ¬DataFrame

        Returns:
            ç‰¹å¾DataFrame
        """
        log.info("å¼€å§‹ç‰¹å¾æå–...")

        # åˆå¹¶æ­£è´Ÿæ ·æœ¬
        # ç¡®ä¿æ­£æ ·æœ¬æœ‰labelå­—æ®µï¼ˆå€¼ä¸º1ï¼‰ï¼Œè´Ÿæ ·æœ¬æœ‰labelå­—æ®µï¼ˆå€¼ä¸º0ï¼‰
        if 'label' not in positive_samples.columns:
            positive_samples['label'] = 1
        else:
            positive_samples['label'] = 1  # ç¡®ä¿æ­£æ ·æœ¬æ ‡ç­¾ä¸º1
        
        if 'label' not in negative_samples.columns:
            negative_samples['label'] = 0
        else:
            negative_samples['label'] = 0  # ç¡®ä¿è´Ÿæ ·æœ¬æ ‡ç­¾ä¸º0
        
        all_samples = pd.concat([positive_samples, negative_samples], ignore_index=True)
        all_samples['unique_sample_id'] = range(len(all_samples))

        log.info(f"æ€»æ ·æœ¬æ•°: {len(all_samples)} (æ­£æ ·æœ¬: {len(positive_samples)}, è´Ÿæ ·æœ¬: {len(negative_samples)})")

        # ä¸ºæ¯ä¸ªæ ·æœ¬æå–34å¤©ç‰¹å¾æ•°æ®
        feature_data_list = []

        failed_count = 0
        success_count = 0
        
        for idx, sample in all_samples.iterrows():
            if idx % 50 == 0:
                log.info(f"å¤„ç†æ ·æœ¬ {idx + 1}/{len(all_samples)} (æˆåŠŸ: {success_count}, å¤±è´¥: {failed_count})")

            try:
                # è·å–è¯¥æ ·æœ¬çš„34å¤©åŸå§‹æ•°æ®
                sample_features = self._extract_single_sample_raw_data(sample)
                if not sample_features.empty:
                    feature_data_list.append(sample_features)
                    success_count += 1
                else:
                    failed_count += 1
                    log.debug(f"æ ·æœ¬ {sample['ts_code']} {sample['t0_date']} è¿”å›ç©ºæ•°æ®ï¼Œè·³è¿‡")

            except Exception as e:
                failed_count += 1
                error_msg = str(e)[:200] if str(e) else type(e).__name__
                log.warning(f"æ ·æœ¬ {sample['ts_code']} {sample['t0_date']} ç‰¹å¾æå–å¤±è´¥: {error_msg}")
                # å¦‚æœæ˜¯APIé”™è¯¯ï¼Œç­‰å¾…ä¸€ä¸‹å†ç»§ç»­
                if "API" in error_msg or "ERROR" in error_msg or "è°ƒç”¨å¤±è´¥" in error_msg:
                    import time
                    time.sleep(1)  # çŸ­æš‚ç­‰å¾…ï¼Œé¿å…è¿ç»­å¤±è´¥
                continue
        
        log.info(f"ç‰¹å¾æå–å®Œæˆ: æˆåŠŸ {success_count} ä¸ªï¼Œå¤±è´¥ {failed_count} ä¸ª")

        if not feature_data_list:
            log.error("æ²¡æœ‰æˆåŠŸæå–åˆ°ä»»ä½•ç‰¹å¾æ•°æ®")
            return pd.DataFrame()

        # åˆå¹¶æ‰€æœ‰æ ·æœ¬çš„ç‰¹å¾æ•°æ®
        raw_feature_data = pd.concat(feature_data_list, ignore_index=True)
        log.info(f"åŸå§‹ç‰¹å¾æ•°æ®è¡Œæ•°: {len(raw_feature_data)}")

        # ä½¿ç”¨ç‰¹å¾å·¥ç¨‹å™¨æå–ç»Ÿè®¡ç‰¹å¾
        final_features = self.feature_engineer.extract_features(raw_feature_data)

        # ä¿å­˜ç‰¹å¾æ•°æ®
        os.makedirs('data/training/features', exist_ok=True)
        final_features.to_csv('data/training/features/left_breakout_features.csv', index=False)

        log.info(f"ç‰¹å¾æå–å®Œæˆï¼Œæœ€ç»ˆç‰¹å¾ç»´åº¦: {len(final_features)} è¡Œ Ã— {len(final_features.columns)} åˆ—")

        return final_features

    def _extract_single_sample_raw_data(self, sample) -> pd.DataFrame:
        """
        æå–å•ä¸ªæ ·æœ¬çš„34å¤©åŸå§‹æ•°æ®

        Args:
            sample: æ ·æœ¬è®°å½•

        Returns:
            34å¤©åŸå§‹æ•°æ®çš„DataFrame
        """
        ts_code = sample['ts_code']
        t0_date = sample['t0_date']

        try:
            # è®¡ç®—æ•°æ®è·å–çš„æ—¶é—´èŒƒå›´ï¼ˆT0å‰34å¤©åˆ°T0ï¼‰
            end_date = str(t0_date)  # ç¡®ä¿æ˜¯å­—ç¬¦ä¸²æ ¼å¼

            # è®¡ç®—å¤§çº¦34ä¸ªäº¤æ˜“æ—¥å¯¹åº”çš„æ—¥å†å¤©æ•°ï¼ˆå¤§çº¦45-50å¤©ï¼‰
            import datetime
            end_dt = datetime.datetime.strptime(end_date, '%Y%m%d')
            start_dt = end_dt - datetime.timedelta(days=60)  # å¤šå–ä¸€äº›å¤©æ•°ä»¥ç¡®ä¿æœ‰è¶³å¤Ÿäº¤æ˜“æ—¥
            start_date = start_dt.strftime('%Y%m%d')

            # è·å–äº¤æ˜“æ—¥å†ï¼ˆå¸¦é‡è¯•ï¼‰
            try:
                calendar_df = self.dm.get_trade_calendar(start_date, end_date)
            except Exception as e:
                log.debug(f"è·å–äº¤æ˜“æ—¥å†å¤±è´¥ {ts_code} {t0_date}: {e}")
                return pd.DataFrame()
                
            if calendar_df.empty:
                return pd.DataFrame()

            # ç­›é€‰äº¤æ˜“æ—¥
            trading_days = calendar_df[calendar_df['is_open'] == 1]['cal_date'].sort_values().tolist()
            if len(trading_days) < 20:  # æœ€å°‘éœ€è¦20å¤©æ•°æ®
                return pd.DataFrame()

            # å–æœ€è¿‘çš„34ä¸ªäº¤æ˜“æ—¥
            recent_trading_days = trading_days[-34:] if len(trading_days) >= 34 else trading_days
            start_date = recent_trading_days[0]

            # è·å–æ—¥çº¿æ•°æ®å’ŒæŠ€æœ¯æŒ‡æ ‡ï¼ˆå¸¦é”™è¯¯å¤„ç†ï¼‰
            try:
                df = self.dm.get_complete_data(ts_code, start_date, end_date)
            except Exception as e:
                error_msg = str(e)[:200] if str(e) else type(e).__name__
                log.debug(f"è·å–æ—¥çº¿æ•°æ®å¤±è´¥ {ts_code} {t0_date}: {error_msg}")
                return pd.DataFrame()
                
            if df.empty or len(df) < 20:
                return pd.DataFrame()

            # è·å–æŠ€æœ¯å› å­æ•°æ®ï¼ˆå¯é€‰ï¼Œå¤±è´¥ä¸å½±å“ï¼‰
            try:
                df_factor = self.dm.get_stk_factor(ts_code, start_date, end_date)
                if not df_factor.empty:
                    df = pd.merge(df, df_factor, on='trade_date', how='left')
            except Exception as e:
                log.debug(f"è·å–æŠ€æœ¯å› å­å¤±è´¥ {ts_code} {t0_date}ï¼Œç»§ç»­ä½¿ç”¨åŸºç¡€æ•°æ®: {e}")

            # æ·»åŠ æ ·æœ¬æ ‡è¯†å’Œæ ‡ç­¾
            df['unique_sample_id'] = sample['unique_sample_id']
            df['ts_code'] = ts_code
            df['name'] = sample['name']
            df['t0_date'] = t0_date
            df['label'] = sample['label']

            # æ·»åŠ days_to_t1å­—æ®µï¼ˆè·ç¦»T0çš„å¤©æ•°ï¼‰
            df['trade_date_dt'] = pd.to_datetime(df['trade_date'])
            # ä¿®å¤ï¼št0_dateå¯èƒ½æ˜¯intç±»å‹ï¼Œéœ€è¦å…ˆè½¬æ¢ä¸ºå­—ç¬¦ä¸²å†è§£æ
            t0_dt = pd.to_datetime(str(t0_date), format='%Y%m%d')
            df['days_to_t1'] = (df['trade_date_dt'] - t0_dt).dt.days

            # åªä¿ç•™T0å‰çš„34å¤©æ•°æ®
            df = df[df['days_to_t1'] <= 0].tail(34).reset_index(drop=True)

            return df

        except Exception as e:
            error_msg = str(e)[:200] if str(e) else type(e).__name__
            log.debug(f"æå–æ ·æœ¬ {ts_code} {t0_date} åŸå§‹æ•°æ®å¤±è´¥: {error_msg}")
            return pd.DataFrame()

    def train_model(self, features_df: pd.DataFrame) -> Dict:
        """
        è®­ç»ƒXGBoostæ¨¡å‹

        Args:
            features_df: ç‰¹å¾DataFrame

        Returns:
            è®­ç»ƒç»“æœå­—å…¸
        """
        log.info("å¼€å§‹è®­ç»ƒå·¦ä¾§æ½œåŠ›ç‰›è‚¡æ¨¡å‹...")

        if features_df.empty:
            log.error("ç‰¹å¾æ•°æ®ä¸ºç©ºï¼Œæ— æ³•è®­ç»ƒæ¨¡å‹")
            return {}

        # å‡†å¤‡è®­ç»ƒæ•°æ®
        # å…ˆæ¸…ç†æ‰labelä¸ºNaNçš„æ ·æœ¬
        features_df = features_df.dropna(subset=['label'])
        if features_df.empty:
            log.error("æ¸…ç†NaNåç‰¹å¾æ•°æ®ä¸ºç©ºï¼Œæ— æ³•è®­ç»ƒæ¨¡å‹")
            return {}
        
        feature_cols = [col for col in features_df.columns
                       if col not in ['unique_sample_id', 'ts_code', 'name', 't0_date', 'label']]

        X = features_df[feature_cols].values
        y = features_df['label'].values

        # ç¡®ä¿yæ˜¯æ•´æ•°ç±»å‹ï¼Œæ²¡æœ‰NaN
        y = y.astype(int)
        
        # æ¸…ç†ç‰¹å¾æ•°æ®ä¸­çš„infå’ŒNaNå€¼
        import numpy as np
        # æ›¿æ¢infä¸ºNaN
        X = np.where(np.isinf(X), np.nan, X)
        # æ›¿æ¢NaNä¸º0ï¼ˆæˆ–è€…ä½¿ç”¨ä¸­ä½æ•°å¡«å……ï¼‰
        X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å¼‚å¸¸å€¼
        if np.any(np.isinf(X)) or np.any(np.isnan(X)):
            log.warning("ç‰¹å¾æ•°æ®ä¸­ä»æœ‰å¼‚å¸¸å€¼ï¼Œä½¿ç”¨æ›´ä¸¥æ ¼çš„æ¸…ç†")
            X = np.nan_to_num(X, nan=0.0, posinf=1e6, neginf=-1e6)
        
        log.info(f"ç‰¹å¾æ•°æ®æ¸…ç†å®Œæˆ: {X.shape}, å¼‚å¸¸å€¼å·²å¤„ç†")
        
        log.info(f"è®­ç»ƒæ•°æ®ç»´åº¦: {X.shape[0]} æ ·æœ¬ Ã— {X.shape[1]} ç‰¹å¾")
        log.info(f"æ­£æ ·æœ¬æ¯”ä¾‹: {np.mean(y):.3f}")

        # æ—¶é—´åºåˆ—åˆ†å‰²
        if self.config['model']['training']['time_series_split']:
            tscv = TimeSeriesSplit(n_splits=self.config['model']['training']['n_splits'])
            splits = list(tscv.split(X))

            # ä½¿ç”¨æœ€åä¸€ä¸ªåˆ†å‰²ä½œä¸ºè®­ç»ƒ/æµ‹è¯•é›†
            train_idx, test_idx = splits[-1]
        else:
            # ç®€å•åˆ†å‰²ï¼ˆéæ—¶é—´åºåˆ—ï¼‰
            split_point = int(len(X) * (1 - self.config['model']['training']['test_size']))
            train_idx = np.arange(split_point)
            test_idx = np.arange(split_point, len(X))

        X_train, X_test = X[train_idx], X[test_idx]
        y_train, y_test = y[train_idx], y[test_idx]

        log.info(f"è®­ç»ƒé›†: {len(X_train)} æ ·æœ¬, æµ‹è¯•é›†: {len(X_test)} æ ·æœ¬")

        # è®­ç»ƒæ¨¡å‹
        model_params = self.config['model']['parameters']
        self.model = xgb.XGBClassifier(**model_params)

        log.info("å¼€å§‹æ¨¡å‹è®­ç»ƒ...")
        self.model.fit(X_train, y_train)

        # è¯„ä¼°æ¨¡å‹
        train_metrics = self._evaluate_model(self.model, X_train, y_train, "è®­ç»ƒé›†")
        test_metrics = self._evaluate_model(self.model, X_test, y_test, "æµ‹è¯•é›†")

        # åˆ†æç‰¹å¾é‡è¦æ€§
        log.info("ğŸ“Š åˆ†æç‰¹å¾é‡è¦æ€§...")
        feature_importance = self._analyze_feature_importance(feature_cols)
        
        # ä¿å­˜ç‰¹å¾é‡è¦æ€§
        importance_path = os.path.join(self.model_dir, f"feature_importance_{self.config['model']['version']}.csv")
        feature_importance.to_csv(importance_path, index=False, encoding='utf-8')
        log.info(f"ç‰¹å¾é‡è¦æ€§å·²ä¿å­˜è‡³: {importance_path}")
        
        # æ˜¾ç¤ºTop 20é‡è¦ç‰¹å¾
        log.info("\n" + "="*60)
        log.info("ğŸ† Top 20 é‡è¦ç‰¹å¾:")
        log.info("="*60)
        for idx, row in feature_importance.head(20).iterrows():
            log.info(f"  {idx+1:2d}. {row['feature']:30s}: {row['importance']:.6f} ({row['importance_pct']:.2f}%)")
        log.info("="*60)

        # ä¿å­˜æ¨¡å‹
        model_path = os.path.join(self.model_dir, f"left_breakout_{self.config['model']['version']}.joblib")
        joblib.dump(self.model, model_path)
        log.info(f"æ¨¡å‹å·²ä¿å­˜è‡³: {model_path}")

        # ä¿å­˜ç‰¹å¾åˆ—å
        self.feature_columns = feature_cols
        feature_cols_path = os.path.join(self.model_dir, f"feature_columns_{self.config['model']['version']}.txt")
        with open(feature_cols_path, 'w') as f:
            f.write('\n'.join(feature_cols))

        # ç¼–è¯‘è®­ç»ƒç»“æœ
        training_results = {
            'model_path': model_path,
            'feature_columns_path': feature_cols_path,
            'feature_importance_path': importance_path,
            'feature_columns': feature_cols,
            'feature_importance': feature_importance.to_dict('records'),
            'top_features': feature_importance.head(20).to_dict('records'),
            'train_samples': len(X_train),
            'test_samples': len(X_test),
            'train_metrics': train_metrics,
            'test_metrics': test_metrics,
            'training_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }

        # ä¿å­˜è®­ç»ƒæŠ¥å‘Š
        self._save_training_report(training_results)

        log.info("æ¨¡å‹è®­ç»ƒå®Œæˆ")
        return training_results

    def _analyze_feature_importance(self, feature_cols: List[str]) -> pd.DataFrame:
        """
        åˆ†æç‰¹å¾é‡è¦æ€§
        
        Args:
            feature_cols: ç‰¹å¾åˆ—ååˆ—è¡¨
            
        Returns:
            ç‰¹å¾é‡è¦æ€§DataFrameï¼ŒåŒ…å«ç‰¹å¾åã€é‡è¦æ€§åˆ†æ•°ã€ç™¾åˆ†æ¯”å’Œæ’å
        """
        if self.model is None:
            log.error("æ¨¡å‹æœªè®­ç»ƒ")
            return pd.DataFrame()
        
        try:
            # è·å–ç‰¹å¾é‡è¦æ€§åˆ†æ•°
            importance_scores = self.model.feature_importances_
            
            # è®¡ç®—æ€»é‡è¦æ€§ï¼ˆç”¨äºè®¡ç®—ç™¾åˆ†æ¯”ï¼‰
            total_importance = np.sum(importance_scores)
            
            # æ„å»ºDataFrame
            importance_df = pd.DataFrame({
                'feature': feature_cols,
                'importance': importance_scores,
                'importance_pct': (importance_scores / total_importance * 100) if total_importance > 0 else 0
            })
            
            # æŒ‰é‡è¦æ€§æ’åº
            importance_df = importance_df.sort_values('importance', ascending=False).reset_index(drop=True)
            
            # æ·»åŠ æ’å
            importance_df['rank'] = range(1, len(importance_df) + 1)
            
            return importance_df
            
        except Exception as e:
            log.error(f"åˆ†æç‰¹å¾é‡è¦æ€§å¤±è´¥: {e}")
            return pd.DataFrame()

    def _evaluate_model(self, model, X: np.ndarray, y: np.ndarray, dataset_name: str) -> Dict:
        """
        è¯„ä¼°æ¨¡å‹æ€§èƒ½

        Args:
            model: è®­ç»ƒå¥½çš„æ¨¡å‹
            X: ç‰¹å¾çŸ©é˜µ
            y: æ ‡ç­¾å‘é‡
            dataset_name: æ•°æ®é›†åç§°

        Returns:
            è¯„ä¼°æŒ‡æ ‡å­—å…¸
        """
        try:
            # é¢„æµ‹
            y_pred_proba = model.predict_proba(X)[:, 1]
            y_pred = (y_pred_proba > 0.5).astype(int)

            # è®¡ç®—æŒ‡æ ‡
            metrics = {
                'accuracy': accuracy_score(y, y_pred),
                'precision': precision_score(y, y_pred, zero_division=0),
                'recall': recall_score(y, y_pred, zero_division=0),
                'f1_score': f1_score(y, y_pred, zero_division=0),
                'auc_roc': roc_auc_score(y, y_pred_proba)
            }

            log.info(f"{dataset_name}è¯„ä¼°ç»“æœ:")
            for metric, value in metrics.items():
                log.info(f"  {metric}: {value:.4f}")

            return metrics

        except Exception as e:
            log.error(f"æ¨¡å‹è¯„ä¼°å¤±è´¥: {e}")
            return {}

    def _save_training_report(self, training_results: Dict):
        """ä¿å­˜è®­ç»ƒæŠ¥å‘Š"""
        try:
            report_path = os.path.join(self.model_dir, f"training_report_{self.config['model']['version']}.txt")

            with open(report_path, 'w', encoding='utf-8') as f:
                f.write("="*80 + "\n")
                f.write("å·¦ä¾§æ½œåŠ›ç‰›è‚¡æ¨¡å‹è®­ç»ƒæŠ¥å‘Š\n")
                f.write("="*80 + "\n\n")

                f.write(f"è®­ç»ƒæ—¶é—´: {training_results['training_time']}\n")
                f.write(f"æ¨¡å‹ç‰ˆæœ¬: {self.config['model']['version']}\n")
                f.write(f"æ¨¡å‹è·¯å¾„: {training_results['model_path']}\n\n")

                f.write("æ•°æ®ç»Ÿè®¡:\n")
                f.write(f"  è®­ç»ƒæ ·æœ¬: {training_results['train_samples']}\n")
                f.write(f"  æµ‹è¯•æ ·æœ¬: {training_results['test_samples']}\n")
                f.write(f"  ç‰¹å¾æ•°é‡: {len(training_results['feature_columns'])}\n\n")

                f.write("è®­ç»ƒé›†æ€§èƒ½:\n")
                for metric, value in training_results['train_metrics'].items():
                    f.write(f"  {metric}: {value:.4f}\n")

                f.write("\næµ‹è¯•é›†æ€§èƒ½:\n")
                for metric, value in training_results['test_metrics'].items():
                    f.write(f"  {metric}: {value:.4f}\n")

                f.write("\næ¨¡å‹å‚æ•°:\n")
                for param, value in self.config['model']['parameters'].items():
                    f.write(f"  {param}: {value}\n")

                f.write("\nç‰¹å¾åˆ—è¡¨:\n")
                for i, feature in enumerate(training_results['feature_columns'], 1):
                    f.write(f"  {i:2d}. {feature}\n")

                # æ·»åŠ ç‰¹å¾é‡è¦æ€§åˆ†æ
                if 'feature_importance' in training_results and training_results['feature_importance']:
                    f.write("\n" + "="*80 + "\n")
                    f.write("ç‰¹å¾é‡è¦æ€§åˆ†æ (Top 20)\n")
                    f.write("="*80 + "\n")
                    f.write(f"{'æ’å':<6} {'ç‰¹å¾å':<35} {'é‡è¦æ€§åˆ†æ•°':<15} {'å æ¯”(%)':<10}\n")
                    f.write("-"*80 + "\n")
                    
                    top_features = training_results.get('top_features', [])
                    for feat in top_features[:20]:
                        f.write(f"{feat.get('rank', 0):<6} {feat.get('feature', ''):<35} "
                               f"{feat.get('importance', 0):<15.6f} {feat.get('importance_pct', 0):<10.2f}\n")
                    
                    f.write("\nç‰¹å¾é‡è¦æ€§ç»Ÿè®¡:\n")
                    if top_features:
                        total_importance = sum(f.get('importance', 0) for f in top_features[:20])
                        f.write(f"  Top 20ç‰¹å¾æ€»é‡è¦æ€§: {total_importance:.6f}\n")
                        f.write(f"  Top 20ç‰¹å¾å æ¯”: {sum(f.get('importance_pct', 0) for f in top_features[:20]):.2f}%\n")

            log.info(f"è®­ç»ƒæŠ¥å‘Šå·²ä¿å­˜è‡³: {report_path}")

        except Exception as e:
            log.error(f"ä¿å­˜è®­ç»ƒæŠ¥å‘Šå¤±è´¥: {e}")

    def load_model(self, version: str = None) -> bool:
        """
        åŠ è½½å·²è®­ç»ƒçš„æ¨¡å‹

        Args:
            version: æ¨¡å‹ç‰ˆæœ¬ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®ä¸­çš„ç‰ˆæœ¬

        Returns:
            æ˜¯å¦åŠ è½½æˆåŠŸ
        """
        if version is None:
            version = self.config['model']['version']

        try:
            model_path = os.path.join(self.model_dir, f"left_breakout_{version}.joblib")
            feature_cols_path = os.path.join(self.model_dir, f"feature_columns_{version}.txt")

            if not os.path.exists(model_path):
                log.error(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
                return False

            # åŠ è½½æ¨¡å‹
            self.model = joblib.load(model_path)
            log.info(f"æ¨¡å‹å·²åŠ è½½: {model_path}")

            # åŠ è½½ç‰¹å¾åˆ—
            if os.path.exists(feature_cols_path):
                with open(feature_cols_path, 'r') as f:
                    self.feature_columns = [line.strip() for line in f.readlines()]
                log.info(f"ç‰¹å¾åˆ—å·²åŠ è½½: {len(self.feature_columns)} ä¸ªç‰¹å¾")
            else:
                log.warning(f"ç‰¹å¾åˆ—æ–‡ä»¶ä¸å­˜åœ¨: {feature_cols_path}")

            return True

        except Exception as e:
            log.error(f"åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
            return False

    def predict_stocks(self, stock_features: pd.DataFrame) -> pd.DataFrame:
        """
        å¯¹è‚¡ç¥¨è¿›è¡Œé¢„æµ‹è¯„åˆ†

        Args:
            stock_features: è‚¡ç¥¨ç‰¹å¾DataFrame

        Returns:
            é¢„æµ‹ç»“æœDataFrame
        """
        if self.model is None:
            log.error("æ¨¡å‹æœªåŠ è½½ï¼Œæ— æ³•è¿›è¡Œé¢„æµ‹")
            return pd.DataFrame()

        if stock_features.empty:
            log.warning("è¾“å…¥ç‰¹å¾æ•°æ®ä¸ºç©º")
            return pd.DataFrame()

        try:
            # å‡†å¤‡ç‰¹å¾æ•°æ®
            available_features = [col for col in self.feature_columns if col in stock_features.columns]

            if len(available_features) == 0:
                log.error("æ²¡æœ‰å¯ç”¨çš„ç‰¹å¾åˆ—")
                return pd.DataFrame()

            if len(available_features) < len(self.feature_columns):
                log.warning(f"ç¼ºå°‘ {len(self.feature_columns) - len(available_features)} ä¸ªç‰¹å¾åˆ—")

            X = stock_features[available_features].values

            # é¢„æµ‹æ¦‚ç‡
            probabilities = self.model.predict_proba(X)[:, 1]

            # æ„å»ºç»“æœ
            results = stock_features[['ts_code', 'name']].copy()
            results['probability'] = probabilities
            results['prediction_time'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')

            # æŒ‰æ¦‚ç‡æ’åº
            results = results.sort_values('probability', ascending=False).reset_index(drop=True)

            log.info(f"é¢„æµ‹å®Œæˆï¼Œå…± {len(results)} åªè‚¡ç¥¨")

            return results

        except Exception as e:
            log.error(f"é¢„æµ‹å¤±è´¥: {e}")
            return pd.DataFrame()

    def get_feature_importance(self) -> pd.DataFrame:
        """
        è·å–ç‰¹å¾é‡è¦æ€§

        Returns:
            ç‰¹å¾é‡è¦æ€§DataFrame
        """
        if self.model is None:
            log.error("æ¨¡å‹æœªåŠ è½½")
            return pd.DataFrame()

        try:
            importance_scores = self.model.feature_importances_
            importance_df = pd.DataFrame({
                'feature': self.feature_columns,
                'importance': importance_scores
            })

            importance_df = importance_df.sort_values('importance', ascending=False).reset_index(drop=True)

            return importance_df

        except Exception as e:
            log.error(f"è·å–ç‰¹å¾é‡è¦æ€§å¤±è´¥: {e}")
            return pd.DataFrame()
