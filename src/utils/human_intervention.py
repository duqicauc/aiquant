"""
äººå·¥ä»‹å…¥æé†’å·¥å…·

åœ¨éœ€è¦äººå·¥å†³ç­–çš„å…³é”®ç¯èŠ‚ï¼Œæä¾›æ˜ç¡®çš„æé†’å’Œæ£€æŸ¥æœºåˆ¶
"""
from typing import List, Dict, Optional
from datetime import datetime
from pathlib import Path
import json
from src.utils.logger import log


class HumanInterventionChecker:
    """äººå·¥ä»‹å…¥æ£€æŸ¥å™¨"""
    
    def __init__(self):
        self.interventions = []
    
    def check_positive_sample_criteria(self, config_path: str = 'config/settings.yaml') -> Dict:
        """
        æ£€æŸ¥æ­£æ ·æœ¬ç­›é€‰æ¡ä»¶æ˜¯å¦éœ€è¦äººå·¥ç¡®è®¤
        
        Returns:
            æ£€æŸ¥ç»“æœå­—å…¸
        """
        from config.settings import settings
        
        criteria = settings.get('data.sample_preparation.positive_criteria', {})
        
        # æ£€æŸ¥å…³é”®é˜ˆå€¼æ˜¯å¦ä½¿ç”¨é»˜è®¤å€¼
        default_values = {
            'consecutive_weeks': 3,
            'total_return_threshold': 50,
            'max_return_threshold': 70,
            'min_listing_days': 180
        }
        
        warnings = []
        suggestions = []
        
        for key, default_value in default_values.items():
            current_value = criteria.get(key, default_value)
            if current_value == default_value:
                warnings.append(
                    f"âš ï¸  ä½¿ç”¨é»˜è®¤å€¼: {key} = {current_value}ã€‚"
                    f"è¯·ç¡®è®¤æ˜¯å¦ç¬¦åˆå½“å‰éœ€æ±‚ã€‚"
                )
        
        # æ£€æŸ¥æ—¥æœŸèŒƒå›´
        start_date = settings.get('data.sample_preparation.start_date', '20000101')
        end_date = settings.get('data.sample_preparation.end_date', None)
        
        if start_date == '20000101':
            suggestions.append(
                "ğŸ’¡ æ•°æ®èµ·å§‹æ—¥æœŸä¸º2000-01-01ï¼Œè¯·ç¡®è®¤æ˜¯å¦éœ€è¦è°ƒæ•´ã€‚"
            )
        
        return {
            'needs_intervention': len(warnings) > 0,
            'warnings': warnings,
            'suggestions': suggestions,
            'criteria': criteria
        }
    
    def check_feature_selection(self, feature_extraction_file: str = None) -> Dict:
        """
        æ£€æŸ¥ç‰¹å¾é€‰æ‹©æ˜¯å¦éœ€è¦äººå·¥ç¡®è®¤
        
        Returns:
            æ£€æŸ¥ç»“æœå­—å…¸
        """
        warnings = []
        suggestions = []
        
        # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨äº†åŸºç¡€ç‰¹å¾
        basic_features = [
            'close_mean', 'close_std', 'pct_chg_mean', 
            'volume_ratio_mean', 'macd_mean'
        ]
        
        suggestions.append(
            "ğŸ’¡ å½“å‰ä½¿ç”¨åŸºç¡€ç‰¹å¾é›†ã€‚è€ƒè™‘æ·»åŠ ï¼š\n"
            "  - åŸºæœ¬é¢ç‰¹å¾ï¼ˆPEã€PBã€ROEç­‰ï¼‰\n"
            "  - å…¶ä»–æŠ€æœ¯æŒ‡æ ‡ï¼ˆKDJã€OBVã€å¸ƒæ—å¸¦ç­‰ï¼‰\n"
            "  - è¡Œä¸šç‰¹å¾\n"
            "  - å¸‚åœºæƒ…ç»ªç‰¹å¾"
        )
        
        return {
            'needs_intervention': False,  # ç‰¹å¾é€‰æ‹©æ˜¯æŒç»­ä¼˜åŒ–è¿‡ç¨‹
            'warnings': warnings,
            'suggestions': suggestions
        }
    
    def check_model_config(self, model_name: str, config_path: str = None) -> Dict:
        """
        æ£€æŸ¥æ¨¡å‹é…ç½®æ˜¯å¦éœ€è¦äººå·¥ç¡®è®¤
        
        Args:
            model_name: æ¨¡å‹åç§°
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
            
        Returns:
            æ£€æŸ¥ç»“æœå­—å…¸
        """
        if config_path is None:
            config_path = f"config/models/{model_name}.yaml"
        
        config_file = Path(config_path)
        if not config_file.exists():
            return {
                'needs_intervention': True,
                'warnings': [f"âš ï¸  é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}ã€‚è¯·å…ˆåˆ›å»ºé…ç½®æ–‡ä»¶ã€‚"],
                'suggestions': []
            }
        
        import yaml
        with open(config_file, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        
        warnings = []
        suggestions = []
        
        # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨é»˜è®¤è¶…å‚æ•°
        model_params = config.get('model_params', {})
        default_params = {
            'n_estimators': 100,
            'max_depth': 5,
            'learning_rate': 0.1
        }
        
        for key, default_value in default_params.items():
            current_value = model_params.get(key, default_value)
            if current_value == default_value:
                warnings.append(
                    f"âš ï¸  ä½¿ç”¨é»˜è®¤è¶…å‚æ•°: {key} = {current_value}ã€‚"
                    f"å»ºè®®æ ¹æ®æ•°æ®ç‰¹ç‚¹è¿›è¡Œè°ƒä¼˜ã€‚"
                )
        
        # æ£€æŸ¥ç®—æ³•ç±»å‹
        model_type = config.get('model', {}).get('type', 'xgboost')
        if model_type == 'xgboost':
            suggestions.append(
                "ğŸ’¡ å½“å‰ä½¿ç”¨XGBoostã€‚è€ƒè™‘å°è¯•ï¼š\n"
                "  - LightGBMï¼ˆé€Ÿåº¦æ›´å¿«ï¼‰\n"
                "  - CatBoostï¼ˆå¤„ç†ç±»åˆ«ç‰¹å¾ï¼‰\n"
                "  - é›†æˆæ¨¡å‹ï¼ˆå¤šæ¨¡å‹èåˆï¼‰"
            )
        
        return {
            'needs_intervention': len(warnings) > 0,
            'warnings': warnings,
            'suggestions': suggestions,
            'config': config
        }
    
    def check_training_results(self, model_name: str, version: str) -> Dict:
        """
        æ£€æŸ¥è®­ç»ƒç»“æœæ˜¯å¦éœ€è¦äººå·¥ä»‹å…¥
        
        Args:
            model_name: æ¨¡å‹åç§°
            version: ç‰ˆæœ¬å·
            
        Returns:
            æ£€æŸ¥ç»“æœå­—å…¸
        """
        from src.models.lifecycle.iterator import ModelIterator
        
        try:
            iterator = ModelIterator(model_name)
            info = iterator.get_version_info(version)
            
            metrics = info.get('metrics', {})
            test_metrics = metrics.get('test', {})
            
            warnings = []
            suggestions = []
            
            # æ£€æŸ¥æŒ‡æ ‡æ˜¯å¦è¾¾æ ‡
            auc = test_metrics.get('auc', 0)
            accuracy = test_metrics.get('accuracy', 0)
            f1 = test_metrics.get('f1', 0)
            
            if auc < 0.7:
                warnings.append(
                    f"âš ï¸  AUC = {auc:.3f} < 0.7ï¼Œæ¨¡å‹æ€§èƒ½å¯èƒ½ä¸ä½³ã€‚"
                    f"å»ºè®®æ£€æŸ¥ç‰¹å¾é€‰æ‹©æˆ–è°ƒæ•´è¶…å‚æ•°ã€‚"
                )
            
            if accuracy < 0.75:
                warnings.append(
                    f"âš ï¸  å‡†ç¡®ç‡ = {accuracy:.3f} < 0.75ï¼Œæ¨¡å‹æ€§èƒ½å¯èƒ½ä¸ä½³ã€‚"
                    f"å»ºè®®æ£€æŸ¥æ•°æ®è´¨é‡æˆ–è°ƒæ•´æ¨¡å‹ã€‚"
                )
            
            if f1 < 0.7:
                warnings.append(
                    f"âš ï¸  F1åˆ†æ•° = {f1:.3f} < 0.7ï¼Œæ¨¡å‹å¯èƒ½å­˜åœ¨è¿‡æ‹Ÿåˆæˆ–æ¬ æ‹Ÿåˆã€‚"
                    f"å»ºè®®è°ƒæ•´æ¨¡å‹å‚æ•°ã€‚"
                )
            
            # æ£€æŸ¥æ˜¯å¦è¿‡æ‹Ÿåˆ
            train_metrics = metrics.get('training', {})
            train_accuracy = train_metrics.get('accuracy', 0)
            if train_accuracy - accuracy > 0.15:
                warnings.append(
                    f"âš ï¸  è®­ç»ƒå‡†ç¡®ç‡({train_accuracy:.3f})ä¸æµ‹è¯•å‡†ç¡®ç‡({accuracy:.3f})å·®è·è¾ƒå¤§ï¼Œ"
                    f"å¯èƒ½å­˜åœ¨è¿‡æ‹Ÿåˆã€‚å»ºè®®å¢åŠ æ­£åˆ™åŒ–æˆ–å‡å°‘æ¨¡å‹å¤æ‚åº¦ã€‚"
                )
            
            return {
                'needs_intervention': len(warnings) > 0,
                'warnings': warnings,
                'suggestions': suggestions,
                'metrics': test_metrics
            }
        except Exception as e:
            return {
                'needs_intervention': True,
                'warnings': [f"âš ï¸  æ— æ³•è·å–è®­ç»ƒç»“æœ: {e}"],
                'suggestions': []
            }
    
    def check_version_comparison(self, model_name: str, old_version: str, new_version: str) -> Dict:
        """
        æ£€æŸ¥ç‰ˆæœ¬å¯¹æ¯”ç»“æœï¼Œæé†’æ˜¯å¦éœ€è¦äººå·¥å†³ç­–
        
        Args:
            model_name: æ¨¡å‹åç§°
            old_version: æ—§ç‰ˆæœ¬å·
            new_version: æ–°ç‰ˆæœ¬å·
            
        Returns:
            æ£€æŸ¥ç»“æœå­—å…¸
        """
        from src.models.lifecycle.iterator import ModelIterator
        
        try:
            iterator = ModelIterator(model_name)
            old_info = iterator.get_version_info(old_version)
            new_info = iterator.get_version_info(new_version)
            
            old_metrics = old_info.get('metrics', {}).get('test', {})
            new_metrics = new_info.get('metrics', {}).get('test', {})
            
            warnings = []
            suggestions = []
            
            # å¯¹æ¯”å…³é”®æŒ‡æ ‡
            old_auc = old_metrics.get('auc', 0)
            new_auc = new_metrics.get('auc', 0)
            
            old_accuracy = old_metrics.get('accuracy', 0)
            new_accuracy = new_metrics.get('accuracy', 0)
            
            if new_auc < old_auc:
                warnings.append(
                    f"âš ï¸  æ–°ç‰ˆæœ¬AUC({new_auc:.3f}) < æ—§ç‰ˆæœ¬AUC({old_auc:.3f})ï¼Œ"
                    f"æ€§èƒ½ä¸‹é™ã€‚å»ºè®®å›æ»šæˆ–ç»§ç»­ä¼˜åŒ–ã€‚"
                )
            
            if new_accuracy < old_accuracy:
                warnings.append(
                    f"âš ï¸  æ–°ç‰ˆæœ¬å‡†ç¡®ç‡({new_accuracy:.3f}) < æ—§ç‰ˆæœ¬å‡†ç¡®ç‡({old_accuracy:.3f})ï¼Œ"
                    f"æ€§èƒ½ä¸‹é™ã€‚å»ºè®®å›æ»šæˆ–ç»§ç»­ä¼˜åŒ–ã€‚"
                )
            
            if new_auc > old_auc and new_accuracy > old_accuracy:
                suggestions.append(
                    f"âœ… æ–°ç‰ˆæœ¬å…¨é¢ä¼˜äºæ—§ç‰ˆæœ¬ï¼ˆAUC: {old_auc:.3f} â†’ {new_auc:.3f}, "
                    f"å‡†ç¡®ç‡: {old_accuracy:.3f} â†’ {new_accuracy:.3f}ï¼‰ã€‚"
                    f"å»ºè®®å‡çº§åˆ°æ–°ç‰ˆæœ¬ã€‚"
                )
            
            return {
                'needs_intervention': True,  # ç‰ˆæœ¬å¯¹æ¯”æ€»æ˜¯éœ€è¦äººå·¥å†³ç­–
                'warnings': warnings,
                'suggestions': suggestions,
                'comparison': {
                    'old': old_metrics,
                    'new': new_metrics
                }
            }
        except Exception as e:
            return {
                'needs_intervention': True,
                'warnings': [f"âš ï¸  æ— æ³•è¿›è¡Œç‰ˆæœ¬å¯¹æ¯”: {e}"],
                'suggestions': []
            }
    
    def print_intervention_reminder(self, title: str, check_result: Dict):
        """
        æ‰“å°äººå·¥ä»‹å…¥æé†’
        
        Args:
            title: æ£€æŸ¥æ ‡é¢˜
            check_result: æ£€æŸ¥ç»“æœ
        """
        log.info("=" * 80)
        log.info(f"ğŸ‘¤ äººå·¥ä»‹å…¥æ£€æŸ¥: {title}")
        log.info("=" * 80)
        
        if check_result.get('needs_intervention', False):
            log.warning("âš ï¸  éœ€è¦äººå·¥ä»‹å…¥ï¼")
        else:
            log.success("âœ“ å½“å‰é…ç½®æ­£å¸¸")
        
        warnings = check_result.get('warnings', [])
        if warnings:
            log.warning("\nè­¦å‘Š:")
            for warning in warnings:
                log.warning(f"  {warning}")
        
        suggestions = check_result.get('suggestions', [])
        if suggestions:
            log.info("\nå»ºè®®:")
            for suggestion in suggestions:
                log.info(f"  {suggestion}")
        
        log.info("=" * 80)
        
        return check_result.get('needs_intervention', False)


def require_human_confirmation(
    message: str,
    default: bool = False,
    timeout: Optional[int] = None
) -> bool:
    """
    è¦æ±‚äººå·¥ç¡®è®¤
    
    Args:
        message: ç¡®è®¤æ¶ˆæ¯
        default: é»˜è®¤å€¼ï¼ˆå¦‚æœè¶…æ—¶ï¼‰
        timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼ŒNoneè¡¨ç¤ºä¸è¶…æ—¶
        
    Returns:
        ç”¨æˆ·ç¡®è®¤ç»“æœ
    """
    # æ£€æŸ¥ç¯å¢ƒå˜é‡ï¼Œå¦‚æœè®¾ç½®äº† AUTO_CONFIRM=1ï¼Œåˆ™è‡ªåŠ¨ç¡®è®¤
    import os
    auto_confirm = os.environ.get('AUTO_CONFIRM', '0')
    if auto_confirm == '1':
        log.info("=" * 80)
        log.info("ğŸ¤– è‡ªåŠ¨ç¡®è®¤æ¨¡å¼ï¼ˆAUTO_CONFIRM=1ï¼‰")
        log.info("=" * 80)
        log.info(message)
        log.info("=" * 80)
        log.info(f"è‡ªåŠ¨ä½¿ç”¨é»˜è®¤å€¼: {default}")
        return default
    
    log.warning("=" * 80)
    log.warning("ğŸ‘¤ éœ€è¦äººå·¥ç¡®è®¤")
    log.warning("=" * 80)
    log.warning(message)
    log.warning("=" * 80)
    
    if timeout:
        log.warning(f"â° è¶…æ—¶æ—¶é—´: {timeout}ç§’ï¼Œè¶…æ—¶å°†ä½¿ç”¨é»˜è®¤å€¼: {default}")
    
    try:
        response = input(f"\nè¯·ç¡®è®¤ (y/nï¼Œé»˜è®¤: {'y' if default else 'n'}): ").strip().lower()
        
        if not response:
            return default
        
        return response in ['y', 'yes', 'æ˜¯', 'ç¡®è®¤']
    except (KeyboardInterrupt, EOFError):
        log.warning(f"\nç”¨æˆ·ä¸­æ–­ï¼Œä½¿ç”¨é»˜è®¤å€¼: {default}")
        return default


def prompt_human_input(
    message: str,
    input_type: type = str,
    default: Optional[any] = None,
    validator: Optional[callable] = None
) -> any:
    """
    æç¤ºäººå·¥è¾“å…¥
    
    Args:
        message: æç¤ºæ¶ˆæ¯
        input_type: è¾“å…¥ç±»å‹
        default: é»˜è®¤å€¼
        validator: éªŒè¯å‡½æ•°
        
    Returns:
        ç”¨æˆ·è¾“å…¥å€¼
    """
    log.info("=" * 80)
    log.info("ğŸ‘¤ éœ€è¦äººå·¥è¾“å…¥")
    log.info("=" * 80)
    log.info(message)
    
    if default is not None:
        log.info(f"é»˜è®¤å€¼: {default}")
    
    log.info("=" * 80)
    
    while True:
        try:
            user_input = input(f"\nè¯·è¾“å…¥: ").strip()
            
            if not user_input and default is not None:
                return default
            
            if not user_input:
                log.warning("è¾“å…¥ä¸èƒ½ä¸ºç©ºï¼Œè¯·é‡æ–°è¾“å…¥")
                continue
            
            # ç±»å‹è½¬æ¢
            try:
                converted_value = input_type(user_input)
            except ValueError:
                log.warning(f"è¾“å…¥æ ¼å¼é”™è¯¯ï¼ŒæœŸæœ›ç±»å‹: {input_type.__name__}")
                continue
            
            # éªŒè¯
            if validator and not validator(converted_value):
                log.warning("è¾“å…¥éªŒè¯å¤±è´¥ï¼Œè¯·é‡æ–°è¾“å…¥")
                continue
            
            return converted_value
            
        except (KeyboardInterrupt, EOFError):
            if default is not None:
                log.warning(f"\nç”¨æˆ·ä¸­æ–­ï¼Œä½¿ç”¨é»˜è®¤å€¼: {default}")
                return default
            raise

